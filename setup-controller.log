+ dirname /local/repository/setup-controller.sh
+ DIRNAME=/local/repository
+ [ -ne 0 ]
/local/repository/setup-controller.sh: 12: [: -ne: unexpected operator
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-controller.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=a9f3d26fda5b4ee3d9a8
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x = x ]
+ UPDATING=0
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ cat /root/setup/manifests.0.xml
+ perl -e $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } }
+ xargs
DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ [ controller != controller ]
+ logtstart controller
+ area=controller
+ echo controller
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=controller
+ date +%s
+ stamp=1556827517
+ date
+ date=Thu May  2 14:05:17 MDT 2019
+ eval LOGTIMESTART_controller=1556827517
+ LOGTIMESTART_controller=1556827517
+ echo START controller 1556827517 Thu May  2 14:05:17 MDT 2019
+ [ -f /root/setup/settings ]
+ . /root/setup/settings
+ GENIUSER=1
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ STORAGEHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=compute-1 
+ MIRRORSETUP=1
+ maybe_install_packages qemu-utils wget lockfile-progs rpm
+ [ ! 0 -eq 0 ]
+ are_packages_installed qemu-utils wget lockfile-progs rpm
+ retval=1
+ [ ! -z qemu-utils ]
+ dpkg -s qemu-utils
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z wget ]
+ dpkg -s wget
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z lockfile-progs ]
+ dpkg -s lockfile-progs
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z rpm ]
+ dpkg -s rpm
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ x86_64 = aarch64 ]
+ maybe_install_packages pssh
+ [ ! 0 -eq 0 ]
+ are_packages_installed pssh
+ retval=1
+ [ ! -z pssh ]
+ dpkg -s pssh
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ PSSH=/usr/bin/parallel-ssh -t 0 -O StrictHostKeyChecking=no 
+ PSCP=/usr/bin/parallel-scp -t 0 -O StrictHostKeyChecking=no 
+ maybe_install_packages dma
+ [ ! 0 -eq 0 ]
+ are_packages_installed dma
+ retval=1
+ [ ! -z dma ]
+ dpkg -s dma
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ maybe_install_packages mailutils
+ [ ! 0 -eq 0 ]
+ are_packages_installed mailutils
+ retval=1
+ [ ! -z mailutils ]
+ dpkg -s mailutils
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ echo hp104.utah.cloudlab.us
+ sleep 2
+ hostname
+ + [mail 17 -s -ge OpenStack Instance Setting Up 11 tboudwin@wcupa.edu ]

+ maybe_install_packages python-openstackclient
+ [ !+  0 -eq 0 ]
+ are_packages_installed/local/repository/setup-images.sh python-openstackclient

+ retval=1
+ [ ! -z python-openstackclient ]
+ dpkg -s python-openstackclient
+ echo Your OpenStack instance is setting up on controller.tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us .
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -ge 11 -a 17 -lt 14 ]
+ [ -z  ]
+ logtstart database
+ area=database
+ echo database
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=database
+ date +%s
+ stamp=1556827519
+ date
+ date=Thu May  2 14:05:19 MDT 2019
+ eval LOGTIMESTART_database=1556827519
+ LOGTIMESTART_database=1556827519
+ echo START database 1556827519 Thu May  2 14:05:19 MDT 2019
+ maybe_install_packages mariadb-server python-pymysql
+ [ ! 0 -eq 0 ]
+ are_packages_installed mariadb-server python-pymysql
+ retval=1
+ [ ! -z mariadb-server ]
+ dpkg -s mariadb-server
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-pymysql ]
+ dpkg -s python-pymysql
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ service_stop mysql
+ service=mysql
+ [ 1 -eq 0 ]
+ systemctl stop mysql
+ sleep 8
+ mysqld_safe --skip-grant-tables --skip-networking
190502 14:05:22 mysqld_safe Logging to syslog.
190502 14:05:22 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
+ openssl rand -hex 10
+ DB_ROOT_PASS=26e250af84ca2965f0e6
+ echo use mysql; update user set password=PASSWORD("26e250af84ca2965f0e6") where User='root'; delete from user where User=''; delete from user where User='root' and Host not in ('localhost', '127.0.0.1', '::1'); drop database test; delete from db where Db='test' or Db='test\_%'; flush privileges;
+ mysql -u root
ERROR 1008 (HY000) at line 1: Can't drop database 'test'; database doesn't exist
+ mysqladmin --password=26e250af84ca2965f0e6 shutdown
+ echo [mysqld]
+ echo bind-address = 192.168.0.1
+ echo default-storage-engine = innodb
+ echo innodb_file_per_table
+ echo collation-server = utf8_general_ci
+ echo init-connect = 'SET NAMES utf8'
+ echo character-set-server = utf8
+ echo max_connections = 4096
+ service_restart mysql
+ service=mysql
+ [ 1 -eq 0 ]
+ systemctl restart mysql
+ service_enable mysql
+ service=mysql
+ [ 1 -eq 0 ]
+ systemctl enable mysql
mysql.service is not a native service, redirecting to systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable mysql
+ echo DB_ROOT_PASS="26e250af84ca2965f0e6"
+ [ -z  -a 17 -ge 12 ]
+ cat
+ chmod 755 /etc/init.d/legacy-openvpn-net-waiter
+ update-rc.d legacy-openvpn-net-waiter defaults
+ update-rc.d legacy-openvpn-net-waiter enable
+ logtend database
+ area=database
+ echo database
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=database
+ date +%s
+ stamp=1556827535
+ date
+ date=Thu May  2 14:05:35 MDT 2019
+ eval tss=$LOGTIMESTART_database
+ tss=1556827519
+ expr 1556827535 - 1556827519
+ tsres=16
+ perl -e print 16 / 60.0 . "\n"
+ resmin=0.266666666666667
+ echo END database 1556827535 Thu May  2 14:05:35 MDT 2019
+ echo TOTAL database 16 0.266666666666667
+ [ -z  ]
+ logtstart rabbit
+ area=rabbit
+ echo rabbit
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=rabbit
+ date +%s
+ stamp=1556827535
+ date
+ date=Thu May  2 14:05:35 MDT 2019
+ eval LOGTIMESTART_rabbit=1556827535
+ LOGTIMESTART_rabbit=1556827535
+ echo START rabbit 1556827535 Thu May  2 14:05:35 MDT 2019
+ maybe_install_packages rabbitmq-server
+ [ ! 0 -eq 0 ]
+ are_packages_installed rabbitmq-server
+ retval=1
+ [ ! -z rabbitmq-server ]
+ dpkg -s rabbitmq-server
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ service_restart rabbitmq-server
+ service=rabbitmq-server
+ [ 1 -eq 0 ]
+ systemctl restart rabbitmq-server
+ service_enable rabbitmq-server
+ service=rabbitmq-server
+ [ 1 -eq 0 ]
+ systemctl enable rabbitmq-server
Synchronizing state of rabbitmq-server.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable rabbitmq-server
+ rabbitmqctl start_app
Starting node rabbit@controller
+ [ ! 0 -eq 0 ]
+ [ 17 -lt 14 ]
+ [ queens = juno ]
+ RABBIT_USER=openstack
+ rabbitmqctl add_vhost /
Creating vhost "/"
Error: vhost_already_exists: /
+ openssl rand -hex 10
+ RABBIT_PASS=c00d09297284c93378d5
+ RABBIT_URL=rabbit://openstack:c00d09297284c93378d5@controller
+ rabbitmqctl change_password openstack c00d09297284c93378d5
Changing password for user "openstack"
Error: no_such_user: openstack
+ [ ! 70 -eq 0 ]
+ rabbitmqctl add_user openstack c00d09297284c93378d5
Creating user "openstack"
+ rabbitmqctl set_permissions openstack .* .* .*
Setting permissions for user "openstack" in vhost "/"
+ echo RABBIT_USER="openstack"
+ echo RABBIT_PASS="c00d09297284c93378d5"
+ echo RABBIT_URL="rabbit://openstack:c00d09297284c93378d5@controller"
+ rabbitmqctl stop_app
Stopping rabbit application on node rabbit@controller
+ service_restart rabbitmq-server
+ service=rabbitmq-server
+ [ 1 -eq 0 ]
+ systemctl restart rabbitmq-server
+ rabbitmqctl start_app
Starting node rabbit@controller
+ [ ! 0 -eq 0 ]
+ [ -z  -a 17 -ge 12 ]
+ cat
+ systemctl enable openvpn-net-waiter.service
Created symlink /etc/systemd/system/multi-user.target.wants/openvpn-net-waiter.service → /etc/systemd/system/openvpn-net-waiter.service.
+ logtend rabbit
+ area=rabbit
+ echo rabbit
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=rabbit
+ date +%s
+ stamp=1556827544
+ date
+ date=Thu May  2 14:05:44 MDT 2019
+ eval tss=$LOGTIMESTART_rabbit
+ tss=1556827535
+ expr 1556827544 - 1556827535
+ tsres=9
+ perl -e print 9 / 60.0 . "\n"
+ resmin=0.15
+ echo END rabbit 1556827544 Thu May  2 14:05:44 MDT 2019
+ echo TOTAL rabbit 9 0.15
+ DOMARG=
+ [ x3 = x3 ]
+ DOMARG=--domain default
+ [ -z  ]
+ logtstart memcache
+ area=memcache
+ echo memcache
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=memcache
+ date +%s
+ stamp=1556827544
+ date
+ date=Thu May  2 14:05:44 MDT 2019
+ eval LOGTIMESTART_memcache=1556827544
+ LOGTIMESTART_memcache=1556827544
+ echo START memcache 1556827544 Thu May  2 14:05:44 MDT 2019
+ maybe_install_packages memcached python-memcache
+ [ ! 0 -eq 0 ]
+ are_packages_installed memcached python-memcache
+ retval=1
+ [ ! -z memcached ]
+ dpkg -s memcached
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-memcache ]
+ dpkg -s python-memcache
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ cat
+ [ 1 -eq 1 ]
+ mkdir /etc/systemd/system/memcached.service.d
+ systemctl list-units
+ grep -q networking.service
+ [ 1 -eq 0 ]
+ systemctl list-units
+ grep -q network-online.target
+ [ 0 -eq 0 ]
+ cat
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ service_enable memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl enable memcached
Synchronizing state of memcached.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable memcached
+ echo MEMCACHE_DONE=1
+ logtend memcache
+ area=memcache
+ echo memcache
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=memcache
+ date +%s
+ stamp=1556827545
+ date
+ date=Thu May  2 14:05:45 MDT 2019
+ eval tss=$LOGTIMESTART_memcache
+ tss=1556827544
+ expr 1556827545 - 1556827544
+ tsres=1
+ perl -e print 1 / 60.0 . "\n"
+ resmin=0.0166666666666667
+ echo END memcache 1556827545 Thu May  2 14:05:45 MDT 2019
+ echo TOTAL memcache 1 0.0166666666666667
+ [ ! x86_64 -eq aarch64 -a 17 -ge 17 -a -z  ]
/local/repository/setup-controller.sh: 323: [: Illegal number: x86_64
+ [ -z  ]
+ logtstart keystone
+ area=keystone
+ echo keystone
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=keystone
+ date +%s
+ stamp=1556827545
+ date
+ date=Thu May  2 14:05:45 MDT 2019
+ eval LOGTIMESTART_keystone=1556827545
+ LOGTIMESTART_keystone=1556827545
+ echo START keystone 1556827545 Thu May  2 14:05:45 MDT 2019
+ openssl rand -hex 10
+ KEYSTONE_DBPASS=0a4f4c06d3fefbade0ee
+ echo create database keystone
+ mysql -u root --password=26e250af84ca2965f0e6
+ + echo grant all privileges on keystone.* to 'keystone'@'localhost' identified by '0a4f4c06d3fefbade0ee'
mysql -u root --password=26e250af84ca2965f0e6
+ + echo grant all privileges on keystone.* to 'keystone'@'%' identified by '0a4f4c06d3fefbade0ee'
mysql -u root --password=26e250af84ca2965f0e6
+ maybe_install_packages keystone python-keystoneclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed keystone python-keystoneclient
+ retval=1
+ [ ! -z keystone ]
+ dpkg -s keystone
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-keystoneclient ]
+ dpkg -s python-keystoneclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -ge 11 ]
+ maybe_install_packages apache2
+ [ ! 0 -eq 0 ]
+ are_packages_installed apache2
+ retval=1
+ [ ! -z apache2 ]
+ dpkg -s apache2
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ maybe_install_packages libapache2-mod-wsgi
+ [ ! 0 -eq 0 ]
+ are_packages_installed libapache2-mod-wsgi
+ retval=1
+ [ ! -z libapache2-mod-wsgi ]
+ dpkg -s libapache2-mod-wsgi
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ openssl rand -hex 10
+ ADMIN_TOKEN=ceb7fc9e5a6e2ee26f05
+ crudini --set /etc/keystone/keystone.conf DEFAULT admin_token ceb7fc9e5a6e2ee26f05
+ crudini --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:0a4f4c06d3fefbade0ee@controller/keystone
+ crudini --set /etc/keystone/keystone.conf token expiration 14400
+ [ 17 -le 10 ]
+ [ 17 -le 11 ]
+ [ 17 -le 13 ]
+ crudini --set /etc/keystone/keystone.conf token provider fernet
+ [ 1 -eq 1 ]
+ [ 17 -lt 17 ]
+ crudini --set /etc/keystone/keystone.conf cache backend dogpile.cache.memcached
+ crudini --set /etc/keystone/keystone.conf cache backend_argument url:127.0.0.1:11211
+ crudini --set /etc/keystone/keystone.conf cache enable true
+ crudini --set /etc/keystone/keystone.conf cache enabled true
+ crudini --set /etc/keystone/keystone.conf cache memcache_servers 127.0.0.1:11211
+ crudini --set /etc/keystone/keystone.conf cache memcached_servers 127.0.0.1:11211
+ crudini --set /etc/keystone/keystone.conf memcache servers 127.0.0.1:11211
+ crudini --set /etc/keystone/keystone.conf DEFAULT verbose False
+ crudini --set /etc/keystone/keystone.conf DEFAULT debug False
+ su -s /bin/sh -c /usr/bin/keystone-manage db_sync keystone
+ [ 17 -ge 14 ]
+ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
+ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
+ [ 17 -eq 11 -a 1 -eq 1 ]
+ [ 17 -ge 12 -a 1 -eq 1 -a 17 -lt 14 ]
+ [ 17 -le 10 -o 1 -eq 0 ]
+ service_stop keystone
+ service=keystone
+ [ 1 -eq 0 ]
+ systemctl stop keystone
Failed to stop keystone.service: Unit keystone.service not loaded.
+ service_disable keystone
+ service=keystone
+ [ 1 -eq 0 ]
+ systemctl disable keystone
Failed to disable unit: Unit file keystone.service does not exist.
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ service_enable apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl enable apache2
Synchronizing state of apache2.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable apache2
+ rm -f /var/lib/keystone/keystone.db
+ sleep 8
+ crontab -l -u keystone
+ grep -q token_flush
+ echo @hourly /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1
+ [ 17 -lt 11 ]
+ export OS_TOKEN=ceb7fc9e5a6e2ee26f05
+ export OS_URL=http://controller:5000/v3
+ [ x3 = x3 ]
+ export OS_IDENTITY_API_VERSION=3
+ [ 17 -lt 11 ]
+ __openstack service create --name keystone --description OpenStack Identity identity
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name keystone --description OpenStack Identity identity
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Identity               |
| enabled     | True                             |
| id          | 186f97efe41e4d1c979123b561e7f5f7 |
| name        | keystone                         |
| type        | identity                         |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne identity public http://controller:5000/v3
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne identity public http://controller:5000/v3
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | bbac285b85be4edfaea6a7db2ece7e50 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 186f97efe41e4d1c979123b561e7f5f7 |
| service_name | keystone                         |
| service_type | identity                         |
| url          | http://controller:5000/v3        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne identity internal http://controller:5000/v3
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne identity internal http://controller:5000/v3
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 229d925c455140bea1dfe82d8bc9779d |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 186f97efe41e4d1c979123b561e7f5f7 |
| service_name | keystone                         |
| service_type | identity                         |
| url          | http://controller:5000/v3        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne identity admin http://controller:5000/v3
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne identity admin http://controller:5000/v3
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 8cbf3d5fd21341f29f28703ac3a267d2 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 186f97efe41e4d1c979123b561e7f5f7 |
| service_name | keystone                         |
| service_type | identity                         |
| url          | http://controller:5000/v3        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ x7ae0982e4f73 = x ]
+ APSWD=7ae0982e4f73
+ [ 17 -eq 10 ]
+ [ 17 -ge 13 ]
+ openstack domain create --description Default Domain default
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Default Domain                   |
| enabled     | True                             |
| id          | 1876f1092a7c41c38cd70d7d116a9abc |
| name        | default                          |
| tags        | []                               |
+-------------+----------------------------------+
+ __openstack project create --domain default --description Admin Project admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack project create --domain default --description Admin Project admin
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Admin Project                    |
| domain_id   | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled     | True                             |
| id          | a3b1d2ed83f74877a4ef333e937e3474 |
| is_domain   | False                            |
| name        | admin                            |
| parent_id   | 1876f1092a7c41c38cd70d7d116a9abc |
| tags        | []                               |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack user create --domain default --password 7ae0982e4f73 --email tboudwin@wcupa.edu admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 7ae0982e4f73 --email tboudwin@wcupa.edu admin
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| email               | tboudwin@wcupa.edu               |
| enabled             | True                             |
| id                  | c424f540b7c64cd1bb376270cf6e1082 |
| name                | admin                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role create admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role create admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | 0293a46f2b094c5aa3388a6357dfc0d3 |
| name      | admin                            |
+-----------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project admin --user admin admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project admin --user admin admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role create user
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role create user
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | aee9b80157f94f21aa9199c22b63ab21 |
| name      | user                             |
+-----------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project admin --user admin user
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project admin --user admin user
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack project create --domain default --description Service Project service
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack project create --domain default --description Service Project service
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Service Project                  |
| domain_id   | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled     | True                             |
| id          | 367c297931354b41a48d631e9353dda9 |
| is_domain   | False                            |
| name        | service                          |
| parent_id   | 1876f1092a7c41c38cd70d7d116a9abc |
| tags        | []                               |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack user create --domain default --password a9f3d26fda5b4ee3d9a8 --email tboudwin@wcupa.edu adminapi
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password a9f3d26fda5b4ee3d9a8 --email tboudwin@wcupa.edu adminapi
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| email               | tboudwin@wcupa.edu               |
| enabled             | True                             |
| id                  | b89da9ba4b764d70933f07659a7cdeb5 |
| name                | adminapi                         |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project admin --user adminapi admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project admin --user adminapi admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project admin --user adminapi user
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project admin --user adminapi user
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ x7ae0982e4f73 = x ]
+ [ 17 -lt 11 ]
+ unset OS_TOKEN OS_URL
+ unset OS_IDENTITY_API_VERSION
+ crudini --del /etc/keystone/keystone.conf DEFAULT admin_token
+ echo ADMIN_API="adminapi"
+ echo ADMIN_API_PASS="a9f3d26fda5b4ee3d9a8"
+ echo KEYSTONE_DBPASS="0a4f4c06d3fefbade0ee"
+ logtend keystone
+ area=keystone
+ echo keystone
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=keystone
+ date +%s
+ stamp=1556827576
+ date
+ date=Thu May  2 14:06:16 MDT 2019
+ eval tss=$LOGTIMESTART_keystone
+ tss=1556827545
+ expr 1556827576 - 1556827545
+ tsres=31
+ perl -e print 31 / 60.0 . "\n"
+ resmin=0.516666666666667
+ echo END keystone 1556827576 Thu May  2 14:06:16 MDT 2019
+ echo TOTAL keystone 31 0.516666666666667
+ echo export OS_TENANT_NAME=admin
+ echo export OS_USERNAME=adminapi
+ echo export OS_PASSWORD=a9f3d26fda5b4ee3d9a8
+ echo export OS_AUTH_URL=http://controller:5000/v2.0
+ echo OS_TENANT_NAME="admin"
+ echo OS_USERNAME="adminapi"
+ echo OS_PASSWORD="a9f3d26fda5b4ee3d9a8"
+ echo OS_AUTH_URL="http://controller:5000/v2.0"
+ [ x3 = x3 ]
+ echo OS_IDENTITY_API_VERSION=3
+ [ x3 = x3 ]
+ [ 17 -lt 13 ]
+ echo export OS_PROJECT_DOMAIN_NAME=default
+ echo export OS_USER_DOMAIN_NAME=default
+ echo export OS_PROJECT_NAME=admin
+ echo export OS_TENANT_NAME=admin
+ echo export OS_USERNAME=adminapi
+ echo export OS_PASSWORD=a9f3d26fda5b4ee3d9a8
+ echo export OS_AUTH_URL=http://controller:5000/v3
+ [ x3 = x3 ]
+ echo export OS_IDENTITY_API_VERSION=3
+ [ 17 -ge 14 ]
+ echo export OS_IMAGE_API_VERSION=2
+ [ 17 -ge 17 ]
+ echo export OS_AUTH_TYPE=password
+ [ x3 = x3 ]
+ [ 17 -lt 13 ]
+ echo OS_PROJECT_DOMAIN_NAME="default"
+ echo OS_USER_DOMAIN_NAME="default"
+ echo OS_PROJECT_NAME="admin"
+ echo OS_TENANT_NAME="admin"
+ echo OS_USERNAME="adminapi"
+ echo OS_PASSWORD="a9f3d26fda5b4ee3d9a8"
+ echo OS_AUTH_URL="http://controller:5000/v3"
+ [ x3 = x3 ]
+ echo OS_IDENTITY_API_VERSION=3
+ [ 17 -ge 14 ]
+ echo OS_IMAGE_API_VERSION=2
+ [ 17 -ge 17 ]
+ echo OS_AUTH_TYPE='password'
+ [ 17 -eq 10 ]
+ [ x3 = x3 ]
+ [ 17 -lt 13 ]
+ export OS_PROJECT_DOMAIN_NAME=default
+ export OS_USER_DOMAIN_NAME=default
+ export OS_PROJECT_NAME=admin
+ export OS_TENANT_NAME=admin
+ export OS_USERNAME=adminapi
+ export OS_PASSWORD=a9f3d26fda5b4ee3d9a8
+ export OS_AUTH_URL=http://controller:5000/v3
+ [ x3 = x3 ]
+ export OS_IDENTITY_API_VERSION=3
+ ln -sf /root/setup/admin-openrc-newcli.sh /root/setup/admin-openrc.sh
+ ln -sf /root/setup/admin-openrc-newcli.py /root/setup/admin-openrc.py
+ [ -z  ]
+ logtstart glance
+ area=glance
+ echo glance
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=glance
+ date +%s
+ stamp=1556827576
+ date
+ date=Thu May  2 14:06:16 MDT 2019
+ eval LOGTIMESTART_glance=1556827576
+ LOGTIMESTART_glance=1556827576
+ echo START glance 1556827576 Thu May  2 14:06:16 MDT 2019
+ openssl rand -hex 10
+ GLANCE_DBPASS=a6614c0139dcbb0d7b6c
+ openssl rand -hex 10
+ GLANCE_PASS=a3e74e251a8755f42955
+ echo create database glance
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on glance.* to 'glance'@'localhost' identified by 'a6614c0139dcbb0d7b6c'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on glance.* to 'glance'@'%' identified by 'a6614c0139dcbb0d7b6c'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -lt 11 ]
+ __openstack user create --domain default --password a3e74e251a8755f42955 glance
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password a3e74e251a8755f42955 glance
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | d58b4416ac9842c1abf4eaf8050e33e4 |
| name                | glance                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user glance --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user glance --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name glance --description OpenStack Image Service image
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name glance --description OpenStack Image Service image
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Image Service          |
| enabled     | True                             |
| id          | 458daeca157849e5a31beff67c698fd9 |
| name        | glance                           |
| type        | image                            |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne image public http://controller:9292
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne image public http://controller:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | f84490cd198847e2802aa7fd0d74529f |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 458daeca157849e5a31beff67c698fd9 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://controller:9292           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne image internal http://controller:9292
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne image internal http://controller:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 91cca74f110d4de1b40f1a1f0ea3b629 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 458daeca157849e5a31beff67c698fd9 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://controller:9292           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne image admin http://controller:9292
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne image admin http://controller:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | c9895de47c2845959507a15d183bc822 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 458daeca157849e5a31beff67c698fd9 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://controller:9292           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages glance python-glanceclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed glance python-glanceclient
+ retval=1
+ [ ! -z glance ]
+ dpkg -s glance
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-glanceclient ]
+ dpkg -s python-glanceclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ crudini --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:a6614c0139dcbb0d7b6c@controller/glance
+ crudini --set /etc/glance/glance-api.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/glance/glance-api.conf DEFAULT verbose False
+ crudini --set /etc/glance/glance-api.conf DEFAULT debug False
+ crudini --set /etc/glance/glance-api.conf paste_deploy flavor keystone
+ [ 17 -eq 10 ]
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken auth_type password
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken project_name service
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken username glance
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken password a3e74e251a8755f42955
+ crudini --set /etc/glance/glance-api.conf glance_store default_store file
+ crudini --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/
+ [ 17 -ge 14 ]
+ crudini --set /etc/glance/glance-api.conf glance_store stores file,http
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller:11211
+ crudini --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:a6614c0139dcbb0d7b6c@controller/glance
+ crudini --set /etc/glance/glance-registry.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/glance/glance-registry.conf DEFAULT verbose False
+ crudini --set /etc/glance/glance-registry.conf DEFAULT debug False
+ crudini --set /etc/glance/glance-registry.conf paste_deploy flavor keystone
+ [ 17 -eq 10 ]
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken project_name service
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken username glance
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken password a3e74e251a8755f42955
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller:11211
+ su -s /bin/sh -c /usr/bin/glance-manage db_sync glance
/usr/lib/python2.7/dist-packages/oslo_db/sqlalchemy/enginefacade.py:1334: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade
  expire_on_commit=expire_on_commit, _conf=conf)
2019-05-02 14:06:27.325 11626 INFO alembic.runtime.migration [-] Context impl MySQLImpl.
2019-05-02 14:06:27.326 11626 INFO alembic.runtime.migration [-] Will assume non-transactional DDL.
2019-05-02 14:06:27.337 11626 INFO alembic.runtime.migration [-] Context impl MySQLImpl.
2019-05-02 14:06:27.337 11626 INFO alembic.runtime.migration [-] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> liberty, liberty initial
INFO  [alembic.runtime.migration] Running upgrade liberty -> mitaka01, add index on created_at and updated_at columns of 'images' table
INFO  [alembic.runtime.migration] Running upgrade mitaka01 -> mitaka02, update metadef os_nova_server
INFO  [alembic.runtime.migration] Running upgrade mitaka02 -> ocata_expand01, add visibility to images
INFO  [alembic.runtime.migration] Running upgrade ocata_expand01 -> pike_expand01, empty expand for symmetry with pike_contract01
INFO  [alembic.runtime.migration] Running upgrade pike_expand01 -> queens_expand01
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade mitaka02 -> ocata_contract01, remove is_public from images
INFO  [alembic.runtime.migration] Running upgrade ocata_contract01 -> pike_contract01, drop glare artifacts tables
INFO  [alembic.runtime.migration] Running upgrade pike_contract01 -> queens_contract01
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
Upgraded database to: queens_expand01, current revision(s): queens_expand01
Database migration is up to date. No migration needed.
Upgraded database to: queens_contract01, current revision(s): queens_contract01
Database is synced successfully.
+ [ -n 32 -a ! 32 = 0 ]
+ service_stop glance-registry
+ service=glance-registry
+ [ 1 -eq 0 ]
+ systemctl stop glance-registry
+ service_stop glance-api
+ service=glance-api
+ [ 1 -eq 0 ]
+ systemctl stop glance-api
+ /local/repository/setup-extra-space.sh
+ id -u
+ EUID=0
+ [ 0 -ne 0 ]
+ dirname /local/repository/setup-extra-space.sh
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-extra-space.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=c1e340688ff4ab74f8e7
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x = x ]
+ UPDATING=0
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ cat /root/setup/manifests.0.xml
+ perl -e $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } }
+ xargs
DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ [ -f /root/setup/extra-space-done ]
+ logtstart extra-space
+ area=extra-space
+ echo extra-space
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=extra_space
+ date +%s
+ stamp=1556827587
+ date
+ date=Thu May  2 14:06:27 MDT 2019
+ eval LOGTIMESTART_extra_space=1556827587
+ LOGTIMESTART_extra_space=1556827587
+ echo START extra-space 1556827587 Thu May  2 14:06:27 MDT 2019
+ [ -f /root/setup/settings ]
+ . /root/setup/settings
+ GENIUSER=1
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ STORAGEHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=compute-1 
+ MIRRORSETUP=1
+ DB_ROOT_PASS=26e250af84ca2965f0e6
+ RABBIT_USER=openstack
+ RABBIT_PASS=c00d09297284c93378d5
+ RABBIT_URL=rabbit://openstack:c00d09297284c93378d5@controller
+ MEMCACHE_DONE=1
+ ADMIN_API=adminapi
+ ADMIN_API_PASS=a9f3d26fda5b4ee3d9a8
+ KEYSTONE_DBPASS=0a4f4c06d3fefbade0ee
+ [ -f /root/setup/settings.local ]
+ . /root/setup/settings.local
+ STORAGEDIR=/storage
+ VGNAME=openstack-volumes
+ uname -m
+ ARCH=x86_64
+ maybe_install_packages lvm2
+ [ ! 0 -eq 0 ]
+ are_packages_installed lvm2
+ retval=1
+ [ ! -z lvm2 ]
+ dpkg -s lvm2
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -ge 15 ]
+ maybe_install_packages thin-provisioning-tools
+ [ ! 0 -eq 0 ]
+ are_packages_installed thin-provisioning-tools
+ retval=1
+ [ ! -z thin-provisioning-tools ]
+ dpkg -s thin-provisioning-tools
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ mkdir -p /storage
+ echo STORAGEDIR=/storage
+ vgdisplay emulab
File descriptor 3 (/local/repository/setup-extra-space.sh) leaked on vgdisplay invocation. Parent PID 11636: /bin/sh
  Volume group "emulab" not found
  Cannot process volume group emulab
+ [ 5 -eq 0 ]
+ [ -z  ]
+ LVM=1
+ DONE=0
+ MKEXTRAFS_ARGS=-l -v openstack-volumes -m util -z 1024
+ [ x86_64 = aarch64 -o x86_64 = ppc64le ]
+ lsblk -n -P -b -o NAME,FSTYPE,MOUNTPOINT,PARTTYPE,PARTUUID,TYPE,PKNAME,SIZE
+ perl -e my %devs = (); while (<STDIN>) { $_ =~ s/([A-Z0-9a-z]+=)/;\$$1/g; eval "$_"; if (!($TYPE eq "disk" || $TYPE eq "part")) { next; }; if (exists($devs{$PKNAME})) { delete $devs{$PKNAME}; } if ($FSTYPE eq "" && $MOUNTPOINT eq "" && ($PARTTYPE eq "" || $PARTTYPE eq "0x0") && (int($SIZE) > 3221225472)) { $devs{$NAME} = "/dev/$NAME"; } }; print join(" ",values(%devs))."\n"
+ cat /tmp/devs
+ DEVS=/dev/sda4
+ [ -n /dev/sda4 ]
+ pvcreate /dev/sda4
File descriptor 3 (/local/repository/setup-extra-space.sh) leaked on pvcreate invocation. Parent PID 11636: /bin/sh
  Physical volume "/dev/sda4" successfully created.
+ vgcreate openstack-volumes /dev/sda4
File descriptor 3 (/local/repository/setup-extra-space.sh) leaked on vgcreate invocation. Parent PID 11636: /bin/sh
  Volume group "openstack-volumes" successfully created
+ [ ! 0 -eq 0 ]
+ DONE=1
+ [ 1 -eq 0 ]
+ vgs -o vg_size --noheadings --units G openstack-volumes
+ sed -ne s/ *\([0-9]*\)[0-9\.]*G/\1/p
File descriptor 3 (/local/repository/setup-extra-space.sh) leaked on vgs invocation. Parent PID 11724: /bin/sh
+ VGTOTAL=456
+ echo VGNAME=openstack-volumes
+ echo VGTOTAL=456
+ echo LVM=1
+ [ 1 -eq 1 ]
+ expr 456 - 1
+ vgt=455
+ [ controller = ctl ]
+ [ controller = controller ]
+ expr 455 - 32
+ vgt=423
+ [ 423 -lt 20 ]
+ perl -e print 0.75 * 423;
+ CINDER_LV_SIZE=317.25
+ echo SWIFT_LV_SIZE=4
+ echo GLANCE_LV_SIZE=32
+ echo CINDER_LV_SIZE=317.25
+ logtend extra-space
+ area=extra-space
+ echo extra-space
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=extra_space
+ date +%s
+ stamp=1556827588
+ date
+ date=Thu May  2 14:06:28 MDT 2019
+ eval tss=$LOGTIMESTART_extra_space
+ tss=1556827587
+ expr 1556827588 - 1556827587
+ tsres=1
+ perl -e print 1 / 60.0 . "\n"
+ resmin=0.0166666666666667
+ echo END extra-space 1556827588 Thu May  2 14:06:28 MDT 2019
+ echo TOTAL extra-space 1 0.0166666666666667
+ touch /root/setup/extra-space-done
+ . /root/setup/settings.local
+ STORAGEDIR=/storage
+ VGNAME=openstack-volumes
+ VGTOTAL=456
+ LVM=1
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ CINDER_LV_SIZE=317.25
+ mkdir -p /storage/glance
+ chown glance:glance /storage/glance
+ chmod 770 /storage/glance
+ [ 1 = 1 ]
+ lvcreate -L 32G -n glance openstack-volumes
  Logical volume "glance" created.
+ [ -f /sbin/mkfs.ext4 ]
+ ftype=ext4
+ mkfs.ext4 /dev/openstack-volumes/glance
mke2fs 1.44.1 (24-Mar-2018)
Discarding device blocks:    4096/83886083149824/83886086295552/8388608               done                            
Creating filesystem with 8388608 4k blocks and 2097152 inodes
Filesystem UUID: ddebda38-bff1-4fd8-922b-ffa66310e570
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624

Allocating group tables:   0/256       done                            
Writing inode tables:   0/256       done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information:   0/256       done

+ echo /dev/openstack-volumes/glance /storage/glance none defaults 0 0
+ mount /dev/openstack-volumes/glance /storage/glance
+ rsync -avz /var/lib/glance/ /storage/glance/
sending incremental file list
./
.gnupg/
.gnupg/private-keys-v1.d/
image-cache/
image-cache/incomplete/
image-cache/invalid/
image-cache/queue/
images/

sent 278 bytes  received 47 bytes  650.00 bytes/sec
total size is 0  speedup is 0.00
+ rm -rf /var/lib/glance/image-cache /var/lib/glance/images
+ mount -o bind /storage/glance /var/lib/glance
+ echo /storage/glance /var/lib/glance none defaults,bind 0 0
+ service_restart glance-registry
+ service=glance-registry
+ [ 1 -eq 0 ]
+ systemctl restart glance-registry
+ service_enable glance-registry
+ service=glance-registry
+ [ 1 -eq 0 ]
+ systemctl enable glance-registry
Synchronizing state of glance-registry.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable glance-registry
+ service_restart glance-api
+ service=glance-api
+ [ 1 -eq 0 ]
+ systemctl restart glance-api
+ service_enable glance-api
+ service=glance-api
+ [ 1 -eq 0 ]
+ systemctl enable glance-api
Synchronizing state of glance-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable glance-api
+ rm -f /var/lib/glance/glance.sqlite
+ echo GLANCE_DBPASS="a6614c0139dcbb0d7b6c"
+ echo GLANCE_PASS="a3e74e251a8755f42955"
+ logtend glance
+ area=glance
+ echo glance
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=glance
+ date +%s
+ stamp=1556827592
+ date
+ date=Thu May  2 14:06:32 MDT 2019
+ eval tss=$LOGTIMESTART_glance
+ tss=1556827576
+ expr 1556827592 - 1556827576
+ tsres=16
+ perl -e print 16 / 60.0 . "\n"
+ resmin=0.266666666666667
+ echo END glance 1556827592 Thu May  2 14:06:32 MDT 2019
+ echo TOTAL glance 16 0.266666666666667
+ [ -z  ]
+ logtstart nova
+ area=nova
+ echo nova
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=nova
+ date +%s
+ stamp=1556827592
+ date
+ date=Thu May  2 14:06:32 MDT 2019
+ eval LOGTIMESTART_nova=1556827592
+ LOGTIMESTART_nova=1556827592
+ echo START nova 1556827592 Thu May  2 14:06:32 MDT 2019
+ openssl rand -hex 10
+ NOVA_DBPASS=c959955373a27ee1342a
+ openssl rand -hex 10
+ NOVA_PASS=1da42256777cafc9360e
+ maybe_install_packages nova-api
+ [ ! 0 -eq 0 ]
+ are_packages_installed nova-api
+ retval=1
+ [ ! -z nova-api ]
+ dpkg -s nova-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ echo create database nova
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on nova.* to 'nova'@'localhost' identified by 'c959955373a27ee1342a'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on nova.* to 'nova'@'%' identified by 'c959955373a27ee1342a'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -ge 13 ]
+ echo create database nova_api
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on nova_api.* to 'nova'@'localhost' identified by 'c959955373a27ee1342a'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on nova_api.* to 'nova'@'%' identified by 'c959955373a27ee1342a'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -ge 15 ]
+ echo create database nova_cell0
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on nova_cell0.* to 'nova'@'localhost' identified by 'c959955373a27ee1342a'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on nova_cell0.* to 'nova'@'%' identified by 'c959955373a27ee1342a'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password 1da42256777cafc9360e nova
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 1da42256777cafc9360e nova
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | f10da676d1c94b82943135707fe6f522 |
| name                | nova                             |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user nova --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user nova --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name nova --description OpenStack Compute Service compute
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name nova --description OpenStack Compute Service compute
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Compute Service        |
| enabled     | True                             |
| id          | 6d12c933c7f84258a32ba44e3b2225aa |
| name        | nova                             |
| type        | compute                          |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ [ 17 -lt 14 ]
+ __openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | cffd4d4c464d4160ba51c6c79f6c872b |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 6d12c933c7f84258a32ba44e3b2225aa |
| service_name | nova                             |
| service_type | compute                          |
| url          | http://controller:8774/v2.1      |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 14155e35b6b44701963477916f72291e |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 6d12c933c7f84258a32ba44e3b2225aa |
| service_name | nova                             |
| service_type | compute                          |
| url          | http://controller:8774/v2.1      |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | fe43aad8fc404c019e60891ec1b1c326 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 6d12c933c7f84258a32ba44e3b2225aa |
| service_name | nova                             |
| service_type | compute                          |
| url          | http://controller:8774/v2.1      |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 17 -ge 15 ]
+ openssl rand -hex 10
+ PLACEMENT_PASS=4eb4cb430c0078c3cc6b
+ __openstack user create --domain default --password 4eb4cb430c0078c3cc6b placement
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 4eb4cb430c0078c3cc6b placement
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | 692192c11b3d4481be87daeb8a794d5d |
| name                | placement                        |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user placement --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user placement --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name placement --description OpenStack Placement API placement
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name placement --description OpenStack Placement API placement
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Placement API          |
| enabled     | True                             |
| id          | 8188a58837a44243ada468e4c6336c27 |
| name        | placement                        |
| type        | placement                        |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ [ 17 -lt 14 ]
+ __openstack endpoint create --region RegionOne placement public http://controller:8778
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne placement public http://controller:8778
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | cdc6d79606ad4dcaaf372dce3807631d |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 8188a58837a44243ada468e4c6336c27 |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller:8778           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne placement internal http://controller:8778
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne placement internal http://controller:8778
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 8711ab88e759450f82e6e1e76aa1434a |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 8188a58837a44243ada468e4c6336c27 |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller:8778           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne placement admin http://controller:8778
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne placement admin http://controller:8778
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | bc536a003316403a90488b72da705ab5 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 8188a58837a44243ada468e4c6336c27 |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller:8778           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages nova-api nova-conductor nova-consoleauth nova-novncproxy nova-scheduler python-novaclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed nova-api nova-conductor nova-consoleauth nova-novncproxy nova-scheduler python-novaclient
+ retval=1
+ [ ! -z nova-api ]
+ dpkg -s nova-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z nova-conductor ]
+ dpkg -s nova-conductor
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z nova-consoleauth ]
+ dpkg -s nova-consoleauth
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z nova-novncproxy ]
+ dpkg -s nova-novncproxy
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z nova-scheduler ]
+ dpkg -s nova-scheduler
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-novaclient ]
+ dpkg -s python-novaclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -lt 17 ]
+ [ 17 -ge 15 ]
+ maybe_install_packages nova-placement-api
+ [ ! 0 -eq 0 ]
+ are_packages_installed nova-placement-api
+ retval=1
+ [ ! -z nova-placement-api ]
+ dpkg -s nova-placement-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 0 = 1 ]
+ [ 17 -ge 12 ]
+ crudini --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata
+ crudini --set /etc/nova/nova.conf database connection mysql+pymysql://nova:c959955373a27ee1342a@controller/nova
+ [ 17 -ge 13 ]
+ crudini --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:c959955373a27ee1342a@controller/nova_api
+ crudini --set /etc/nova/nova.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/nova/nova.conf DEFAULT my_ip 192.168.0.1
+ [ 17 -lt 13 ]
+ crudini --set /etc/nova/nova.conf glance api_servers http://controller:9292
+ crudini --set /etc/nova/nova.conf DEFAULT verbose False
+ crudini --set /etc/nova/nova.conf DEFAULT debug False
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 11 ]
+ crudini --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/nova/nova.conf keystone_authtoken auth_type password
+ crudini --set /etc/nova/nova.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/nova/nova.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/nova/nova.conf keystone_authtoken project_name service
+ crudini --set /etc/nova/nova.conf keystone_authtoken username nova
+ crudini --set /etc/nova/nova.conf keystone_authtoken password 1da42256777cafc9360e
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211
+ [ 17 -ge 13 ]
+ crudini --set /etc/nova/nova.conf DEFAULT use_neutron True
+ crudini --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
+ [ 17 -lt 12 ]
+ crudini --set /etc/nova/nova.conf vnc vncserver_listen 192.168.0.1
+ crudini --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.0.1
+ [ 17 -eq 11 ]
+ [ 17 -ge 12 -a 17 -lt 15 ]
+ [ 17 -ge 15 ]
+ crudini --set /etc/nova/nova.conf filter_scheduler available_filters nova.scheduler.filters.all_filters
+ crudini --set /etc/nova/nova.conf filter_scheduler enabled_filters RetryFilter, AvailabilityZoneFilter, RamFilter, ComputeFilter, ComputeCapabilitiesFilter, ImagePropertiesFilter, ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter
+ [ 17 -ge 11 ]
+ crudini --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp
+ [ 0 = 1 ]
+ [ 17 -ge 12 ]
+ cat /proc/cpuinfo
+ grep -i processor.*:
+ wc -l
+ ncpus=20
+ crudini --set /etc/nova/nova.conf api_database max_overflow 20
+ crudini --set /etc/nova/nova.conf api_database max_pool_size 20
+ [ 17 -ge 15 ]
+ crudini --set /etc/nova/nova.conf placement os_region_name RegionOne
+ crudini --set /etc/nova/nova.conf placement auth_url http://controller:5000/v3
+ crudini --set /etc/nova/nova.conf placement auth_type password
+ crudini --set /etc/nova/nova.conf placement project_domain_name default
+ crudini --set /etc/nova/nova.conf placement user_domain_name default
+ crudini --set /etc/nova/nova.conf placement project_name service
+ crudini --set /etc/nova/nova.conf placement username placement
+ crudini --set /etc/nova/nova.conf placement password 4eb4cb430c0078c3cc6b
+ [ 17 -ge 13 ]
+ su -s /bin/sh -c nova-manage api_db sync nova
+ [ 17 -ge 15 ]
+ su -s /bin/sh -c nova-manage cell_v2 map_cell0 nova
+ su -s /bin/sh -c nova-manage cell_v2 create_cell --name=cell1 --verbose nova
501dfe63-e12a-4188-a66a-15adf5b27f9e
+ su -s /bin/sh -c nova-manage db sync nova
/usr/lib/python2.7/dist-packages/pymysql/cursors.py:165: Warning: (1831, u'Duplicate index `block_device_mapping_instance_uuid_virtual_name_device_name_idx`. This is deprecated and will be disallowed in a future release.')
  result = self._query(query)
/usr/lib/python2.7/dist-packages/pymysql/cursors.py:165: Warning: (1831, u'Duplicate index `uniq_instances0uuid`. This is deprecated and will be disallowed in a future release.')
  result = self._query(query)
+ [ 17 -eq 16 ]
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ service_restart nova-api
+ service=nova-api
+ [ 1 -eq 0 ]
+ systemctl restart nova-api
+ service_enable nova-api
+ service=nova-api
+ [ 1 -eq 0 ]
+ systemctl enable nova-api
Synchronizing state of nova-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nova-api
+ [ 17 -lt 17 ]
+ service_restart nova-consoleauth
+ service=nova-consoleauth
+ [ 1 -eq 0 ]
+ systemctl restart nova-consoleauth
+ service_enable nova-consoleauth
+ service=nova-consoleauth
+ [ 1 -eq 0 ]
+ systemctl enable nova-consoleauth
Synchronizing state of nova-consoleauth.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nova-consoleauth
+ service_restart nova-scheduler
+ service=nova-scheduler
+ [ 1 -eq 0 ]
+ systemctl restart nova-scheduler
+ service_enable nova-scheduler
+ service=nova-scheduler
+ [ 1 -eq 0 ]
+ systemctl enable nova-scheduler
Synchronizing state of nova-scheduler.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nova-scheduler
+ service_restart nova-conductor
+ service=nova-conductor
+ [ 1 -eq 0 ]
+ systemctl restart nova-conductor
+ service_enable nova-conductor
+ service=nova-conductor
+ [ 1 -eq 0 ]
+ systemctl enable nova-conductor
Synchronizing state of nova-conductor.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nova-conductor
+ service_restart nova-novncproxy
+ service=nova-novncproxy
+ [ 1 -eq 0 ]
+ systemctl restart nova-novncproxy
+ service_enable nova-novncproxy
+ service=nova-novncproxy
+ [ 1 -eq 0 ]
+ systemctl enable nova-novncproxy
Synchronizing state of nova-novncproxy.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nova-novncproxy
+ service_restart nova-serialproxy
+ service=nova-serialproxy
+ [ 1 -eq 0 ]
+ systemctl restart nova-serialproxy
Failed to restart nova-serialproxy.service: Unit nova-serialproxy.service not found.
+ service_enable nova-serialproxy
+ service=nova-serialproxy
+ [ 1 -eq 0 ]
+ systemctl enable nova-serialproxy
Failed to enable unit: Unit file nova-serialproxy.service does not exist.
+ [ 17 -ge 15 ]
+ a2ensite nova-placement-api.conf
Site nova-placement-api already enabled
+ service apache2 reload
+ rm -f /var/lib/nova/nova.sqlite
+ /usr/bin/openstack flavor show m1.tiny
No flavor with a name or ID of 'm1.tiny' exists.
+ [ ! 1 -eq 0 ]
+ __openstack flavor create m1.tiny --id 1 --ram 512 --disk 1 --vcpus 1 --public
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack flavor create m1.tiny --id 1 --ram 512 --disk 1 --vcpus 1 --public
+----------------------------+---------+
| Field                      | Value   |
+----------------------------+---------+
| OS-FLV-DISABLED:disabled   | False   |
| OS-FLV-EXT-DATA:ephemeral  | 0       |
| disk                       | 1       |
| id                         | 1       |
| name                       | m1.tiny |
| os-flavor-access:is_public | True    |
| properties                 |         |
| ram                        | 512     |
| rxtx_factor                | 1.0     |
| swap                       |         |
| vcpus                      | 1       |
+----------------------------+---------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ /usr/bin/openstack flavor show m1.small
No flavor with a name or ID of 'm1.small' exists.
+ [ ! 1 -eq 0 ]
+ __openstack flavor create m1.small --id 2 --ram 2048 --disk 20 --vcpus 1 --public
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack flavor create m1.small --id 2 --ram 2048 --disk 20 --vcpus 1 --public
+----------------------------+----------+
| Field                      | Value    |
+----------------------------+----------+
| OS-FLV-DISABLED:disabled   | False    |
| OS-FLV-EXT-DATA:ephemeral  | 0        |
| disk                       | 20       |
| id                         | 2        |
| name                       | m1.small |
| os-flavor-access:is_public | True     |
| properties                 |          |
| ram                        | 2048     |
| rxtx_factor                | 1.0      |
| swap                       |          |
| vcpus                      | 1        |
+----------------------------+----------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ /usr/bin/openstack flavor show m1.medium
No flavor with a name or ID of 'm1.medium' exists.
+ [ ! 1 -eq 0 ]
+ __openstack flavor create m1.medium --id 3 --ram 4096 --disk 40 --vcpus 2 --public
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack flavor create m1.medium --id 3 --ram 4096 --disk 40 --vcpus 2 --public
+----------------------------+-----------+
| Field                      | Value     |
+----------------------------+-----------+
| OS-FLV-DISABLED:disabled   | False     |
| OS-FLV-EXT-DATA:ephemeral  | 0         |
| disk                       | 40        |
| id                         | 3         |
| name                       | m1.medium |
| os-flavor-access:is_public | True      |
| properties                 |           |
| ram                        | 4096      |
| rxtx_factor                | 1.0       |
| swap                       |           |
| vcpus                      | 2         |
+----------------------------+-----------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ /usr/bin/openstack flavor show m1.large
No flavor with a name or ID of 'm1.large' exists.
+ [ ! 1 -eq 0 ]
+ __openstack flavor create m1.large --id 4 --ram 8192 --disk 80 --vcpus 4 --public
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack flavor create m1.large --id 4 --ram 8192 --disk 80 --vcpus 4 --public
+----------------------------+----------+
| Field                      | Value    |
+----------------------------+----------+
| OS-FLV-DISABLED:disabled   | False    |
| OS-FLV-EXT-DATA:ephemeral  | 0        |
| disk                       | 80       |
| id                         | 4        |
| name                       | m1.large |
| os-flavor-access:is_public | True     |
| properties                 |          |
| ram                        | 8192     |
| rxtx_factor                | 1.0      |
| swap                       |          |
| vcpus                      | 4        |
+----------------------------+----------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ /usr/bin/openstack flavor show m1.xlarge
No flavor with a name or ID of 'm1.xlarge' exists.
+ [ ! 1 -eq 0 ]
+ __openstack flavor create m1.xlarge --id 5 --ram 16384 --disk 160 --vcpus 8 --public
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack flavor create m1.xlarge --id 5 --ram 16384 --disk 160 --vcpus 8 --public
+----------------------------+-----------+
| Field                      | Value     |
+----------------------------+-----------+
| OS-FLV-DISABLED:disabled   | False     |
| OS-FLV-EXT-DATA:ephemeral  | 0         |
| disk                       | 160       |
| id                         | 5         |
| name                       | m1.xlarge |
| os-flavor-access:is_public | True      |
| properties                 |           |
| ram                        | 16384     |
| rxtx_factor                | 1.0       |
| swap                       |           |
| vcpus                      | 8         |
+----------------------------+-----------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ x86_64 = ppc64le ]
+ echo NOVA_DBPASS="c959955373a27ee1342a"
+ echo NOVA_PASS="1da42256777cafc9360e"
+ echo PLACEMENT_PASS="4eb4cb430c0078c3cc6b"
+ logtend nova
+ area=nova
+ echo nova
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=nova
+ date +%s
+ stamp=1556827654
+ date
+ date=Thu May  2 14:07:34 MDT 2019
+ eval tss=$LOGTIMESTART_nova
+ tss=1556827592
+ expr 1556827654 - 1556827592
+ tsres=62
+ perl -e print 62 / 60.0 . "\n"
+ resmin=1.03333333333333
+ echo END nova 1556827654 Thu May  2 14:07:34 MDT 2019
+ echo TOTAL nova 62 1.03333333333333
+ PHOSTS=
+ mkdir -p /root/setup/pssh.setup-compute.stdout /root/setup/pssh.setup-compute.stderr
+ [ -z  ]
+ logtstart nova-computenodes
+ area=nova-computenodes
+ echo nova-computenodes
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=nova_computenodes
+ date +%s
+ stamp=1556827654
+ date
+ date=Thu May  2 14:07:34 MDT 2019
+ eval LOGTIMESTART_nova_computenodes=1556827654
+ LOGTIMESTART_nova_computenodes=1556827654
+ echo START nova-computenodes 1556827654 Thu May  2 14:07:34 MDT 2019
+ NOVA_COMPUTENODES_DONE=1
+ getfqdn compute-1
+ n=compute-1
+ cat /root/setup/fqdn.map
+ grep -E compute-1\s
+ cut -f2
+ fqdn=compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ echo compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ fqdn=compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ scp -o StrictHostKeyChecking=no /root/setup/settings admin-openrc.sh compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us:/root/setup
+ PHOSTS= -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ echo *** Setting up Cmopute service on nodes:  -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
*** Setting up Cmopute service on nodes:  -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ /usr/bin/parallel-ssh -t 0 -O StrictHostKeyChecking=no -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us -o /root/setup/pssh.setup-compute.stdout -e /root/setup/pssh.setup-compute.stderr /local/repository/setup-compute.sh
[1] 14:08:10 [SUCCESS] compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ touch /root/setup/compute-done-compute-1
+ [ 17 -ge 15 ]
+ su -s /bin/sh -c nova-manage cell_v2 discover_hosts --verbose nova
Found 2 cell mappings.
Skipping cell0 since it does not contain hosts.
Getting computes from cell 'cell1': 501dfe63-e12a-4188-a66a-15adf5b27f9e
Found 0 unmapped computes in cell: 501dfe63-e12a-4188-a66a-15adf5b27f9e
+ crudini --set /etc/nova/nova.conf scheduler discover_hosts_in_cells_interval 300
+ echo NOVA_COMPUTENODES_DONE="1"
+ logtend nova-computenodes
+ area=nova-computenodes
+ echo nova-computenodes
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=nova_computenodes
+ date +%s
+ stamp=1556827693
+ date
+ date=Thu May  2 14:08:13 MDT 2019
+ eval tss=$LOGTIMESTART_nova_computenodes
+ tss=1556827654
+ expr 1556827693 - 1556827654
+ tsres=39
+ perl -e print 39 / 60.0 . "\n"
+ resmin=0.65
+ echo END nova-computenodes 1556827693 Thu May  2 14:08:13 MDT 2019
+ echo TOTAL nova-computenodes 39 0.65
+ [ -z  ]
+ logtstart neutron
+ area=neutron
+ echo neutron
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron
+ date +%s
+ stamp=1556827693
+ date
+ date=Thu May  2 14:08:13 MDT 2019
+ eval LOGTIMESTART_neutron=1556827693
+ LOGTIMESTART_neutron=1556827693
+ echo START neutron 1556827693 Thu May  2 14:08:13 MDT 2019
+ openssl rand -hex 10
+ NEUTRON_DBPASS=8271678362114f5f9c59
+ openssl rand -hex 10
+ NEUTRON_PASS=60bd8ad0f9e9306d6413
+ openssl rand -hex 10
+ NEUTRON_METADATA_SECRET=1cb9ce7786a31ef78c37
+ . /root/setup/neutron.vars
+ network_types=flat,gre,vxlan
+ flat_networks=external,flat-lan-1
+ bridge_mappings=bridge_mappings=external:br-ex,flat-lan-1:br-flat-lan-1
+ extra_mappings=
+ network_vlan_ranges=network_vlan_ranges=
+ gre_local_ip=local_ip = 10.11.10.1
+ enable_tunneling=enable_tunneling = True
+ tunnel_types=tunnel_types = gre
+ interface_driver=neutron.agent.linux.interface.OVSInterfaceDriver
+ fwdriver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
+ echo create database neutron
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on neutron.* to 'neutron'@'localhost' identified by '8271678362114f5f9c59'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on neutron.* to 'neutron'@'%' identified by '8271678362114f5f9c59'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password 60bd8ad0f9e9306d6413 neutron
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 60bd8ad0f9e9306d6413 neutron
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | bd5c20def3ab4758ba2d109e55c83a22 |
| name                | neutron                          |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user neutron --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user neutron --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name neutron --description OpenStack Networking Service network
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name neutron --description OpenStack Networking Service network
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Networking Service     |
| enabled     | True                             |
| id          | 0f5e420c24c84235bd52df209ad68678 |
| name        | neutron                          |
| type        | network                          |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne network public http://controller:9696
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne network public http://controller:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 4fdd3556427e4cbebf93f5d4dfe1d244 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 0f5e420c24c84235bd52df209ad68678 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller:9696           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne network internal http://controller:9696
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne network internal http://controller:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | ea0868ebea974113bd7c7faa5060acd1 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 0f5e420c24c84235bd52df209ad68678 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller:9696           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne network admin http://controller:9696
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne network admin http://controller:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | e982ccec8be143e1b24d8db867fc8250 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 0f5e420c24c84235bd52df209ad68678 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller:9696           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages neutron-server neutron-plugin-ml2 python-neutronclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed neutron-server neutron-plugin-ml2 python-neutronclient
+ retval=1
+ [ ! -z neutron-server ]
+ dpkg -s neutron-server
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z neutron-plugin-ml2 ]
+ dpkg -s neutron-plugin-ml2
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-neutronclient ]
+ dpkg -s python-neutronclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ maybe_install_packages python-neutron-lbaas
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-neutron-lbaas
+ retval=1
+ [ ! -z python-neutron-lbaas ]
+ dpkg -s python-neutron-lbaas
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ crudini --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:8271678362114f5f9c59@controller/neutron
+ crudini --del /etc/neutron/neutron.conf keystone_authtoken auth_host
+ crudini --del /etc/neutron/neutron.conf keystone_authtoken auth_port
+ crudini --del /etc/neutron/neutron.conf keystone_authtoken auth_protocol
+ crudini --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/neutron/neutron.conf DEFAULT verbose False
+ crudini --set /etc/neutron/neutron.conf DEFAULT debug False
+ crudini --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
+ [ 17 -lt 14 ]
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ crudini --set /etc/neutron/neutron.conf DEFAULT service_plugins router,metering,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
+ crudini --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True
+ [ 17 -le 11 ]
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 11 ]
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken auth_type password
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken project_name service
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken username neutron
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken password 60bd8ad0f9e9306d6413
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211
+ crudini --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes True
+ crudini --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes True
+ crudini --set /etc/neutron/neutron.conf DEFAULT nova_url http://controller:8774/v2.1
+ [ 17 -eq 10 ]
+ crudini --set /etc/neutron/neutron.conf nova auth_url http://controller:5000
+ crudini --set /etc/neutron/neutron.conf nova auth_type password
+ crudini --set /etc/neutron/neutron.conf nova project_domain_name default
+ crudini --set /etc/neutron/neutron.conf nova user_domain_name default
+ crudini --set /etc/neutron/neutron.conf nova region_name RegionOne
+ crudini --set /etc/neutron/neutron.conf nova project_name service
+ crudini --set /etc/neutron/neutron.conf nova username nova
+ crudini --set /etc/neutron/neutron.conf nova password 1da42256777cafc9360e
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/neutron/neutron.conf nova memcached_servers controller:11211
+ [ 17 -ge 15 ]
+ crudini --set /etc/neutron/neutron.conf placement os_region_name RegionOne
+ crudini --set /etc/neutron/neutron.conf placement auth_url http://controller:5000/v3
+ crudini --set /etc/neutron/neutron.conf placement auth_type password
+ crudini --set /etc/neutron/neutron.conf placement project_domain_name default
+ crudini --set /etc/neutron/neutron.conf placement user_domain_name default
+ crudini --set /etc/neutron/neutron.conf placement project_name service
+ crudini --set /etc/neutron/neutron.conf placement username placement
+ crudini --set /etc/neutron/neutron.conf placement password 4eb4cb430c0078c3cc6b
+ [ 17 -lt 13 ]
+ crudini --set /etc/neutron/neutron.conf oslo_messaging_notifications driver messagingv2
+ [ 17 -ge 12 ]
+ cat /proc/cpuinfo
+ grep -i processor.*:
+ wc -l
+ ncpus=20
+ crudini --set /etc/neutron/neutron.conf database max_overflow 20
+ crudini --set /etc/neutron/neutron.conf database max_pool_size 20
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,gre,vxlan
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types flat,gre,vxlan
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks external,flat-lan-1
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_gre tunnel_id_ranges 1:1000
+ cat
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 3000:4000
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group True
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True
+ [ -n neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver ]
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ crudini --set /etc/neutron/neutron_lbaas.conf service_providers service_provider LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default
+ crudini --set /etc/nova/nova.conf DEFAULT network_api_class nova.network.neutronv2.api.API
+ crudini --set /etc/nova/nova.conf DEFAULT security_group_api neutron
+ [ openvswitch = openvswitch ]
+ crudini --set /etc/nova/nova.conf DEFAULT linuxnet_interface_driver nova.network.linux_net.LinuxOVSInterfaceDriver
+ crudini --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
+ crudini --set /etc/nova/nova.conf neutron url http://controller:9696
+ crudini --set /etc/nova/nova.conf neutron auth_strategy keystone
+ [ 17 -le 11 ]
+ crudini --set /etc/nova/nova.conf neutron auth_url http://controller:5000
+ [ 17 -lt 13 ]
+ crudini --set /etc/nova/nova.conf neutron project_domain_name default
+ crudini --set /etc/nova/nova.conf neutron user_domain_name default
+ crudini --set /etc/nova/nova.conf neutron auth_type password
+ crudini --set /etc/nova/nova.conf neutron project_name service
+ crudini --set /etc/nova/nova.conf neutron username neutron
+ crudini --set /etc/nova/nova.conf neutron password 60bd8ad0f9e9306d6413
+ crudini --set /etc/nova/nova.conf neutron region_name RegionOne
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/nova/nova.conf neutron memcached_servers controller:11211
+ crudini --set /etc/nova/nova.conf neutron service_metadata_proxy True
+ crudini --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret 1cb9ce7786a31ef78c37
+ [ 17 -ge 14 ]
+ su -s /bin/sh -c neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head neutron
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> kilo, kilo_initial
INFO  [alembic.runtime.migration] Running upgrade kilo -> 354db87e3225, nsxv_vdr_metadata.py
INFO  [alembic.runtime.migration] Running upgrade 354db87e3225 -> 599c6a226151, neutrodb_ipam
INFO  [alembic.runtime.migration] Running upgrade 599c6a226151 -> 52c5312f6baf, Initial operations in support of address scopes
INFO  [alembic.runtime.migration] Running upgrade 52c5312f6baf -> 313373c0ffee, Flavor framework
INFO  [alembic.runtime.migration] Running upgrade 313373c0ffee -> 8675309a5c4f, network_rbac
INFO  [alembic.runtime.migration] Running upgrade 8675309a5c4f -> 45f955889773, quota_usage
INFO  [alembic.runtime.migration] Running upgrade 45f955889773 -> 26c371498592, subnetpool hash
INFO  [alembic.runtime.migration] Running upgrade 26c371498592 -> 1c844d1677f7, add order to dnsnameservers
INFO  [alembic.runtime.migration] Running upgrade 1c844d1677f7 -> 1b4c6e320f79, address scope support in subnetpool
INFO  [alembic.runtime.migration] Running upgrade 1b4c6e320f79 -> 48153cb5f051, qos db changes
INFO  [alembic.runtime.migration] Running upgrade 48153cb5f051 -> 9859ac9c136, quota_reservations
INFO  [alembic.runtime.migration] Running upgrade 9859ac9c136 -> 34af2b5c5a59, Add dns_name to Port
INFO  [alembic.runtime.migration] Running upgrade 34af2b5c5a59 -> 59cb5b6cf4d, Add availability zone
INFO  [alembic.runtime.migration] Running upgrade 59cb5b6cf4d -> 13cfb89f881a, add is_default to subnetpool
INFO  [alembic.runtime.migration] Running upgrade 13cfb89f881a -> 32e5974ada25, Add standard attribute table
INFO  [alembic.runtime.migration] Running upgrade 32e5974ada25 -> ec7fcfbf72ee, Add network availability zone
INFO  [alembic.runtime.migration] Running upgrade ec7fcfbf72ee -> dce3ec7a25c9, Add router availability zone
INFO  [alembic.runtime.migration] Running upgrade dce3ec7a25c9 -> c3a73f615e4, Add ip_version to AddressScope
INFO  [alembic.runtime.migration] Running upgrade c3a73f615e4 -> 659bf3d90664, Add tables and attributes to support external DNS integration
INFO  [alembic.runtime.migration] Running upgrade 659bf3d90664 -> 1df244e556f5, add_unique_ha_router_agent_port_bindings
INFO  [alembic.runtime.migration] Running upgrade 1df244e556f5 -> 19f26505c74f, Auto Allocated Topology - aka Get-Me-A-Network
INFO  [alembic.runtime.migration] Running upgrade 19f26505c74f -> 15be73214821, add dynamic routing model data
INFO  [alembic.runtime.migration] Running upgrade 15be73214821 -> b4caf27aae4, add_bgp_dragent_model_data
INFO  [alembic.runtime.migration] Running upgrade b4caf27aae4 -> 15e43b934f81, rbac_qos_policy
INFO  [alembic.runtime.migration] Running upgrade 15e43b934f81 -> 31ed664953e6, Add resource_versions row to agent table
INFO  [alembic.runtime.migration] Running upgrade 31ed664953e6 -> 2f9e956e7532, tag support
INFO  [alembic.runtime.migration] Running upgrade 2f9e956e7532 -> 3894bccad37f, add_timestamp_to_base_resources
INFO  [alembic.runtime.migration] Running upgrade 3894bccad37f -> 0e66c5227a8a, Add desc to standard attr table
INFO  [alembic.runtime.migration] Running upgrade 0e66c5227a8a -> 45f8dd33480b, qos dscp db addition
INFO  [alembic.runtime.migration] Running upgrade 45f8dd33480b -> 5abc0278ca73, Add support for VLAN trunking
INFO  [alembic.runtime.migration] Running upgrade kilo -> 30018084ec99, Initial no-op Liberty contract rule.
INFO  [alembic.runtime.migration] Running upgrade 30018084ec99 -> 4ffceebfada, network_rbac
INFO  [alembic.runtime.migration] Running upgrade 4ffceebfada -> 5498d17be016, Drop legacy OVS and LB plugin tables
INFO  [alembic.runtime.migration] Running upgrade 5498d17be016 -> 2a16083502f3, Metaplugin removal
INFO  [alembic.runtime.migration] Running upgrade 2a16083502f3 -> 2e5352a0ad4d, Add missing foreign keys
INFO  [alembic.runtime.migration] Running upgrade 2e5352a0ad4d -> 11926bcfe72d, add geneve ml2 type driver
INFO  [alembic.runtime.migration] Running upgrade 11926bcfe72d -> 4af11ca47297, Drop cisco monolithic tables
INFO  [alembic.runtime.migration] Running upgrade 4af11ca47297 -> 1b294093239c, Drop embrane plugin table
INFO  [alembic.runtime.migration] Running upgrade 1b294093239c -> 8a6d8bdae39, standardattributes migration
INFO  [alembic.runtime.migration] Running upgrade 8a6d8bdae39 -> 2b4c2465d44b, DVR sheduling refactoring
INFO  [alembic.runtime.migration] Running upgrade 2b4c2465d44b -> e3278ee65050, Drop NEC plugin tables
INFO  [alembic.runtime.migration] Running upgrade e3278ee65050 -> c6c112992c9, rbac_qos_policy
INFO  [alembic.runtime.migration] Running upgrade c6c112992c9 -> 5ffceebfada, network_rbac_external
INFO  [alembic.runtime.migration] Running upgrade 5ffceebfada -> 4ffceebfcdc, standard_desc
INFO  [alembic.runtime.migration] Running upgrade 4ffceebfcdc -> 7bbb25278f53, device_owner_ha_replicate_int
INFO  [alembic.runtime.migration] Running upgrade 7bbb25278f53 -> 89ab9a816d70, Rename ml2_network_segments table
INFO  [alembic.runtime.migration] Running upgrade 5abc0278ca73 -> d3435b514502, Add device_id index to Port
INFO  [alembic.runtime.migration] Running upgrade d3435b514502 -> 30107ab6a3ee, provisioning_blocks.py
INFO  [alembic.runtime.migration] Running upgrade 30107ab6a3ee -> c415aab1c048, add revisions table
INFO  [alembic.runtime.migration] Running upgrade c415aab1c048 -> a963b38d82f4, add dns name to portdnses
INFO  [alembic.runtime.migration] Running upgrade 89ab9a816d70 -> c879c5e1ee90, Add segment_id to subnet
INFO  [alembic.runtime.migration] Running upgrade c879c5e1ee90 -> 8fd3918ef6f4, Add segment_host_mapping table.
INFO  [alembic.runtime.migration] Running upgrade 8fd3918ef6f4 -> 4bcd4df1f426, Rename ml2_dvr_port_bindings
INFO  [alembic.runtime.migration] Running upgrade 4bcd4df1f426 -> b67e765a3524, Remove mtu column from networks.
INFO  [alembic.runtime.migration] Running upgrade a963b38d82f4 -> 3d0e74aa7d37, Add flavor_id to Router
INFO  [alembic.runtime.migration] Running upgrade 3d0e74aa7d37 -> 030a959ceafa, uniq_routerports0port_id
INFO  [alembic.runtime.migration] Running upgrade 030a959ceafa -> a5648cfeeadf, Add support for Subnet Service Types
INFO  [alembic.runtime.migration] Running upgrade a5648cfeeadf -> 0f5bef0f87d4, add_qos_minimum_bandwidth_rules
INFO  [alembic.runtime.migration] Running upgrade 0f5bef0f87d4 -> 67daae611b6e, add standardattr to qos policies
INFO  [alembic.runtime.migration] Running upgrade 67daae611b6e -> 6b461a21bcfc, uniq_floatingips0floating_network_id0fixed_port_id0fixed_ip_addr
INFO  [alembic.runtime.migration] Running upgrade 6b461a21bcfc -> 5cd92597d11d, Add ip_allocation to port
INFO  [alembic.runtime.migration] Running upgrade 5cd92597d11d -> 929c968efe70, add_pk_version_table
INFO  [alembic.runtime.migration] Running upgrade 929c968efe70 -> a9c43481023c, extend_pk_with_host_and_add_status_to_ml2_port_binding
INFO  [alembic.runtime.migration] Running upgrade a9c43481023c -> 804a3c76314c, Add data_plane_status to Port
INFO  [alembic.runtime.migration] Running upgrade 804a3c76314c -> 2b42d90729da, qos add direction to bw_limit_rule table
INFO  [alembic.runtime.migration] Running upgrade 2b42d90729da -> 62c781cb6192, add is default to qos policies
INFO  [alembic.runtime.migration] Running upgrade 62c781cb6192 -> c8c222d42aa9, logging api
INFO  [alembic.runtime.migration] Running upgrade c8c222d42aa9 -> 349b6fd605a6, Add dns_domain to portdnses
INFO  [alembic.runtime.migration] Running upgrade 349b6fd605a6 -> 7d32f979895f, add mtu for networks
INFO  [alembic.runtime.migration] Running upgrade 7d32f979895f -> 594422d373ee, fip qos
INFO  [alembic.runtime.migration] Running upgrade b67e765a3524 -> a84ccf28f06a, migrate dns name from port
INFO  [alembic.runtime.migration] Running upgrade a84ccf28f06a -> 7d9d8eeec6ad, rename tenant to project
INFO  [alembic.runtime.migration] Running upgrade 7d9d8eeec6ad -> a8b517cff8ab, Add routerport bindings for L3 HA
INFO  [alembic.runtime.migration] Running upgrade a8b517cff8ab -> 3b935b28e7a0, migrate to pluggable ipam
INFO  [alembic.runtime.migration] Running upgrade 3b935b28e7a0 -> b12a3ef66e62, add standardattr to qos policies
INFO  [alembic.runtime.migration] Running upgrade b12a3ef66e62 -> 97c25b0d2353, Add Name and Description to the networksegments table
INFO  [alembic.runtime.migration] Running upgrade 97c25b0d2353 -> 2e0d7a8a1586, Add binding index to RouterL3AgentBinding
INFO  [alembic.runtime.migration] Running upgrade 2e0d7a8a1586 -> 5c85685d616d, Remove availability ranges.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> start_neutron_fwaas, start neutron-fwaas chain
INFO  [alembic.runtime.migration] Running upgrade start_neutron_fwaas -> 4202e3047e47, add_index_tenant_id
INFO  [alembic.runtime.migration] Running upgrade 4202e3047e47 -> 540142f314f4, FWaaS router insertion
INFO  [alembic.runtime.migration] Running upgrade 540142f314f4 -> 796c68dffbb, cisco_csr_fwaas
INFO  [alembic.runtime.migration] Running upgrade 796c68dffbb -> kilo, kilo
INFO  [alembic.runtime.migration] Running upgrade kilo -> c40fbb377ad, Initial Liberty no-op script.
INFO  [alembic.runtime.migration] Running upgrade c40fbb377ad -> 4b47ea298795, add reject rule
INFO  [alembic.runtime.migration] Running upgrade 4b47ea298795 -> d6a12e637e28, neutron-fwaas v2.0
INFO  [alembic.runtime.migration] Running upgrade d6a12e637e28 -> 876782258a43, create_default_firewall_groups_table
INFO  [alembic.runtime.migration] Running upgrade 876782258a43 -> f24e0d5e5bff, uniq_firewallgroupportassociation0port
INFO  [alembic.runtime.migration] Running upgrade kilo -> 67c8e8d61d5, Initial Liberty no-op script.
INFO  [alembic.runtime.migration] Running upgrade 67c8e8d61d5 -> 458aa42b14b, fw_table_alter script to make <name> column case sensitive
INFO  [alembic.runtime.migration] Running upgrade 458aa42b14b -> f83a0b2964d0, rename tenant to project
INFO  [alembic.runtime.migration] Running upgrade f83a0b2964d0 -> fd38cd995cc0, change shared attribute for firewall resource
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> start_neutron_lbaas, start neutron-lbaas chain
INFO  [alembic.runtime.migration] Running upgrade start_neutron_lbaas -> lbaasv2, lbaas version 2 api
INFO  [alembic.runtime.migration] Running upgrade lbaasv2 -> 4deef6d81931, add provisioning and operating statuses
INFO  [alembic.runtime.migration] Running upgrade 4deef6d81931 -> 4b6d8d5310b8, add_index_tenant_id
INFO  [alembic.runtime.migration] Running upgrade 4b6d8d5310b8 -> 364f9b6064f0, agentv2
INFO  [alembic.runtime.migration] Running upgrade 364f9b6064f0 -> lbaasv2_tls, lbaasv2 TLS
INFO  [alembic.runtime.migration] Running upgrade lbaasv2_tls -> 4ba00375f715, edge_driver
INFO  [alembic.runtime.migration] Running upgrade 4ba00375f715 -> kilo, kilo
INFO  [alembic.runtime.migration] Running upgrade kilo -> 3345facd0452, Initial Liberty no-op expand script.
INFO  [alembic.runtime.migration] Running upgrade 3345facd0452 -> 4a408dd491c2, Addition of Name column to lbaas_members and lbaas_healthmonitors table
INFO  [alembic.runtime.migration] Running upgrade 4a408dd491c2 -> 3426acbc12de, Add flavor id
INFO  [alembic.runtime.migration] Running upgrade 3426acbc12de -> 6aee0434f911, independent pools
INFO  [alembic.runtime.migration] Running upgrade 6aee0434f911 -> 3543deab1547, add_l7_tables
INFO  [alembic.runtime.migration] Running upgrade 3543deab1547 -> 62deca5010cd, Add tenant-id index for L7 tables
INFO  [alembic.runtime.migration] Running upgrade kilo -> 130ebfdef43, Initial Liberty no-op contract revision.
INFO  [alembic.runtime.migration] Running upgrade 130ebfdef43 -> 4b4dc6d5d843, rename tenant to project
INFO  [alembic.runtime.migration] Running upgrade 4b4dc6d5d843 -> e6417a8b114d, Drop v1 tables
INFO  [alembic.runtime.migration] Running upgrade 62deca5010cd -> 844352f9fe6f, Add healthmonitor max retries down
Running upgrade for neutron ...
OK
Running upgrade for neutron-fwaas ...
OK
Running upgrade for neutron-lbaas ...
OK
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ git clone https://git.openstack.org/openstack/neutron-lbaas-dashboard
Cloning into 'neutron-lbaas-dashboard'...
warning: redirecting to https://opendev.org/openstack/neutron-lbaas-dashboard/
+ cd neutron-lbaas-dashboard
+ git checkout stable/queens
Switched to a new branch 'stable/queens'
Branch 'stable/queens' set up to track remote branch 'stable/queens' from 'origin'.
+ python setup.py install
running install
[pbr] Writing ChangeLog
[pbr] Generating ChangeLog
[pbr] ChangeLog complete (0.0s)
[pbr] Generating AUTHORS
[pbr] AUTHORS complete (0.0s)
running build
running build_py
creating build
creating build/lib.linux-x86_64-2.7
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/unit
copying neutron_lbaas_dashboard/tests/unit/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/unit
copying neutron_lbaas_dashboard/tests/unit/registration.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/unit
copying neutron_lbaas_dashboard/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/enabled
copying neutron_lbaas_dashboard/enabled/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/enabled
copying neutron_lbaas_dashboard/enabled/_1481_project_ng_loadbalancersv2_panel.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/enabled
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest
copying neutron_lbaas_dashboard/api/rest/barbican.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest
copying neutron_lbaas_dashboard/api/rest/lbaasv2.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest
copying neutron_lbaas_dashboard/api/rest/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest
copying neutron_lbaas_dashboard/tests/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests
copying neutron_lbaas_dashboard/tests/urls.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests
copying neutron_lbaas_dashboard/tests/settings.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests
copying neutron_lbaas_dashboard/api/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project
copying neutron_lbaas_dashboard/dashboards/project/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project
copying neutron_lbaas_dashboard/dashboards/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/__init__.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/panel.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/urls.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/views.py -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
running egg_info
creating neutron_lbaas_dashboard.egg-info
writing requirements to neutron_lbaas_dashboard.egg-info/requires.txt
writing neutron_lbaas_dashboard.egg-info/PKG-INFO
writing top-level names to neutron_lbaas_dashboard.egg-info/top_level.txt
writing dependency_links to neutron_lbaas_dashboard.egg-info/dependency_links.txt
writing pbr to neutron_lbaas_dashboard.egg-info/pbr.json
[pbr] Processing SOURCES.txt
writing manifest file 'neutron_lbaas_dashboard.egg-info/SOURCES.txt'
[pbr] In git context, generating filelist from git
warning: no previously-included files matching '*.pyc' found anywhere in distribution
writing manifest file 'neutron_lbaas_dashboard.egg-info/SOURCES.txt'
copying neutron_lbaas_dashboard/karma.conf.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard
copying neutron_lbaas_dashboard/post_install.sh -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/de
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/de/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/de/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/de/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/de/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/de/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/en_GB
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/fr
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/fr/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/fr/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/id
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/id/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/id/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/id/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/id/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/id/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ja
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/ja/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/ja/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ko_KR
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/pt_BR
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/pt_BR/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/pt_BR/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/pt_BR/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ru
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/ru/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/ru/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/tr_TR
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/zh_CN
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES/django.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES
copying neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES/djangojs.po -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying neutron_lbaas_dashboard/static/app/core/openstack-service-api/barbican.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying neutron_lbaas_dashboard/static/app/core/openstack-service-api/barbican.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying neutron_lbaas_dashboard/static/app/core/openstack-service-api/lbaasv2.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying neutron_lbaas_dashboard/static/app/core/openstack-service-api/lbaasv2.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.module.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.module.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.scss -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/healthmonitors.module.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/healthmonitors.module.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/row-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/row-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/create.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/create.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete/delete.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete/delete.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/edit.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/edit.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/listeners.module.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/listeners.module.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/batch-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/batch-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/row-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/row-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete/delete.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete/delete.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.module.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.module.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/batch-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/batch-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/row-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/row-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete/delete.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete/delete.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip/modal.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip/modal.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.module.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.module.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/batch-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/batch-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/row-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/row-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/update-member-list.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/update-member-list.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/pools.module.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/pools.module.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/row-actions.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/row-actions.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/create.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/create.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete/delete.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete/delete.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/edit.action.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/edit.action.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/wizard.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/wizard.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators/validate-unique.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators/validate-unique.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.directive.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.directive.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.directive.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.directive.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/modal.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/modal.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/model.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/model.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/workflow.service.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/workflow.service.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.help.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.help.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.help.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.help.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.controller.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.controller.spec.js -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.help.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool/pool.help.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool
copying neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool/pool.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates
creating build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates/ngloadbalancersv2
copying neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates/ngloadbalancersv2/index.html -> build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates/ngloadbalancersv2
running install_lib
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/enabled
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/enabled/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/enabled
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/enabled/_1481_project_ng_loadbalancersv2_panel.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/enabled
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/post_install.sh -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.scss -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators/validate-unique.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators/validate-unique.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.module.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/batch-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip/modal.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip/modal.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/row-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/batch-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete/delete.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete/delete.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/row-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.module.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/batch-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/row-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/update-member-list.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/update-member-list.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/batch-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/row-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.module.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.module.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/workflow.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.help.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/modal.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.help.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/model.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.help.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/workflow.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool/pool.help.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool/pool.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/model.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.help.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/modal.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.help.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/listeners.module.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/batch-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/row-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/batch-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete/delete.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete/delete.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/row-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/listeners.module.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/pools.module.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/row-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/edit.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/edit.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/create.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/create.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete/delete.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete/delete.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/row-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/pools.module.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.directive.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.directive.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.directive.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.directive.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/healthmonitors.module.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/row-actions.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/edit.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/edit.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/create.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/wizard.controller.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/create.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/wizard.controller.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete/delete.action.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete/delete.action.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/row-actions.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/healthmonitors.module.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.module.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.module.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api/lbaasv2.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api/barbican.service.spec.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api/barbican.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/static/app/core/openstack-service-api/lbaasv2.service.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/urls.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/unit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/unit/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/unit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/unit/registration.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/unit
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/tests/settings.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/panel.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates/ngloadbalancersv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates/ngloadbalancersv2/index.html -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/templates/ngloadbalancersv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/urls.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/views.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ko_KR
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ko_KR/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/zh_CN
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/zh_CN/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ja
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ja/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/pt_BR
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/pt_BR/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/pt_BR/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/pt_BR/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/de
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/de/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/de/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/de/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/de/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/de/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/tr_TR
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/tr_TR/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/id
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/id/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/id/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/id/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/id/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/id/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ru
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/ru/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/fr
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/fr/LC_MESSAGES
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/en_GB
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES/django.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES/djangojs.po -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/locale/en_GB/LC_MESSAGES
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/karma.conf.js -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api
creating /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest/barbican.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest/lbaasv2.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/rest/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest
copying build/lib.linux-x86_64-2.7/neutron_lbaas_dashboard/api/__init__.py -> /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/enabled/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/enabled/_1481_project_ng_loadbalancersv2_panel.py to _1481_project_ng_loadbalancersv2_panel.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/urls.py to urls.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/unit/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/unit/registration.py to registration.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/tests/settings.py to settings.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/panel.py to panel.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/urls.py to urls.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/dashboards/project/ngloadbalancersv2/views.py to views.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest/barbican.py to barbican.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest/lbaasv2.py to lbaasv2.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/rest/__init__.py to __init__.pyc
byte-compiling /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/api/__init__.py to __init__.pyc
running install_egg_info
Copying neutron_lbaas_dashboard.egg-info to /usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard-4.0.1.dev9.egg-info
running install_scripts
+ cp -p neutron_lbaas_dashboard/enabled/_1481_project_ng_loadbalancersv2_panel.py /usr/share/openstack-dashboard/openstack_dashboard/local/enabled/
+ echo OPENSTACK_NEUTRON_NETWORK['enable_lb'] = True
+ /usr/share/openstack-dashboard/manage.py collectstatic --noinput
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.scss'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.module.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/lbaasv2.module.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators/validate-unique.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/util/validators/validate-unique.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.module.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/detail.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/table.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/loadbalancers.module.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/batch-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/row-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/batch-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/row-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip/modal.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/disassociate-ip/modal.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/edit/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/associate-ip/modal.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/create/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete/delete.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/loadbalancers/actions/delete/delete.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/detail.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/table.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.module.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/members.module.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/batch-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/row-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/batch-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/row-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/edit-weight/modal.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/update-member-list.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/members/actions/update-list/update-member-list.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/workflow.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/modal.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/model.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/workflow.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/model.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/modal.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/members/members.help.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.help.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/listener/listener.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/certificates/certificates.help.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool/pool.help.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/pool/pool.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/loadbalancer/loadbalancer.help.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.help.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/workflow/monitor/monitor.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/detail.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/table.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/listeners.module.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/listeners.module.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/batch-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/row-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/batch-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/row-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/edit/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/create/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete/delete.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/listeners/actions/delete/delete.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/pools.module.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/detail.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/pools.module.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/row-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/row-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/edit.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/edit/edit.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/create.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/create.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/create/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete/delete.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/pools/actions/delete/delete.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.directive.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/table/table-status.directive.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.directive.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/widgets/detail/detail-status.directive.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/healthmonitors.module.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.html'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/detail.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/healthmonitors.module.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/row-actions.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/row-actions.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/edit.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/edit/edit.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/create.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/wizard.controller.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/create.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/create/wizard.controller.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete/delete.action.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/dashboard/project/lbaasv2/healthmonitors/actions/delete/delete.action.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api/lbaasv2.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api/barbican.service.spec.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api/barbican.service.js'
Copying '/usr/local/lib/python2.7/dist-packages/neutron_lbaas_dashboard/static/app/core/openstack-service-api/lbaasv2.service.js'

145 static files copied to '/var/lib/openstack-dashboard/static', 2227 unmodified.
+ /usr/share/openstack-dashboard/manage.py compress
Found 'compress' tags in:
	/usr/share/openstack-dashboard/openstack_dashboard/templates/serial_console.html
	/usr/share/openstack-dashboard/openstack_dashboard/templates/horizon/_conf.html
	/usr/share/openstack-dashboard/openstack_dashboard/templates/_stylesheets.html
	/usr/share/openstack-dashboard/openstack_dashboard/templates/horizon/_scripts.html
Compressing... done
Compressed 8 block(s) from 4 template(s) for 3 context(s).
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ su -s /bin/sh -c neutron-db-manage --config-file /etc/neutron/neutron.conf --subproject neutron-lbaas upgrade head neutron
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
Running upgrade for neutron-lbaas ...
OK
+ service_restart nova-api
+ service=nova-api
+ [ 1 -eq 0 ]
+ systemctl restart nova-api
+ service_restart nova-scheduler
+ service=nova-scheduler
+ [ 1 -eq 0 ]
+ systemctl restart nova-scheduler
+ service_restart nova-conductor
+ service=nova-conductor
+ [ 1 -eq 0 ]
+ systemctl restart nova-conductor
+ service_restart neutron-server
+ service=neutron-server
+ [ 1 -eq 0 ]
+ systemctl restart neutron-server
+ service_enable neutron-server
+ service=neutron-server
+ [ 1 -eq 0 ]
+ systemctl enable neutron-server
Synchronizing state of neutron-server.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-server
+ echo NEUTRON_DBPASS="8271678362114f5f9c59"
+ echo NEUTRON_PASS="60bd8ad0f9e9306d6413"
+ echo NEUTRON_METADATA_SECRET="1cb9ce7786a31ef78c37"
+ logtend neutron
+ area=neutron
+ echo neutron
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron
+ date +%s
+ stamp=1556827767
+ date
+ date=Thu May  2 14:09:27 MDT 2019
+ eval tss=$LOGTIMESTART_neutron
+ tss=1556827693
+ expr 1556827767 - 1556827693
+ tsres=74
+ perl -e print 74 / 60.0 . "\n"
+ resmin=1.23333333333333
+ echo END neutron 1556827767 Thu May  2 14:09:27 MDT 2019
+ echo TOTAL neutron 74 1.23333333333333
+ [ -z  ]
+ logtstart neutron-networkmanager
+ area=neutron-networkmanager
+ echo neutron-networkmanager
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron_networkmanager
+ date +%s
+ stamp=1556827767
+ date
+ date=Thu May  2 14:09:27 MDT 2019
+ eval LOGTIMESTART_neutron_networkmanager=1556827767
+ LOGTIMESTART_neutron_networkmanager=1556827767
+ echo START neutron-networkmanager 1556827767 Thu May  2 14:09:27 MDT 2019
+ NEUTRON_NETWORKMANAGER_DONE=1
+ unified
+ [ controller = controller ]
+ return 0
+ echo *** Setting up unified networkmanager on controller
*** Setting up unified networkmanager on controller
+ /local/repository/setup-networkmanager.sh
+ [ -ne 0 ]
/local/repository/setup-networkmanager.sh: 10: [: -ne: unexpected operator
+ dirname /local/repository/setup-networkmanager.sh
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-networkmanager.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=b8cadc8a39e087ad420a
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x = x ]
+ UPDATING=0
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ cat /root/setup/manifests.0.xml
+ + perl -e $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } }xargs

DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ [ controller != controller ]
+ [ -f /root/setup/setup-networkmanager-done ]
+ logtstart networkmanager
+ area=networkmanager
+ echo networkmanager
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=networkmanager
+ date +%s
+ stamp=1556827767
+ date
+ date=Thu May  2 14:09:27 MDT 2019
+ eval LOGTIMESTART_networkmanager=1556827767
+ LOGTIMESTART_networkmanager=1556827767
+ echo START networkmanager 1556827767 Thu May  2 14:09:27 MDT 2019
+ [ -f /root/setup/settings ]
+ . /root/setup/settings
+ GENIUSER=1
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ STORAGEHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=compute-1 
+ MIRRORSETUP=1
+ DB_ROOT_PASS=26e250af84ca2965f0e6
+ RABBIT_USER=openstack
+ RABBIT_PASS=c00d09297284c93378d5
+ RABBIT_URL=rabbit://openstack:c00d09297284c93378d5@controller
+ MEMCACHE_DONE=1
+ ADMIN_API=adminapi
+ ADMIN_API_PASS=a9f3d26fda5b4ee3d9a8
+ KEYSTONE_DBPASS=0a4f4c06d3fefbade0ee
+ GLANCE_DBPASS=a6614c0139dcbb0d7b6c
+ GLANCE_PASS=a3e74e251a8755f42955
+ NOVA_DBPASS=c959955373a27ee1342a
+ NOVA_PASS=1da42256777cafc9360e
+ PLACEMENT_PASS=4eb4cb430c0078c3cc6b
+ NOVA_COMPUTENODES_DONE=1
+ NEUTRON_DBPASS=8271678362114f5f9c59
+ NEUTRON_PASS=60bd8ad0f9e9306d6413
+ NEUTRON_METADATA_SECRET=1cb9ce7786a31ef78c37
+ /local/repository/setup-network-plugin.sh
+ [ -ne 0 ]
/local/repository/setup-network-plugin.sh: 10: [: -ne: unexpected operator
+ dirname /local/repository/setup-network-plugin.sh
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-network-plugin.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=f9400cfcec21e13479b8
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x = x ]
+ UPDATING=0
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ + perl -e $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } }cat
+  /root/setup/manifests.0.xml
xargs
DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ [ -f /root/setup/setup-network-plugin-done ]
+ logtstart network-plugin
+ area=network-plugin
+ echo network-plugin
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=network_plugin
+ date +%s
+ stamp=1556827767
+ date
+ date=Thu May  2 14:09:27 MDT 2019
+ eval LOGTIMESTART_network_plugin=1556827767
+ LOGTIMESTART_network_plugin=1556827767
+ echo START network-plugin 1556827767 Thu May  2 14:09:27 MDT 2019
+ [ -f /root/setup/settings ]
+ . /root/setup/settings
+ GENIUSER=1
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ STORAGEHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=compute-1 
+ MIRRORSETUP=1
+ DB_ROOT_PASS=26e250af84ca2965f0e6
+ RABBIT_USER=openstack
+ RABBIT_PASS=c00d09297284c93378d5
+ RABBIT_URL=rabbit://openstack:c00d09297284c93378d5@controller
+ MEMCACHE_DONE=1
+ ADMIN_API=adminapi
+ ADMIN_API_PASS=a9f3d26fda5b4ee3d9a8
+ KEYSTONE_DBPASS=0a4f4c06d3fefbade0ee
+ GLANCE_DBPASS=a6614c0139dcbb0d7b6c
+ GLANCE_PASS=a3e74e251a8755f42955
+ NOVA_DBPASS=c959955373a27ee1342a
+ NOVA_PASS=1da42256777cafc9360e
+ PLACEMENT_PASS=4eb4cb430c0078c3cc6b
+ NOVA_COMPUTENODES_DONE=1
+ NEUTRON_DBPASS=8271678362114f5f9c59
+ NEUTRON_PASS=60bd8ad0f9e9306d6413
+ NEUTRON_METADATA_SECRET=1cb9ce7786a31ef78c37
+ [ -f /root/setup/settings.local ]
+ . /root/setup/settings.local
+ STORAGEDIR=/storage
+ VGNAME=openstack-volumes
+ VGTOTAL=456
+ LVM=1
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ CINDER_LV_SIZE=317.25
+ [ openvswitch = linuxbridge ]
+ /local/repository/setup-network-plugin-openvswitch.sh
+ [ -ne 0 ]
/local/repository/setup-network-plugin-openvswitch.sh: 10: [: -ne: unexpected operator
+ dirname /local/repository/setup-network-plugin-openvswitch.sh
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-network-plugin-openvswitch.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=0db87dc8c5445f199ec2
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x = x ]
+ UPDATING=0
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ + cat /root/setup/manifests.0.xml
perl -e $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } }
+ xargs
DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ [ -f /root/setup/setup-network-plugin-openvswitch-done ]
+ logtstart network-plugin-openvswitch
+ area=network-plugin-openvswitch
+ echo network-plugin-openvswitch
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=network_plugin_openvswitch
+ date +%s
+ stamp=1556827768
+ date
+ date=Thu May  2 14:09:28 MDT 2019
+ eval LOGTIMESTART_network_plugin_openvswitch=1556827768
+ LOGTIMESTART_network_plugin_openvswitch=1556827768
+ echo START network-plugin-openvswitch 1556827768 Thu May  2 14:09:28 MDT 2019
+ [ -f /root/setup/settings ]
+ . /root/setup/settings
+ GENIUSER=1
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ STORAGEHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=compute-1 
+ MIRRORSETUP=1
+ DB_ROOT_PASS=26e250af84ca2965f0e6
+ RABBIT_USER=openstack
+ RABBIT_PASS=c00d09297284c93378d5
+ RABBIT_URL=rabbit://openstack:c00d09297284c93378d5@controller
+ MEMCACHE_DONE=1
+ ADMIN_API=adminapi
+ ADMIN_API_PASS=a9f3d26fda5b4ee3d9a8
+ KEYSTONE_DBPASS=0a4f4c06d3fefbade0ee
+ GLANCE_DBPASS=a6614c0139dcbb0d7b6c
+ GLANCE_PASS=a3e74e251a8755f42955
+ NOVA_DBPASS=c959955373a27ee1342a
+ NOVA_PASS=1da42256777cafc9360e
+ PLACEMENT_PASS=4eb4cb430c0078c3cc6b
+ NOVA_COMPUTENODES_DONE=1
+ NEUTRON_DBPASS=8271678362114f5f9c59
+ NEUTRON_PASS=60bd8ad0f9e9306d6413
+ NEUTRON_METADATA_SECRET=1cb9ce7786a31ef78c37
+ [ -f /root/setup/settings.local ]
+ . /root/setup/settings.local
+ STORAGEDIR=/storage
+ VGNAME=openstack-volumes
+ VGTOTAL=456
+ LVM=1
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ CINDER_LV_SIZE=317.25
+ . /root/setup/neutron.vars
+ network_types=flat,gre,vxlan
+ flat_networks=external,flat-lan-1
+ bridge_mappings=bridge_mappings=external:br-ex,flat-lan-1:br-flat-lan-1
+ extra_mappings=
+ network_vlan_ranges=network_vlan_ranges=
+ gre_local_ip=local_ip = 10.11.10.1
+ enable_tunneling=enable_tunneling = True
+ tunnel_types=tunnel_types = gre
+ interface_driver=neutron.agent.linux.interface.OVSInterfaceDriver
+ fwdriver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
+ cat
+ sysctl -p
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
+ maybe_install_packages neutron-plugin-ml2 neutron-plugin-openvswitch-agent conntrack
+ [ ! 0 -eq 0 ]
+ are_packages_installed neutron-plugin-ml2 neutron-plugin-openvswitch-agent conntrack
+ retval=1
+ [ ! -z neutron-plugin-ml2 ]
+ dpkg -s neutron-plugin-ml2
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z neutron-plugin-openvswitch-agent ]
+ dpkg -s neutron-plugin-openvswitch-agent
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z conntrack ]
+ dpkg -s conntrack
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ controller != controller ]
+ crudini --del /etc/neutron/neutron.conf keystone_authtoken auth_host
+ crudini --del /etc/neutron/neutron.conf keystone_authtoken auth_port
+ crudini --del /etc/neutron/neutron.conf keystone_authtoken auth_protocol
+ crudini --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/neutron/neutron.conf DEFAULT verbose False
+ crudini --set /etc/neutron/neutron.conf DEFAULT debug False
+ crudini --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ crudini --set /etc/neutron/neutron.conf DEFAULT service_plugins router,metering,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
+ crudini --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True
+ crudini --set /etc/neutron/neutron.conf DEFAULT notification_driver messagingv2
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 11 ]
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken auth_type password
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken project_name service
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken username neutron
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken password 60bd8ad0f9e9306d6413
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller:11211
+ [ 17 -ge 15 ]
+ crudini --set /etc/neutron/neutron.conf placement os_region_name RegionOne
+ crudini --set /etc/neutron/neutron.conf placement auth_url http://controller:5000/v3
+ crudini --set /etc/neutron/neutron.conf placement auth_type password
+ crudini --set /etc/neutron/neutron.conf placement project_domain_name default
+ crudini --set /etc/neutron/neutron.conf placement user_domain_name default
+ crudini --set /etc/neutron/neutron.conf placement project_name service
+ crudini --set /etc/neutron/neutron.conf placement username placement
+ crudini --set /etc/neutron/neutron.conf placement password 4eb4cb430c0078c3cc6b
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,gre,vxlan
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types flat,gre,vxlan
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch
+ extdrivers=
+ [ 17 -ge 14 ]
+ extdrivers=dns
+ [ -n dns ]
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers dns
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks external,flat-lan-1
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_gre tunnel_id_ranges 1:1000
+ cat
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 3000:4000
+ crudini --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_security_group True
+ crudini --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_ipset True
+ [ -n neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver ]
+ crudini --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group True
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True
+ [ -n neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver ]
+ crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
+ cat
+ [ 17 -ge 13 ]
+ cat
+ echo 128.110.154.185    controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us hp104.utah.cloudlab.us
+ [ 17 -le 12 ]
+ patch -d / -p0
patching file /usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/openvswitch/agent/openflow/native/ofswitch.py
patching file /usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/openvswitch/agent/openflow/br_cookie.py
+ [ 17 -ge 12 -a 17 -lt 14 ]
+ modprobe bridge
+ echo bridge
+ service_restart openvswitch-switch
+ service=openvswitch-switch
+ [ 1 -eq 0 ]
+ systemctl restart openvswitch-switch
+ service_enable openvswitch-switch
+ service=openvswitch-switch
+ [ 1 -eq 0 ]
+ systemctl enable openvswitch-switch
Synchronizing state of openvswitch-switch.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable openvswitch-switch
+ service_restart nova-compute
+ service=nova-compute
+ [ 1 -eq 0 ]
+ systemctl restart nova-compute
Failed to restart nova-compute.service: Unit nova-compute.service not found.
+ service_restart neutron-ovs-cleanup
+ service=neutron-ovs-cleanup
+ [ 1 -eq 0 ]
+ systemctl restart neutron-ovs-cleanup
+ service_enable neutron-ovs-cleanup
+ service=neutron-ovs-cleanup
+ [ 1 -eq 0 ]
+ systemctl enable neutron-ovs-cleanup
Synchronizing state of neutron-ovs-cleanup.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-ovs-cleanup
+ [ 17 -lt 13 ]
+ service_restart neutron-openvswitch-agent
+ service=neutron-openvswitch-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-openvswitch-agent
+ service_enable neutron-openvswitch-agent
+ service=neutron-openvswitch-agent
+ [ 1 -eq 0 ]
+ systemctl enable neutron-openvswitch-agent
Synchronizing state of neutron-openvswitch-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-openvswitch-agent
+ [ 17 -gt 12 ]
+ echo *** Re-adding OVS anti-spoofing flows with reserved cookie...
*** Re-adding OVS anti-spoofing flows with reserved cookie...
+ i=30
+ [ ! -f /var/lib/neutron/ovs-default-flows.reserved_cookie -a 30 -gt 0 ]
+ sleep 1
+ expr 30 - 1
+ i=29
+ [ ! -f /var/lib/neutron/ovs-default-flows.reserved_cookie -a 29 -gt 0 ]
+ sleep 1
+ expr 29 - 1
+ i=28
+ [ ! -f /var/lib/neutron/ovs-default-flows.reserved_cookie -a 28 -gt 0 ]
+ sleep 5
+ service_restart neutron-openvswitch-agent
+ service=neutron-openvswitch-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-openvswitch-agent
+ sleep 5
+ [ -f /var/lib/neutron/ovs-default-flows.reserved_cookie -a -f /etc/neutron/ovs-default-flows/br-ex ]
+ cat /var/lib/neutron/ovs-default-flows.reserved_cookie
+ cookie=4404206835508906427
+ cat /etc/neutron/ovs-default-flows/br-ex
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.152.1,in_port=1,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.152.1,in_port=1,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.154.185,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.154.185,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.158,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.158,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.159,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.159,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.160,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.160,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.161,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.155.161,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.154.185/22,in_port=1,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.154.185/22,in_port=1,actions=NORMAL
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.154.185/22,actions=drop
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=128.110.154.185/22,actions=drop
+ echo cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=172.16.0.0/12,actions=drop
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,dl_type=0x0806,nw_proto=0x2,arp_spa=172.16.0.0/12,actions=drop
+ echo cookie=4404206835508906427,priority=0,actions=NORMAL
+ ovs-ofctl add-flow br-ex cookie=4404206835508906427,priority=0,actions=NORMAL
+ mv /etc/neutron/ovs-default-flows/br-ex.tmp /etc/neutron/ovs-default-flows/br-ex
+ touch /root/setup/setup-network-plugin-openvswitch-done
+ logtend network-plugin-openvswitch
+ area=network-plugin-openvswitch
+ echo network-plugin-openvswitch
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=network_plugin_openvswitch
+ date +%s
+ stamp=1556827785
+ date
+ date=Thu May  2 14:09:45 MDT 2019
+ eval tss=$LOGTIMESTART_network_plugin_openvswitch
+ tss=1556827768
+ expr 1556827785 - 1556827768
+ tsres=17
+ perl -e print 17 / 60.0 . "\n"
+ resmin=0.283333333333333
+ echo END network-plugin-openvswitch 1556827785 Thu May  2 14:09:45 MDT 2019
+ echo TOTAL network-plugin-openvswitch 17 0.283333333333333
+ exit 0
+ [ 0 -eq 0 ]
+ touch /root/setup/setup-network-plugin-done
+ logtend network-plugin
+ area=network-plugin
+ echo network-plugin
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=network_plugin
+ date +%s
+ stamp=1556827785
+ date
+ date=Thu May  2 14:09:45 MDT 2019
+ eval tss=$LOGTIMESTART_network_plugin
+ tss=1556827767
+ expr 1556827785 - 1556827767
+ tsres=18
+ perl -e print 18 / 60.0 . "\n"
+ resmin=0.3
+ echo END network-plugin 1556827785 Thu May  2 14:09:45 MDT 2019
+ echo TOTAL network-plugin 18 0.3
+ exit 0
+ . /root/setup/neutron.vars
+ network_types=flat,gre,vxlan
+ flat_networks=external,flat-lan-1
+ bridge_mappings=bridge_mappings=external:br-ex,flat-lan-1:br-flat-lan-1
+ extra_mappings=
+ network_vlan_ranges=network_vlan_ranges=
+ gre_local_ip=local_ip = 10.11.10.1
+ enable_tunneling=enable_tunneling = True
+ tunnel_types=tunnel_types = gre
+ interface_driver=neutron.agent.linux.interface.OVSInterfaceDriver
+ fwdriver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
+ [ 17 -ge 11 ]
+ maybe_install_packages python-oslo.service
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-oslo.service
+ retval=1
+ [ ! -z python-oslo.service ]
+ dpkg -s python-oslo.service
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ patch -d / -p0
patching file /usr/lib/python2.7/dist-packages/oslo_service/service.py
Reversed (or previously applied) patch detected!  Assume -R? [n] 
Apply anyway? [n] 
Skipping patch.
1 out of 1 hunk ignored -- saving rejects to file /usr/lib/python2.7/dist-packages/oslo_service/service.py.rej
+ cat
+ sysctl -p
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
+ maybe_install_packages neutron-l3-agent neutron-dhcp-agent neutron-metering-agent
+ [ ! 0 -eq 0 ]
+ are_packages_installed neutron-l3-agent neutron-dhcp-agent neutron-metering-agent
+ retval=1
+ [ ! -z neutron-l3-agent ]
+ dpkg -s neutron-l3-agent
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z neutron-dhcp-agent ]
+ dpkg -s neutron-dhcp-agent
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z neutron-metering-agent ]
+ dpkg -s neutron-metering-agent
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ maybe_install_packages neutron-lbaasv2-agent
+ [ ! 0 -eq 0 ]
+ are_packages_installed neutron-lbaasv2-agent
+ retval=1
+ [ ! -z neutron-lbaasv2-agent ]
+ dpkg -s neutron-lbaasv2-agent
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ crudini --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver
+ crudini --set /etc/neutron/l3_agent.ini DEFAULT use_namespaces True
+ [ openvswitch = openvswitch ]
+ crudini --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge br-ex
+ crudini --set /etc/neutron/l3_agent.ini DEFAULT verbose False
+ crudini --set /etc/neutron/l3_agent.ini DEFAULT debug False
+ crudini --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver
+ crudini --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
+ [ openvswitch = openvswitch ]
+ crudini --set /etc/neutron/dhcp_agent.ini DEFAULT use_namespaces True
+ crudini --set /etc/neutron/dhcp_agent.ini DEFAULT verbose False
+ crudini --set /etc/neutron/dhcp_agent.ini DEFAULT debug False
+ crudini --set /etc/neutron/dhcp_agent.ini DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq-neutron.conf
+ cat
+ pkill dnsmasq
+ [ 17 -lt 11 ]
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT auth_uri http://controller:5000
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT auth_url http://controller:5000/v2.0
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT auth_region RegionOne
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT auth_type password
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT project_domain_name default
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT user_domain_name default
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT project_name service
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT username neutron
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT password 60bd8ad0f9e9306d6413
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT admin_tenant_name service
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT admin_user neutron
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT admin_password 60bd8ad0f9e9306d6413
+ [ 17 -lt 16 ]
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_host controller
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret 1cb9ce7786a31ef78c37
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT verbose False
+ crudini --set /etc/neutron/metadata_agent.ini DEFAULT debug False
+ crudini --set /etc/neutron/metering_agent.ini DEFAULT debug True
+ crudini --set /etc/neutron/metering_agent.ini DEFAULT driver neutron.services.metering.drivers.iptables.iptables_driver.IptablesMeteringDriver
+ crudini --set /etc/neutron/metering_agent.ini DEFAULT measure_interval 30
+ crudini --set /etc/neutron/metering_agent.ini DEFAULT report_interval 300
+ crudini --set /etc/neutron/metering_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver
+ crudini --set /etc/neutron/metering_agent.ini DEFAULT use_namespaces True
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ crudini --set /etc/neutron/lbaas_agent.ini DEFAULT device_driver neutron_lbaas.drivers.haproxy.namespace_driver.HaproxyNSDriver
+ [ openvswitch = linuxbridge ]
+ crudini --set /etc/neutron/lbaas_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriver
+ crudini --set /etc/neutron/lbaas_agent.ini haproxy user_group haproxy
+ crudini --set /etc/neutron/neutron_lbaas.conf service_providers service_provider LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default
+ service_restart neutron-l3-agent
+ service=neutron-l3-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-l3-agent
+ service_enable neutron-l3-agent
+ service=neutron-l3-agent
+ [ 1 -eq 0 ]
+ systemctl enable neutron-l3-agent
Synchronizing state of neutron-l3-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-l3-agent
+ service_restart neutron-dhcp-agent
+ service=neutron-dhcp-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-dhcp-agent
+ service_enable neutron-dhcp-agent
+ service=neutron-dhcp-agent
+ [ 1 -eq 0 ]
+ systemctl enable neutron-dhcp-agent
Synchronizing state of neutron-dhcp-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-dhcp-agent
+ service_restart neutron-metadata-agent
+ service=neutron-metadata-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-metadata-agent
+ service_enable neutron-metadata-agent
+ service=neutron-metadata-agent
+ [ 1 -eq 0 ]
+ systemctl enable neutron-metadata-agent
Synchronizing state of neutron-metadata-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-metadata-agent
+ service_restart neutron-metering-agent
+ service=neutron-metering-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-metering-agent
+ service_enable neutron-metering-agent
+ service=neutron-metering-agent
+ [ 1 -eq 0 ]
+ systemctl enable neutron-metering-agent
Synchronizing state of neutron-metering-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-metering-agent
+ [ 1 -eq 1 -a 17 -ge 14 ]
+ service_restart neutron-lbaasv2-agent
+ service=neutron-lbaasv2-agent
+ [ 1 -eq 0 ]
+ systemctl restart neutron-lbaasv2-agent
+ service_enable neutron-lbaasv2-agent
+ service=neutron-lbaasv2-agent
+ [ 1 -eq 0 ]
+ systemctl enable neutron-lbaasv2-agent
Synchronizing state of neutron-lbaasv2-agent.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable neutron-lbaasv2-agent
+ touch /root/setup/setup-networkmanager-done
+ logtend networkmanager
+ area=networkmanager
+ echo networkmanager
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=networkmanager
+ date +%s
+ stamp=1556827788
+ date
+ date=Thu May  2 14:09:48 MDT 2019
+ eval tss=$LOGTIMESTART_networkmanager
+ tss=1556827767
+ expr 1556827788 - 1556827767
+ tsres=21
+ perl -e print 21 / 60.0 . "\n"
+ resmin=0.35
+ echo END networkmanager 1556827788 Thu May  2 14:09:48 MDT 2019
+ echo TOTAL networkmanager 21 0.35
+ exit 0
+ echo NEUTRON_NETWORKMANAGER_DONE="1"
+ logtend neutron-networkmanager
+ area=neutron-networkmanager
+ echo neutron-networkmanager
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron_networkmanager
+ date +%s
+ stamp=1556827788
+ date
+ date=Thu May  2 14:09:48 MDT 2019
+ eval tss=$LOGTIMESTART_neutron_networkmanager
+ tss=1556827767
+ expr 1556827788 - 1556827767
+ tsres=21
+ perl -e print 21 / 60.0 . "\n"
+ resmin=0.35
+ echo END neutron-networkmanager 1556827788 Thu May  2 14:09:48 MDT 2019
+ echo TOTAL neutron-networkmanager 21 0.35
+ [ -z  ]
+ logtstart neutron-computenodes
+ area=neutron-computenodes
+ echo neutron-computenodes
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron_computenodes
+ date +%s
+ stamp=1556827788
+ date
+ date=Thu May  2 14:09:48 MDT 2019
+ eval LOGTIMESTART_neutron_computenodes=1556827788
+ LOGTIMESTART_neutron_computenodes=1556827788
+ echo START neutron-computenodes 1556827788 Thu May  2 14:09:48 MDT 2019
+ NEUTRON_COMPUTENODES_DONE=1
+ PHOSTS=
+ mkdir -p /root/setup/pssh.setup-compute-network.stdout /root/setup/pssh.setup-compute-network.stderr
+ getfqdn compute-1
+ n=compute-1
+ cat /root/setup/fqdn.map
+ grep -E compute-1\s
+ cut -f2
+ fqdn=compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ echo compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ fqdn=compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ scp -o StrictHostKeyChecking=no /root/setup/settings compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us:/root/setup/settings
+ PHOSTS= -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ echo *** Setting up Compute network on nodes:  -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
*** Setting up Compute network on nodes:  -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ /usr/bin/parallel-ssh -t 0 -O StrictHostKeyChecking=no -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us -o /root/setup/pssh.setup-compute-network.stdout -e /root/setup/pssh.setup-compute-network.stderr /local/repository/setup-compute-network.sh
[1] 14:10:13 [SUCCESS] compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ touch /root/setup/compute-network-done-compute-1
+ service_restart neutron-server
+ service=neutron-server
+ [ 1 -eq 0 ]
+ systemctl restart neutron-server
+ retries=30
+ [ 30 -gt 0 ]
+ neutron net-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
Connection to neutron failed: Failed to connect Neutron server
+ [ 1 -eq 0 ]
+ sleep 2
+ expr 30 - 1
+ retries=29
+ [ 29 -gt 0 ]
+ neutron net-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.

+ [ 0 -eq 0 ]
+ break
+ echo NEUTRON_COMPUTENODES_DONE="1"
+ logtend neutron-computenodes
+ area=neutron-computenodes
+ echo neutron-computenodes
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron_computenodes
+ date +%s
+ stamp=1556827819
+ date
+ date=Thu May  2 14:10:19 MDT 2019
+ eval tss=$LOGTIMESTART_neutron_computenodes
+ tss=1556827788
+ expr 1556827819 - 1556827788
+ tsres=31
+ perl -e print 31 / 60.0 . "\n"
+ resmin=0.516666666666667
+ echo END neutron-computenodes 1556827819 Thu May  2 14:10:19 MDT 2019
+ echo TOTAL neutron-computenodes 31 0.516666666666667
+ [ -z  ]
+ logtstart neutron-network-ext-float
+ area=neutron-network-ext-float
+ echo neutron-network-ext-float
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron_network_ext_float
+ date +%s
+ stamp=1556827819
+ date
+ date=Thu May  2 14:10:19 MDT 2019
+ eval LOGTIMESTART_neutron_network_ext_float=1556827819
+ LOGTIMESTART_neutron_network_ext_float=1556827819
+ echo START neutron-network-ext-float 1556827819 Thu May  2 14:10:19 MDT 2019
+ NEUTRON_NETWORKS_DONE=1
+ [ queens = kilo -o queens = liberty ]
+ neutron net-create ext-net --shared --router:external True --provider:physical_network external --provider:network_type flat
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2019-05-02T20:10:20Z                 |
| description               |                                      |
| dns_domain                |                                      |
| id                        | c8ce419e-3b1e-4ec5-967c-94b74378014f |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| is_default                | False                                |
| mtu                       | 1500                                 |
| name                      | ext-net                              |
| project_id                | a3b1d2ed83f74877a4ef333e937e3474     |
| provider:network_type     | flat                                 |
| provider:physical_network | external                             |
| provider:segmentation_id  |                                      |
| revision_number           | 4                                    |
| router:external           | True                                 |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | a3b1d2ed83f74877a4ef333e937e3474     |
| updated_at                | 2019-05-02T20:10:20Z                 |
+---------------------------+--------------------------------------+
+ . /root/setup/ctlnet.vars
+ ctlip=128.110.154.185
+ ctlmac=98:f2:b3:cc:32:80
+ ctlstrippedmac=98f2b3cc3280
+ ctlnetmask=255.255.252.0
+ ctlgw=128.110.152.1
+ ctlnet=128.110.152.0/22
+ ctlprefix=22
+ neutron subnet-create ext-net --name ext-subnet --disable-dhcp --gateway 128.110.152.1 128.110.152.0/22
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
Created a new subnet:
+-------------------+------------------------------------------------------+
| Field             | Value                                                |
+-------------------+------------------------------------------------------+
| allocation_pools  | {"start": "128.110.152.2", "end": "128.110.155.254"} |
| cidr              | 128.110.152.0/22                                     |
| created_at        | 2019-05-02T20:10:22Z                                 |
| description       |                                                      |
| dns_nameservers   |                                                      |
| enable_dhcp       | False                                                |
| gateway_ip        | 128.110.152.1                                        |
| host_routes       |                                                      |
| id                | 2b4dadcc-6d6a-45cb-b644-e9752db06273                 |
| ip_version        | 4                                                    |
| ipv6_address_mode |                                                      |
| ipv6_ra_mode      |                                                      |
| name              | ext-subnet                                           |
| network_id        | c8ce419e-3b1e-4ec5-967c-94b74378014f                 |
| project_id        | a3b1d2ed83f74877a4ef333e937e3474                     |
| revision_number   | 0                                                    |
| service_types     |                                                      |
| subnetpool_id     |                                                      |
| tags              |                                                      |
| tenant_id         | a3b1d2ed83f74877a4ef333e937e3474                     |
| updated_at        | 2019-05-02T20:10:22Z                                 |
+-------------------+------------------------------------------------------+
+ neutron subnet-show ext-subnet
+ awk / id / {print $4}
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+ SID=2b4dadcc-6d6a-45cb-b644-e9752db06273
+ [ ! -z 2b4dadcc-6d6a-45cb-b644-e9752db06273 ]
+ + mysql --password=26e250af84ca2965f0e6 neutron
echo delete from ipallocationpools where subnet_id='2b4dadcc-6d6a-45cb-b644-e9752db06273'
+ echo insert into ipallocationpools values (UUID(),'2b4dadcc-6d6a-45cb-b644-e9752db06273','128.110.155.158','128.110.155.158')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipallocationpools values (UUID(),'2b4dadcc-6d6a-45cb-b644-e9752db06273','128.110.155.159','128.110.155.159')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipallocationpools values (UUID(),'2b4dadcc-6d6a-45cb-b644-e9752db06273','128.110.155.160','128.110.155.160')+ 
mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipallocationpools values (UUID(),'2b4dadcc-6d6a-45cb-b644-e9752db06273','128.110.155.161','128.110.155.161')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ [ 17 -ge 14 ]
+ echo select id from ipamsubnets where neutron_subnet_id='2b4dadcc-6d6a-45cb-b644-e9752db06273'
+ mysql -N --password=8271678362114f5f9c59 neutron
+ IPAMSID=881ba619-f3b8-41ab-8e52-08235cf2e878
+ [ -z 881ba619-f3b8-41ab-8e52-08235cf2e878 ]
+ echo delete from ipamallocationpools where ipam_subnet_id='881ba619-f3b8-41ab-8e52-08235cf2e878'
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipamallocationpools values (UUID(),'881ba619-f3b8-41ab-8e52-08235cf2e878','128.110.155.158','128.110.155.158')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipamallocationpools values (UUID(),'881ba619-f3b8-41ab-8e52-08235cf2e878','128.110.155.159','128.110.155.159')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipamallocationpools values (UUID(),'881ba619-f3b8-41ab-8e52-08235cf2e878','128.110.155.160','128.110.155.160')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo insert into ipamallocationpools values (UUID(),'881ba619-f3b8-41ab-8e52-08235cf2e878','128.110.155.161','128.110.155.161')
+ mysql --password=26e250af84ca2965f0e6 neutron
+ echo NEUTRON_NETWORKS_DONE="1"
+ logtend neutron-network-ext-float
+ area=neutron-network-ext-float
+ echo neutron-network-ext-float
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=neutron_network_ext_float
+ date +%s
+ stamp=1556827824
+ date
+ date=Thu May  2 14:10:24 MDT 2019
+ eval tss=$LOGTIMESTART_neutron_network_ext_float
+ tss=1556827819
+ expr 1556827824 - 1556827819
+ tsres=5
+ perl -e print 5 / 60.0 . "\n"
+ resmin=0.0833333333333333
+ echo END neutron-network-ext-float 1556827824 Thu May  2 14:10:24 MDT 2019
+ echo TOTAL neutron-network-ext-float 5 0.0833333333333333
+ [ -z  ]
+ logtstart horizon
+ area=horizon
+ echo horizon
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=horizon
+ date +%s
+ stamp=1556827824
+ date
+ date=Thu May  2 14:10:24 MDT 2019
+ eval LOGTIMESTART_horizon=1556827824
+ LOGTIMESTART_horizon=1556827824
+ echo START horizon 1556827824 Thu May  2 14:10:24 MDT 2019
+ DASHBOARD_DONE=1
+ maybe_install_packages openstack-dashboard apache2 libapache2-mod-wsgi
+ [ ! 0 -eq 0 ]
+ are_packages_installed openstack-dashboard apache2 libapache2-mod-wsgi
+ retval=1
+ [ ! -z openstack-dashboard ]
+ dpkg -s openstack-dashboard
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z apache2 ]
+ dpkg -s apache2
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z libapache2-mod-wsgi ]
+ dpkg -s libapache2-mod-wsgi
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ echo 
+ sed -i -e s/OPENSTACK_HOST.*=.*$/OPENSTACK_HOST = "controller"/ /etc/openstack-dashboard/local_settings.py
+ sed -i -e s/^.*ALLOWED_HOSTS = \[.*$/ALLOWED_HOSTS = \["*"\]/ /etc/openstack-dashboard/local_settings.py
+ grep -q ^#[ \t]*SESSION_TIMEOUT /etc/openstack-dashboard/local_settings.py
+ [ 1 -eq 0 ]
+ echo SESSION_TIMEOUT = 14400
+ grep -q ^#[ \t]*OPENSTACK_KEYSTONE_DEFAULT_ROLE /etc/openstack-dashboard/local_settings.py
+ [ 1 -eq 0 ]
+ echo OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
+ [ 17 -ge 11 ]
+ cat
+ [ 17 -ge 13 ]
+ cat
+ [ x3 = x3 ]
+ IDVERS=3
+ grep ^#[ \t]*OPENSTACK_KEYSTONE_URL /etc/openstack-dashboard/local_settings.py
#    OPENSTACK_KEYSTONE_URL: 'RegionOne'
+ [ 0 -eq 0 ]
+ sed -i -e s|^#.*OPENSTACK_KEYSTONE_URL.*=.*$|OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST| /etc/openstack-dashboard/local_settings.py
+ grep ^#[ \t]*OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT /etc/openstack-dashboard/local_settings.py
#OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = False
+ [ 0 -eq 0 ]
+ sed -i -e s|^#.*OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT.*=.*$|OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True| /etc/openstack-dashboard/local_settings.py
+ grep ^#[ \t]*OPENSTACK_KEYSTONE_DEFAULT_DOMAIN /etc/openstack-dashboard/local_settings.py
#OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default'
+ [ 0 -eq 0 ]
+ sed -i -e s|^#.*OPENSTACK_KEYSTONE_DEFAULT_DOMAIN.*=.*$|OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'default'| /etc/openstack-dashboard/local_settings.py
+ [ 17 -ge 13 ]
+ IMAGEVERS="image": 2,
+ cat
+ [ 17 -eq 15 ]
+ [ 17 -ge 16 ]
+ chown horizon.www-data /var/lib/openstack-dashboard/secret_key
+ [ 17 -ge 14 ]
+ patch -p0 -d /
patching file /usr/share/openstack-dashboard/openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/launch-instance-model.service.js
Hunk #1 succeeded at 201 (offset 1 line).
+ /usr/share/openstack-dashboard/manage.py collectstatic --noinput
Copying '/usr/share/openstack-dashboard/openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/launch-instance-model.service.js.orig'
Copying '/usr/share/openstack-dashboard/openstack_dashboard/dashboards/project/static/dashboard/project/workflow/launch-instance/launch-instance-model.service.js'

2 static files copied to '/var/lib/openstack-dashboard/static', 2371 unmodified.
+ /usr/share/openstack-dashboard/manage.py compress
Found 'compress' tags in:
	/usr/share/openstack-dashboard/openstack_dashboard/templates/serial_console.html
	/usr/share/openstack-dashboard/openstack_dashboard/templates/horizon/_conf.html
	/usr/share/openstack-dashboard/openstack_dashboard/templates/_stylesheets.html
	/usr/share/openstack-dashboard/openstack_dashboard/templates/horizon/_scripts.html
Compressing... done
Compressed 8 block(s) from 4 template(s) for 3 context(s).
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ service_enable apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl enable apache2
Synchronizing state of apache2.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable apache2
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ echo DASHBOARD_DONE="1"
+ logtend horizon
+ area=horizon
+ echo horizon
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=horizon
+ date +%s
+ stamp=1556827875
+ date
+ date=Thu May  2 14:11:15 MDT 2019
+ eval tss=$LOGTIMESTART_horizon
+ tss=1556827824
+ expr 1556827875 - 1556827824
+ tsres=51
+ perl -e print 51 / 60.0 . "\n"
+ resmin=0.85
+ echo END horizon 1556827875 Thu May  2 14:11:15 MDT 2019
+ echo TOTAL horizon 51 0.85
+ [ -z  ]
+ logtstart cinder
+ area=cinder
+ echo cinder
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=cinder
+ date +%s
+ stamp=1556827875
+ date
+ date=Thu May  2 14:11:15 MDT 2019
+ eval LOGTIMESTART_cinder=1556827875
+ LOGTIMESTART_cinder=1556827875
+ echo START cinder 1556827875 Thu May  2 14:11:15 MDT 2019
+ openssl rand -hex 10
+ CINDER_DBPASS=4b6df6d901da295f578c
+ openssl rand -hex 10
+ CINDER_PASS=bcabcfa033c81284dec0
+ echo create database cinder
+ mysql -u root --password=26e250af84ca2965f0e6
+ + echo grant all privileges on cinder.* to 'cinder'@'localhost' identified by '4b6df6d901da295f578c'
mysql -u root --password=26e250af84ca2965f0e6
+ + echo grant all privileges on cinder.* to 'cinder'@'%' identified by '4b6df6d901da295f578c'
mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password bcabcfa033c81284dec0 cinder
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password bcabcfa033c81284dec0 cinder
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | c82730831c114eaa9ebebe114791709e |
| name                | cinder                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user cinder --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user cinder --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name cinder --description OpenStack Block Storage Service volume
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name cinder --description OpenStack Block Storage Service volume
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage Service  |
| enabled     | True                             |
| id          | 3e44f262cefe4b79abf1175de5267dad |
| name        | cinder                           |
| type        | volume                           |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name cinderv2 --description OpenStack Block Storage Service volumev2
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name cinderv2 --description OpenStack Block Storage Service volumev2
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage Service  |
| enabled     | True                             |
| id          | a723a5f6db5149e5b6eaf450424ed835 |
| name        | cinderv2                         |
| type        | volumev2                         |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ [ 17 -lt 15 ]
+ __openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%(project_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%(project_id)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 3f29cea589ae4cdfa2b7a057fa51bd04         |
| interface    | public                                   |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | a723a5f6db5149e5b6eaf450424ed835         |
| service_name | cinderv2                                 |
| service_type | volumev2                                 |
| url          | http://controller:8776/v2/%(project_id)s |
+--------------+------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%(project_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%(project_id)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 5a4be40af4b14c2cbbb95e9c438126d4         |
| interface    | internal                                 |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | a723a5f6db5149e5b6eaf450424ed835         |
| service_name | cinderv2                                 |
| service_type | volumev2                                 |
| url          | http://controller:8776/v2/%(project_id)s |
+--------------+------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%(project_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%(project_id)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 5462f827d55c4ec3b142b124f02c5d0e         |
| interface    | admin                                    |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | a723a5f6db5149e5b6eaf450424ed835         |
| service_name | cinderv2                                 |
| service_type | volumev2                                 |
| url          | http://controller:8776/v2/%(project_id)s |
+--------------+------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name cinderv3 --description OpenStack Block Storage Service volumev3
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name cinderv3 --description OpenStack Block Storage Service volumev3
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage Service  |
| enabled     | True                             |
| id          | dddb53808809427a8e39daec388ac767 |
| name        | cinderv3                         |
| type        | volumev3                         |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%(project_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%(project_id)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 80697dec431f4968bb33fd08827b4f84         |
| interface    | public                                   |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | dddb53808809427a8e39daec388ac767         |
| service_name | cinderv3                                 |
| service_type | volumev3                                 |
| url          | http://controller:8776/v3/%(project_id)s |
+--------------+------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%(project_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%(project_id)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | db4388c7c91f423eac2109ad849b78e1         |
| interface    | internal                                 |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | dddb53808809427a8e39daec388ac767         |
| service_name | cinderv3                                 |
| service_type | volumev3                                 |
| url          | http://controller:8776/v3/%(project_id)s |
+--------------+------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%(project_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%(project_id)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | d74c9fd96cc74ba58c8b44c94bb3f0f5         |
| interface    | admin                                    |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | dddb53808809427a8e39daec388ac767         |
| service_name | cinderv3                                 |
| service_type | volumev3                                 |
| url          | http://controller:8776/v3/%(project_id)s |
+--------------+------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages cinder-api cinder-scheduler python-cinderclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed cinder-api cinder-scheduler python-cinderclient
+ retval=1
+ [ ! -z cinder-api ]
+ dpkg -s cinder-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z cinder-scheduler ]
+ dpkg -s cinder-scheduler
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-cinderclient ]
+ dpkg -s python-cinderclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ crudini --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:4b6df6d901da295f578c@controller/cinder
+ crudini --del /etc/cinder/cinder.conf keystone_authtoken auth_host
+ crudini --del /etc/cinder/cinder.conf keystone_authtoken auth_port
+ crudini --del /etc/cinder/cinder.conf keystone_authtoken auth_protocol
+ crudini --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/cinder/cinder.conf DEFAULT verbose False
+ crudini --set /etc/cinder/cinder.conf DEFAULT debug False
+ crudini --set /etc/cinder/cinder.conf DEFAULT my_ip 192.168.0.1
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/cinder/cinder.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 11 ]
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken auth_type password
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken project_name service
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken username cinder
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken password bcabcfa033c81284dec0
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller:11211
+ crudini --set /etc/cinder/cinder.conf DEFAULT glance_host controller
+ [ 17 -eq 11 ]
+ [ 17 -ge 12 ]
+ crudini --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp
+ [ 17 -ge 12 ]
+ crudini --set /etc/nova/nova.conf cinder os_region_name RegionOne
+ sed -i -e s/^\(.*volume_group.*=.*\)$/#\1/ /etc/cinder/cinder.conf
+ [ 17 -eq 16 ]
+ su -s /bin/sh -c /usr/bin/cinder-manage db sync cinder
2019-05-02 14:11:37.567 16299 INFO migrate.versioning.api [-] 84 -> 85... 
2019-05-02 14:11:37.798 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.798 16299 INFO migrate.versioning.api [-] 85 -> 86... 
2019-05-02 14:11:37.812 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.812 16299 INFO migrate.versioning.api [-] 86 -> 87... 
2019-05-02 14:11:37.822 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.822 16299 INFO migrate.versioning.api [-] 87 -> 88... 
2019-05-02 14:11:37.847 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.847 16299 INFO migrate.versioning.api [-] 88 -> 89... 
2019-05-02 14:11:37.861 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.861 16299 INFO migrate.versioning.api [-] 89 -> 90... 
2019-05-02 14:11:37.886 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.886 16299 INFO migrate.versioning.api [-] 90 -> 91... 
2019-05-02 14:11:37.958 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.958 16299 INFO migrate.versioning.api [-] 91 -> 92... 
2019-05-02 14:11:37.961 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.962 16299 INFO migrate.versioning.api [-] 92 -> 93... 
2019-05-02 14:11:37.965 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.965 16299 INFO migrate.versioning.api [-] 93 -> 94... 
2019-05-02 14:11:37.968 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.968 16299 INFO migrate.versioning.api [-] 94 -> 95... 
2019-05-02 14:11:37.971 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.971 16299 INFO migrate.versioning.api [-] 95 -> 96... 
2019-05-02 14:11:37.974 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.974 16299 INFO migrate.versioning.api [-] 96 -> 97... 
2019-05-02 14:11:37.987 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.987 16299 INFO migrate.versioning.api [-] 97 -> 98... 
2019-05-02 14:11:37.997 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:37.998 16299 INFO migrate.versioning.api [-] 98 -> 99... 
2019-05-02 14:11:38.014 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.014 16299 INFO migrate.versioning.api [-] 99 -> 100... 
2019-05-02 14:11:38.017 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding attachment_specs_attachment_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.018 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding cgsnapshots_consistencygroup_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.020 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding group_snapshots_group_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.022 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding group_type_specs_group_type_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.024 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding group_volume_type_mapping_group_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.025 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding group_volume_type_mapping_volume_type_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.027 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding quality_of_service_specs_specs_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.029 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding reservations_allocated_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.030 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding reservations_usage_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.032 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding snapshot_metadata_snapshot_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.034 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding snapshots_cgsnapshot_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.036 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding snapshots_group_snapshot_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.038 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding snapshots_volume_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.039 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding transfers_volume_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.042 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_admin_metadata_volume_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.043 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_attachment_volume_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.045 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_glance_metadata_snapshot_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.046 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_glance_metadata_volume_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.048 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_metadata_volume_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.050 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_type_extra_specs_volume_type_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.052 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volume_types_qos_specs_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.054 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volumes_consistencygroup_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.056 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding volumes_group_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.058 16299 INFO 100_add_foreign_key_indexes [-] Skipped adding workers_service_id_idx because an equivalent index already exists.
2019-05-02 14:11:38.060 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.061 16299 INFO migrate.versioning.api [-] 100 -> 101... 
2019-05-02 14:11:38.070 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.070 16299 INFO migrate.versioning.api [-] 101 -> 102... 
2019-05-02 14:11:38.080 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.080 16299 INFO migrate.versioning.api [-] 102 -> 103... 
2019-05-02 14:11:38.097 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.097 16299 INFO migrate.versioning.api [-] 103 -> 104... 
2019-05-02 14:11:38.112 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.112 16299 INFO migrate.versioning.api [-] 104 -> 105... 
2019-05-02 14:11:38.125 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.125 16299 INFO migrate.versioning.api [-] 105 -> 106... 
2019-05-02 14:11:38.129 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.129 16299 INFO migrate.versioning.api [-] 106 -> 107... 
2019-05-02 14:11:38.133 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.133 16299 INFO migrate.versioning.api [-] 107 -> 108... 
2019-05-02 14:11:38.135 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.136 16299 INFO migrate.versioning.api [-] 108 -> 109... 
2019-05-02 14:11:38.139 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.139 16299 INFO migrate.versioning.api [-] 109 -> 110... 
2019-05-02 14:11:38.142 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.142 16299 INFO migrate.versioning.api [-] 110 -> 111... 
2019-05-02 14:11:38.153 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.153 16299 INFO migrate.versioning.api [-] 111 -> 112... 
2019-05-02 14:11:38.170 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.170 16299 INFO migrate.versioning.api [-] 112 -> 113... 
2019-05-02 14:11:38.185 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.185 16299 INFO migrate.versioning.api [-] 113 -> 114... 
2019-05-02 14:11:38.219 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.219 16299 INFO migrate.versioning.api [-] 114 -> 115... 
2019-05-02 14:11:38.240 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.240 16299 INFO migrate.versioning.api [-] 115 -> 116... 
2019-05-02 14:11:38.260 16299 INFO migrate.versioning.api [-] done
2019-05-02 14:11:38.260 16299 INFO migrate.versioning.api [-] 116 -> 117... 
2019-05-02 14:11:38.271 16299 INFO migrate.versioning.api [-] done
+ service_restart cinder-scheduler
+ service=cinder-scheduler
+ [ 1 -eq 0 ]
+ systemctl restart cinder-scheduler
+ service_enable cinder-scheduler
+ service=cinder-scheduler
+ [ 1 -eq 0 ]
+ systemctl enable cinder-scheduler
Synchronizing state of cinder-scheduler.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable cinder-scheduler
+ [ 17 -ge 15 ]
+ a2enconf cinder-wsgi.conf
Conf cinder-wsgi already enabled
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ rm -f /var/lib/cinder/cinder.sqlite
+ echo CINDER_DBPASS="4b6df6d901da295f578c"
+ echo CINDER_PASS="bcabcfa033c81284dec0"
+ logtend cinder
+ area=cinder
+ echo cinder
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=cinder
+ date +%s
+ stamp=1556827899
+ date
+ date=Thu May  2 14:11:39 MDT 2019
+ eval tss=$LOGTIMESTART_cinder
+ tss=1556827875
+ expr 1556827899 - 1556827875
+ tsres=24
+ perl -e print 24 / 60.0 . "\n"
+ resmin=0.4
+ echo END cinder 1556827899 Thu May  2 14:11:39 MDT 2019
+ echo TOTAL cinder 24 0.4
+ [ -z  ]
+ logtstart cinder-host
+ area=cinder-host
+ echo cinder-host
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=cinder_host
+ date +%s
+ stamp=1556827899
+ date
+ date=Thu May  2 14:11:39 MDT 2019
+ eval LOGTIMESTART_cinder_host=1556827899
+ LOGTIMESTART_cinder_host=1556827899
+ echo START cinder-host 1556827899 Thu May  2 14:11:39 MDT 2019
+ getfqdn ctl
+ n=ctl
+ cut -f2
+ grep -E ctl\s
+ cat /root/setup/fqdn.map
+ fqdn=
+ echo
+ fqdn=
+ [ ctl = controller ]
+ scp -o StrictHostKeyChecking=no /root/setup/settings :/root/setup/settings
cp: cannot create regular file ':/root/setup/settings': No such file or directory
+ ssh -o StrictHostKeyChecking=no /local/repository/setup-storage.sh
Pseudo-terminal will not be allocated because stdin is not a terminal.
ssh: Could not resolve hostname /local/repository/setup-storage.sh: Name or service not known
+ echo STORAGE_HOST_DONE="1"
+ logtend cinder-host
+ area=cinder-host
+ echo cinder-host
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=cinder_host
+ date +%s
+ stamp=1556827899
+ date
+ date=Thu May  2 14:11:39 MDT 2019
+ eval tss=$LOGTIMESTART_cinder_host
+ tss=1556827899
+ expr 1556827899 - 1556827899
+ tsres=0
+ perl -e print 0 / 60.0 . "\n"
+ resmin=0
+ echo END cinder-host 1556827899 Thu May  2 14:11:39 MDT 2019
+ echo TOTAL cinder-host 0 0
+ [ 17 -ge 13 -a -z  ]
+ logtstart manila
+ area=manila
+ echo manila
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=manila
+ date +%s
+ stamp=1556827899
+ date
+ date=Thu May  2 14:11:39 MDT 2019
+ eval LOGTIMESTART_manila=1556827899
+ LOGTIMESTART_manila=1556827899
+ echo START manila 1556827899 Thu May  2 14:11:39 MDT 2019
+ openssl rand -hex 10
+ MANILA_DBPASS=f505ef603e7c86506660
+ openssl rand -hex 10
+ MANILA_PASS=4ca14b3298a2cff8544c
+ echo create database manila
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on manila.* to 'manila'@'localhost' identified by 'f505ef603e7c86506660'
+ mysql -u root --password=26e250af84ca2965f0e6
+ + echo grant all privileges on manila.* to 'manila'@'%' identified by 'f505ef603e7c86506660'
mysql -u root --password=26e250af84ca2965f0e6
+ __openstack user create --domain default --password 4ca14b3298a2cff8544c manila
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 4ca14b3298a2cff8544c manila
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | fdce52dcb9c3428d823fa7f10b235f42 |
| name                | manila                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user manila --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user manila --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name manila --description OpenStack Shared File Systems share
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name manila --description OpenStack Shared File Systems share
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Shared File Systems    |
| enabled     | True                             |
| id          | 6f50b6da87624e3f91c5417cf7314d62 |
| name        | manila                           |
| type        | share                            |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name manilav2 --description OpenStack Shared File Systems sharev2
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name manilav2 --description OpenStack Shared File Systems sharev2
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Shared File Systems    |
| enabled     | True                             |
| id          | b2ab099300b14219a4251e761a9a6749 |
| name        | manilav2                         |
| type        | sharev2                          |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne share public http://controller:8786/v1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne share public http://controller:8786/v1/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | dbbc7461305b42818f7f410445e0b1e6        |
| interface    | public                                  |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | 6f50b6da87624e3f91c5417cf7314d62        |
| service_name | manila                                  |
| service_type | share                                   |
| url          | http://controller:8786/v1/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne share internal http://controller:8786/v1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne share internal http://controller:8786/v1/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 22a0a5203c0e4d12ba4ca12d514e018b        |
| interface    | internal                                |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | 6f50b6da87624e3f91c5417cf7314d62        |
| service_name | manila                                  |
| service_type | share                                   |
| url          | http://controller:8786/v1/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne share admin http://controller:8786/v1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne share admin http://controller:8786/v1/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 9b15b82d561649bcaf6adb6f012a477f        |
| interface    | admin                                   |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | 6f50b6da87624e3f91c5417cf7314d62        |
| service_name | manila                                  |
| service_type | share                                   |
| url          | http://controller:8786/v1/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne sharev2 public http://controller:8786/v2/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne sharev2 public http://controller:8786/v2/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 7cac1a94d9314d088e08f0034bf65ef7        |
| interface    | public                                  |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | b2ab099300b14219a4251e761a9a6749        |
| service_name | manilav2                                |
| service_type | sharev2                                 |
| url          | http://controller:8786/v2/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne sharev2 internal http://controller:8786/v2/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne sharev2 internal http://controller:8786/v2/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 5f5af449993a47158b1fea02630ef0fc        |
| interface    | internal                                |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | b2ab099300b14219a4251e761a9a6749        |
| service_name | manilav2                                |
| service_type | sharev2                                 |
| url          | http://controller:8786/v2/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne sharev2 admin http://controller:8786/v2/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne sharev2 admin http://controller:8786/v2/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 6a901507ad1448188e7d6b58cc4e603a        |
| interface    | admin                                   |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | b2ab099300b14219a4251e761a9a6749        |
| service_name | manilav2                                |
| service_type | sharev2                                 |
| url          | http://controller:8786/v2/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages manila-api manila-scheduler python-manilaclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed manila-api manila-scheduler python-manilaclient
+ retval=1
+ [ ! -z manila-api ]
+ dpkg -s manila-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z manila-scheduler ]
+ dpkg -s manila-scheduler
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-manilaclient ]
+ dpkg -s python-manilaclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ crudini --set /etc/manila/manila.conf database connection mysql+pymysql://manila:f505ef603e7c86506660@controller/manila
+ crudini --del /etc/manila/manila.conf keystone_authtoken auth_host
+ crudini --del /etc/manila/manila.conf keystone_authtoken auth_port
+ crudini --del /etc/manila/manila.conf keystone_authtoken auth_protocol
+ crudini --set /etc/manila/manila.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/manila/manila.conf DEFAULT verbose False
+ crudini --set /etc/manila/manila.conf DEFAULT debug False
+ crudini --set /etc/manila/manila.conf DEFAULT my_ip 192.168.0.1
+ crudini --set /etc/manila/manila.conf DEFAULT default_share_type default_share_type
+ crudini --set /etc/manila/manila.conf DEFAULT rootwrap_config /etc/manila/rootwrap.conf
+ [ 17 -lt 14 ]
+ crudini --set /etc/manila/manila.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ crudini --set /etc/manila/manila.conf keystone_authtoken memcached_servers controller:11211
+ crudini --set /etc/manila/manila.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/manila/manila.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/manila/manila.conf keystone_authtoken auth_type password
+ crudini --set /etc/manila/manila.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/manila/manila.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/manila/manila.conf keystone_authtoken project_name service
+ crudini --set /etc/manila/manila.conf keystone_authtoken username manila
+ crudini --set /etc/manila/manila.conf keystone_authtoken password 4ca14b3298a2cff8544c
+ crudini --set /etc/manila/manila.conf oslo_concurrency lock_path /var/lib/manila/tmp
+ su -s /bin/sh -c manila-manage db sync manila
2019-05-02 14:12:00.060 16749 INFO alembic.runtime.migration [-] Context impl MySQLImpl.
2019-05-02 14:12:00.060 16749 INFO alembic.runtime.migration [-] Will assume non-transactional DDL.
2019-05-02 14:12:00.095 16749 INFO alembic.runtime.migration [-] Running upgrade  -> 162a3e673105, manila_init
2019-05-02 14:12:00.246 16749 INFO alembic.runtime.migration [-] Running upgrade 162a3e673105 -> 211836bf835c, add access level
2019-05-02 14:12:00.252 16749 INFO alembic.runtime.migration [-] Running upgrade 211836bf835c -> 38e632621e5a, change volume_type to share_type
2019-05-02 14:12:00.252 16749 INFO 38e632621e5a_change_volume_type_to_share_type_py [-] Renaming column name shares.volume_type_id to shares.share_type.id
2019-05-02 14:12:00.255 16749 INFO 38e632621e5a_change_volume_type_to_share_type_py [-] Renaming volume_types table to share_types
2019-05-02 14:12:00.261 16749 INFO 38e632621e5a_change_volume_type_to_share_type_py [-] Creating share_type_extra_specs table
2019-05-02 14:12:00.265 16749 INFO 38e632621e5a_change_volume_type_to_share_type_py [-] Migrating volume_type_extra_specs to share_type_extra_specs
2019-05-02 14:12:00.266 16749 INFO 38e632621e5a_change_volume_type_to_share_type_py [-] Dropping volume_type_extra_specs table
2019-05-02 14:12:00.269 16749 INFO alembic.runtime.migration [-] Running upgrade 38e632621e5a -> 17115072e1c3, add_nova_net_id_column_to_share_networks
2019-05-02 14:12:00.274 16749 INFO alembic.runtime.migration [-] Running upgrade 17115072e1c3 -> 4ee2cf4be19a, Remove share_snapshots.export_location
2019-05-02 14:12:00.280 16749 INFO alembic.runtime.migration [-] Running upgrade 4ee2cf4be19a -> 59eb64046740, Add required extra spec
/usr/lib/python2.7/dist-packages/sqlalchemy/sql/default_comparator.py:161: SAWarning: The IN-predicate on "share_types.id" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance.
  'strategies for improved performance.' % expr)
2019-05-02 14:12:00.285 16749 INFO alembic.runtime.migration [-] Running upgrade 59eb64046740 -> ef0c02b4366, Add_share_type_projects
2019-05-02 14:12:00.296 16749 INFO alembic.runtime.migration [-] Running upgrade ef0c02b4366 -> 30cb96d995fa, add public column for share
2019-05-02 14:12:00.305 16749 INFO alembic.runtime.migration [-] Running upgrade 30cb96d995fa -> 56cdbe267881, Add share_export_locations table
2019-05-02 14:12:00.318 16749 INFO alembic.runtime.migration [-] Running upgrade 56cdbe267881 -> 3a482171410f, add_driver_private_data_table
2019-05-02 14:12:00.323 16749 INFO alembic.runtime.migration [-] Running upgrade 3a482171410f -> 533646c7af38, Remove unused attr status
2019-05-02 14:12:00.333 16749 INFO alembic.runtime.migration [-] Running upgrade 533646c7af38 -> 3db9992c30f3, Transform statuses to lowercase
2019-05-02 14:12:00.356 16749 INFO alembic.runtime.migration [-] Running upgrade 3db9992c30f3 -> 5077ffcc5f1c, add_share_instances
2019-05-02 14:12:00.483 16749 INFO alembic.runtime.migration [-] Running upgrade 5077ffcc5f1c -> 579c267fbb4d, add_share_instances_access_map
2019-05-02 14:12:00.507 16749 INFO alembic.runtime.migration [-] Running upgrade 579c267fbb4d -> 1f0bd302c1a6, add_availability_zones_table
2019-05-02 14:12:00.567 16749 INFO alembic.runtime.migration [-] Running upgrade 1f0bd302c1a6 -> 55761e5f59c5, Add 'snapshot_support' extra spec to share types
2019-05-02 14:12:00.578 16749 INFO alembic.runtime.migration [-] Running upgrade 55761e5f59c5 -> 3651e16d7c43, Create Consistency Groups Tables and Columns
2019-05-02 14:12:00.618 16749 INFO alembic.runtime.migration [-] Running upgrade 3651e16d7c43 -> 323840a08dc4, Add shares.task_state
2019-05-02 14:12:00.623 16749 INFO alembic.runtime.migration [-] Running upgrade 323840a08dc4 -> dda6de06349, Add DB support for share instance export locations metadata.
2019-05-02 14:12:00.646 16749 INFO alembic.runtime.migration [-] Running upgrade dda6de06349 -> 344c1ac4747f, Remove access rules status and add access_rule_status to share_instance
model
2019-05-02 14:12:00.687 16749 INFO alembic.runtime.migration [-] Running upgrade 344c1ac4747f -> 293fac1130ca, Add replication attributes to Share and ShareInstance models.
2019-05-02 14:12:00.703 16749 INFO alembic.runtime.migration [-] Running upgrade 293fac1130ca -> 5155c7077f99, Add more network info attributes to 'network_allocations' table.
2019-05-02 14:12:00.735 16749 INFO alembic.runtime.migration [-] Running upgrade 5155c7077f99 -> eb6d5544cbbd, add provider_location to share_snapshot_instances
2019-05-02 14:12:00.742 16749 INFO alembic.runtime.migration [-] Running upgrade eb6d5544cbbd -> 221a83cfd85b, change_user_id_length
2019-05-02 14:12:00.742 16749 INFO 221a83cfd85b_change_user_project_id_length_py [-] Changing user_id length for share_networks
2019-05-02 14:12:00.754 16749 INFO 221a83cfd85b_change_user_project_id_length_py [-] Changing project_id length for share_networks
2019-05-02 14:12:00.762 16749 INFO 221a83cfd85b_change_user_project_id_length_py [-] Changing project_id length for security_services
2019-05-02 14:12:00.769 16749 INFO alembic.runtime.migration [-] Running upgrade 221a83cfd85b -> fdfb668d19e1, add_gateway_to_network_allocations_table
2019-05-02 14:12:00.779 16749 INFO alembic.runtime.migration [-] Running upgrade fdfb668d19e1 -> e8ea58723178, Remove host from driver private data
2019-05-02 14:12:00.804 16749 INFO alembic.runtime.migration [-] Running upgrade e8ea58723178 -> 493eaffd79e1, add_mtu_network_allocations
2019-05-02 14:12:00.813 16749 INFO alembic.runtime.migration [-] Running upgrade 493eaffd79e1 -> 63809d875e32, add_access_key
2019-05-02 14:12:00.819 16749 INFO alembic.runtime.migration [-] Running upgrade 63809d875e32 -> 48a7beae3117, move_share_type_id_to_instances
2019-05-02 14:12:00.862 16749 INFO alembic.runtime.migration [-] Running upgrade 48a7beae3117 -> 3e7d62517afa, Add 'create_share_from_snapshot_support' extra spec to share types
2019-05-02 14:12:00.872 16749 INFO alembic.runtime.migration [-] Running upgrade 3e7d62517afa -> 95e3cf760840, remove_nova_net_id_column_from_share_networks
2019-05-02 14:12:00.877 16749 INFO alembic.runtime.migration [-] Running upgrade 95e3cf760840 -> 87ce15c59bbe, add_revert_to_snapshot_support
2019-05-02 14:12:00.884 16749 INFO alembic.runtime.migration [-] Running upgrade 87ce15c59bbe -> 54667b9cade7, add_share_instance_access_map_state
2019-05-02 14:12:00.921 16749 INFO alembic.runtime.migration [-] Running upgrade 54667b9cade7 -> e9f79621d83f, add_cast_rules_to_readonly_to_share_instances
2019-05-02 14:12:00.921 16749 INFO e9f79621d83f_add_cast_rules_to_readonly_to_share_instances_py [-] Adding cast_rules_to_readonly column to share instances.
2019-05-02 14:12:00.964 16749 INFO alembic.runtime.migration [-] Running upgrade e9f79621d83f -> 03da71c0e321, Convert consistency groups to share groups
2019-05-02 14:12:00.964 16749 INFO 03da71c0e321_convert_cgs_to_share_groups_py [-] Renaming consistency group tables
2019-05-02 14:12:01.089 16749 INFO alembic.runtime.migration [-] Running upgrade 03da71c0e321 -> e1949a93157a, Add share group types table
2019-05-02 14:12:01.125 16749 INFO alembic.runtime.migration [-] Running upgrade e1949a93157a -> a77e2ad5012d, add_share_snapshot_access
2019-05-02 14:12:01.157 16749 INFO alembic.runtime.migration [-] Running upgrade a77e2ad5012d -> 927920b37453, Add 'provider_location' attr to 'share_group_snapshot_members' model.
2019-05-02 14:12:01.168 16749 INFO alembic.runtime.migration [-] Running upgrade 927920b37453 -> d5db24264f5c, Add enum 'consistent_snapshot_support' attr to 'share_groups' model.
2019-05-02 14:12:01.175 16749 INFO alembic.runtime.migration [-] Running upgrade d5db24264f5c -> 7d142971c4ef, add_reservation_expire_index
2019-05-02 14:12:01.179 16749 INFO alembic.runtime.migration [-] Running upgrade 7d142971c4ef -> 5237b6625330, Add 'availability_zone_id' field to 'share_groups' table.
2019-05-02 14:12:01.186 16749 INFO alembic.runtime.migration [-] Running upgrade 5237b6625330 -> 31252d671ae5, Squash 'share_group_snapshot_members' and 'share_snapshot_instances' models.
2019-05-02 14:12:01.252 16749 INFO alembic.runtime.migration [-] Running upgrade 31252d671ae5 -> 238720805ce1, Add messages table
2019-05-02 14:12:01.257 16749 INFO alembic.runtime.migration [-] Running upgrade 238720805ce1 -> b516de97bfee, Add ProjectShareTypeQuota model
2019-05-02 14:12:01.276 16749 INFO alembic.runtime.migration [-] Running upgrade b516de97bfee -> 829a09b0ddd4, Fix 'project_share_type_quotas' unique constraint
2019-05-02 14:12:01.289 16749 INFO alembic.runtime.migration [-] Running upgrade 829a09b0ddd4 -> 27cb96d991fa, add description for share type
2019-05-02 14:12:01.295 16749 INFO alembic.runtime.migration [-] Running upgrade 27cb96d991fa -> 4a482571410f, add_backend_info_table
+ __openstack flavor create manila-service-flavor --id 100 --ram 256 --disk 0 --vcpus 1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack flavor create manila-service-flavor --id 100 --ram 256 --disk 0 --vcpus 1
+----------------------------+-----------------------+
| Field                      | Value                 |
+----------------------------+-----------------------+
| OS-FLV-DISABLED:disabled   | False                 |
| OS-FLV-EXT-DATA:ephemeral  | 0                     |
| disk                       | 0                     |
| id                         | 100                   |
| name                       | manila-service-flavor |
| os-flavor-access:is_public | True                  |
| properties                 |                       |
| ram                        | 256                   |
| rxtx_factor                | 1.0                   |
| swap                       |                       |
| vcpus                      | 1                     |
+----------------------------+-----------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 17 -eq 17 ]
+ patch -p0 -d /
patching file /usr/lib/python2.7/dist-packages/manila/network/neutron/api.py
+ service_restart manila-scheduler
+ service=manila-scheduler
+ [ 1 -eq 0 ]
+ systemctl restart manila-scheduler
+ service_enable manila-scheduler
+ service=manila-scheduler
+ [ 1 -eq 0 ]
+ systemctl enable manila-scheduler
Synchronizing state of manila-scheduler.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable manila-scheduler
+ service_restart manila-api
+ service=manila-api
+ [ 1 -eq 0 ]
+ systemctl restart manila-api
+ service_enable manila-api
+ service=manila-api
+ [ 1 -eq 0 ]
+ systemctl enable manila-api
Synchronizing state of manila-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable manila-api
+ rm -f /var/lib/manila/manila.sqlite
+ manila type-create default_share_type True
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| required_extra_specs | driver_handles_share_servers : True  |
| Name                 | default_share_type                   |
| Visibility           | public                               |
| is_default           | -                                    |
| ID                   | f71d2071-9ab0-4741-a8ee-38b3f0341640 |
| optional_extra_specs |                                      |
| Description          | None                                 |
+----------------------+--------------------------------------+
+ maybe_install_packages python-manila-ui
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-manila-ui
+ retval=1
+ [ ! -z python-manila-ui ]
+ dpkg -s python-manila-ui
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ dpkg-query -L python-manila-ui
+ grep -q templates
+ [ ! 0 -eq 0 ]
+ [ 17 -eq 14 -a -f /local/repository/etc/manila-queens-noset.patch ]
+ [ 17 -eq 16 ]
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ echo MANILA_DBPASS="f505ef603e7c86506660"
+ echo MANILA_PASS="4ca14b3298a2cff8544c"
+ logtend manila
+ area=manila
+ echo manila
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=manila
+ date +%s
+ stamp=1556827928
+ date
+ date=Thu May  2 14:12:08 MDT 2019
+ eval tss=$LOGTIMESTART_manila
+ tss=1556827899
+ expr 1556827928 - 1556827899
+ tsres=29
+ perl -e print 29 / 60.0 . "\n"
+ resmin=0.483333333333333
+ echo END manila 1556827928 Thu May  2 14:12:08 MDT 2019
+ echo TOTAL manila 29 0.483333333333333
+ [ -z  ]
+ logtstart manila-host
+ area=manila-host
+ echo manila-host
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=manila_host
+ date +%s
+ stamp=1556827928
+ date
+ date=Thu May  2 14:12:08 MDT 2019
+ eval LOGTIMESTART_manila_host=1556827928
+ LOGTIMESTART_manila_host=1556827928
+ echo START manila-host 1556827928 Thu May  2 14:12:08 MDT 2019
+ getfqdn ctl
+ n=ctl
+ cat /root/setup/fqdn.map
+ grep -E ctl\s
+ cut -f2
+ fqdn=
+ echo
+ fqdn=
+ [ ctl = controller ]
+ scp -o StrictHostKeyChecking=no /root/setup/settings :/root/setup/settings
cp: cannot create regular file ':/root/setup/settings': No such file or directory
+ ssh -o StrictHostKeyChecking=no /local/repository/setup-share-node.sh
Pseudo-terminal will not be allocated because stdin is not a terminal.
ssh: Could not resolve hostname /local/repository/setup-share-node.sh: Name or service not known
+ echo SHARE_HOST_DONE="1"
+ logtend manila-host
+ area=manila-host
+ echo manila-host
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=manila_host
+ date +%s
+ stamp=1556827928
+ date
+ date=Thu May  2 14:12:08 MDT 2019
+ eval tss=$LOGTIMESTART_manila_host
+ tss=1556827928
+ expr 1556827928 - 1556827928
+ tsres=0
+ perl -e print 0 / 60.0 . "\n"
+ resmin=0
+ echo END manila-host 1556827928 Thu May  2 14:12:08 MDT 2019
+ echo TOTAL manila-host 0 0
+ [ -z  ]
+ logtstart swift
+ area=swift
+ echo swift
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=swift
+ date +%s
+ stamp=1556827928
+ date
+ date=Thu May  2 14:12:08 MDT 2019
+ eval LOGTIMESTART_swift=1556827928
+ LOGTIMESTART_swift=1556827928
+ echo START swift 1556827928 Thu May  2 14:12:08 MDT 2019
+ openssl rand -hex 10
+ SWIFT_PASS=c7c1df7fd68c420c3e31
+ openssl rand -hex 10
+ SWIFT_HASH_PATH_PREFIX=e659c49e9cdde2be26f9
+ openssl rand -hex 10
+ SWIFT_HASH_PATH_SUFFIX=e7a14a31a9cd466e9ffc
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password c7c1df7fd68c420c3e31 swift
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password c7c1df7fd68c420c3e31 swift
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | 0bcb6138e28e40eb8432648c924cb1c8 |
| name                | swift                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user swift --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user swift --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name swift --description OpenStack Object Storage Service object-store
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name swift --description OpenStack Object Storage Service object-store
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Object Storage Service |
| enabled     | True                             |
| id          | 92aabc1a7b5e4c549a527dbfd63d42cc |
| name        | swift                            |
| type        | object-store                     |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne object-store public http://controller:8080/v1/AUTH_%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne object-store public http://controller:8080/v1/AUTH_%(tenant_id)s
+--------------+----------------------------------------------+
| Field        | Value                                        |
+--------------+----------------------------------------------+
| enabled      | True                                         |
| id           | 6e243da41cdc479cb6a97034f97e56cf             |
| interface    | public                                       |
| region       | RegionOne                                    |
| region_id    | RegionOne                                    |
| service_id   | 92aabc1a7b5e4c549a527dbfd63d42cc             |
| service_name | swift                                        |
| service_type | object-store                                 |
| url          | http://controller:8080/v1/AUTH_%(tenant_id)s |
+--------------+----------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne object-store internal http://controller:8080/v1/AUTH_%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne object-store internal http://controller:8080/v1/AUTH_%(tenant_id)s
+--------------+----------------------------------------------+
| Field        | Value                                        |
+--------------+----------------------------------------------+
| enabled      | True                                         |
| id           | 6a23ae4a4df84fc8a422af7a3e8a6caa             |
| interface    | internal                                     |
| region       | RegionOne                                    |
| region_id    | RegionOne                                    |
| service_id   | 92aabc1a7b5e4c549a527dbfd63d42cc             |
| service_name | swift                                        |
| service_type | object-store                                 |
| url          | http://controller:8080/v1/AUTH_%(tenant_id)s |
+--------------+----------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne object-store admin http://controller:8080/v1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne object-store admin http://controller:8080/v1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 16339fc518534e88b50773afb1bbcb48 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 92aabc1a7b5e4c549a527dbfd63d42cc |
| service_name | swift                            |
| service_type | object-store                     |
| url          | http://controller:8080/v1        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages swift swift-proxy python-swiftclient python-keystoneclient python-keystonemiddleware
+ [ ! 0 -eq 0 ]
+ are_packages_installed swift swift-proxy python-swiftclient python-keystoneclient python-keystonemiddleware
+ retval=1
+ [ ! -z swift ]
+ dpkg -s swift
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z swift-proxy ]
+ dpkg -s swift-proxy
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-swiftclient ]
+ dpkg -s python-swiftclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-keystoneclient ]
+ dpkg -s python-keystoneclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-keystonemiddleware ]
+ dpkg -s python-keystonemiddleware
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ mkdir -p /etc/swift
+ wget -O /etc/swift/proxy-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/queens
--2019-05-02 14:12:22--  https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/queens
Resolving git.openstack.org (git.openstack.org)... 23.253.125.17, 2001:4800:7817:103:be76:4eff:fe04:e3e3
Connecting to git.openstack.org (git.openstack.org)|23.253.125.17|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://opendev.org/openstack/swift/raw/branch/stable/queens/etc/proxy-server.conf-sample [following]
--2019-05-02 14:12:23--  https://opendev.org/openstack/swift/raw/branch/stable/queens/etc/proxy-server.conf-sample
Resolving opendev.org (opendev.org)... 38.108.68.124, 2604:e100:3:0:f816:3eff:fe6b:ad62
Connecting to opendev.org (opendev.org)|38.108.68.124|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/plain]
Saving to: ‘/etc/swift/proxy-server.conf’

     0K .......... .......... .......... ..........             981K=0.04s

2019-05-02 14:12:23 (981 KB/s) - ‘/etc/swift/proxy-server.conf’ saved [41237]

+ [ ! 0 -eq 0 ]
+ crudini --set /etc/swift/proxy-server.conf DEFAULT bind_port 8080
+ crudini --set /etc/swift/proxy-server.conf DEFAULT user swift
+ crudini --set /etc/swift/proxy-server.conf DEFAULT swift_dir /etc/swift
+ crudini --get /etc/swift/proxy-server.conf pipeline:main pipeline
+ pipeline=catch_errors gatekeeper healthcheck proxy-logging cache listing_formats container_sync bulk tempurl ratelimit tempauth copy container-quotas account-quotas slo dlo versioned_writes symlink proxy-logging proxy-server
+ [ queens = juno ]
+ [ 17 -eq 11 ]
+ crudini --set /etc/swift/proxy-server.conf pipeline:main pipeline catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server
+ crudini --set /etc/swift/proxy-server.conf app:proxy-server use egg:swift#proxy
+ crudini --set /etc/swift/proxy-server.conf app:proxy-server allow_account_management true
+ crudini --set /etc/swift/proxy-server.conf app:proxy-server account_autocreate true
+ crudini --set /etc/swift/proxy-server.conf filter:keystoneauth use egg:swift#keystoneauth
+ [ queens = juno ]
+ crudini --set /etc/swift/proxy-server.conf filter:keystoneauth operator_roles admin,user
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken paste.filter_factory keystonemiddleware.auth_token:filter_factory
+ [ queens = juno ]
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken auth_uri http://controller:5000
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken auth_url http://controller:5000
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken auth_type password
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken project_domain_name default
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken user_domain_name default
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken project_name service
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken username swift
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken password c7c1df7fd68c420c3e31
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/swift/proxy-server.conf memcached_servers controller:11211
+ crudini --set /etc/swift/proxy-server.conf filter:authtoken delay_auth_decision true
+ crudini --set /etc/swift/proxy-server.conf filter:cache use egg:swift#memcache
+ crudini --set /etc/swift/proxy-server.conf filter:cache memcache_servers controller:11211
+ crudini --del /etc/swift/proxy-server.conf keystone_authtoken auth_host
+ crudini --del /etc/swift/proxy-server.conf keystone_authtoken auth_port
+ crudini --del /etc/swift/proxy-server.conf keystone_authtoken auth_protocol
+ mkdir -p /var/log/swift
+ chown -R syslog.adm /var/log/swift
+ crudini --set /etc/swift/proxy-server.conf DEFAULT log_facility LOG_LOCAL1
+ crudini --set /etc/swift/proxy-server.conf DEFAULT log_level INFO
+ crudini --set /etc/swift/proxy-server.conf DEFAULT log_name swift-proxy
+ echo if $programname == "swift-proxy" then { action(type="omfile" file="/var/log/swift/swift-proxy.log") }
+ wget -O /etc/swift/swift.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/queens
--2019-05-02 14:12:24--  https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/queens
Resolving git.openstack.org (git.openstack.org)... 23.253.125.17, 2001:4800:7817:103:be76:4eff:fe04:e3e3
Connecting to git.openstack.org (git.openstack.org)|23.253.125.17|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://opendev.org/openstack/swift/raw/branch/stable/queens/etc/swift.conf-sample [following]
--2019-05-02 14:12:24--  https://opendev.org/openstack/swift/raw/branch/stable/queens/etc/swift.conf-sample
Resolving opendev.org (opendev.org)... 38.108.68.124, 2604:e100:3:0:f816:3eff:fe6b:ad62
Connecting to opendev.org (opendev.org)|38.108.68.124|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/plain]
Saving to: ‘/etc/swift/swift.conf’

     0K .......                                                31.2M=0s

2019-05-02 14:12:24 (31.2 MB/s) - ‘/etc/swift/swift.conf’ saved [7894]

+ [ ! 0 -eq 0 ]
+ crudini --set /etc/swift/swift.conf swift-hash swift_hash_path_suffix e659c49e9cdde2be26f9
+ crudini --set /etc/swift/swift.conf swift-hash swift_hash_path_prefix e7a14a31a9cd466e9ffc
+ crudini --set /etc/swift/swift.conf storage-policy:0 name Policy-0
+ crudini --set /etc/swift/swift.conf storage-policy:0 default yes
+ chown -R swift:swift /etc/swift
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ service_restart rsyslog
+ service=rsyslog
+ [ 1 -eq 0 ]
+ systemctl restart rsyslog
+ [ 1 -eq 0 ]
+ service_restart swift-proxy
+ service=swift-proxy
+ [ 1 -eq 0 ]
+ systemctl restart swift-proxy
+ service_enable swift-proxy
+ service=swift-proxy
+ [ 1 -eq 0 ]
+ systemctl enable swift-proxy
swift-proxy.service is not a native service, redirecting to systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable swift-proxy
+ echo SWIFT_PASS="c7c1df7fd68c420c3e31"
+ echo SWIFT_HASH_PATH_PREFIX="e659c49e9cdde2be26f9"
+ echo SWIFT_HASH_PATH_SUFFIX="e7a14a31a9cd466e9ffc"
+ logtend swift
+ area=swift
+ echo swift
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=swift
+ date +%s
+ stamp=1556827946
+ date
+ date=Thu May  2 14:12:26 MDT 2019
+ eval tss=$LOGTIMESTART_swift
+ tss=1556827928
+ expr 1556827946 - 1556827928
+ tsres=18
+ perl -e print 18 / 60.0 . "\n"
+ resmin=0.3
+ echo END swift 1556827946 Thu May  2 14:12:26 MDT 2019
+ echo TOTAL swift 18 0.3
+ [ -z  ]
+ logtstart swift-host
+ area=swift-host
+ echo swift-host
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=swift_host
+ date +%s
+ stamp=1556827946
+ date
+ date=Thu May  2 14:12:26 MDT 2019
+ eval LOGTIMESTART_swift_host=1556827946
+ LOGTIMESTART_swift_host=1556827946
+ echo START swift-host 1556827946 Thu May  2 14:12:26 MDT 2019
+ getfqdn ctl
+ n=ctl
+ cat /root/setup/fqdn.map
+ grep -E ctl\s
+ cut -f2
+ fqdn=
+ echo
+ fqdn=
+ [ ctl = controller ]
+ scp -o StrictHostKeyChecking=no /root/setup/settings :/root/setup/settings
cp: cannot create regular file ':/root/setup/settings': No such file or directory
+ ssh -o StrictHostKeyChecking=no /local/repository/setup-object-storage.sh
Pseudo-terminal will not be allocated because stdin is not a terminal.
ssh: Could not resolve hostname /local/repository/setup-object-storage.sh: Name or service not known
+ echo OBJECT_HOST_DONE="1"
+ logtend swift-host
+ area=swift-host
+ echo swift-host
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=swift_host
+ date +%s
+ stamp=1556827946
+ date
+ date=Thu May  2 14:12:26 MDT 2019
+ eval tss=$LOGTIMESTART_swift_host
+ tss=1556827946
+ expr 1556827946 - 1556827946
+ tsres=0
+ perl -e print 0 / 60.0 . "\n"
+ resmin=0
+ echo END swift-host 1556827946 Thu May  2 14:12:26 MDT 2019
+ echo TOTAL swift-host 0 0
+ [ -z  ]
+ logtstart swift-rings
+ area=swift-rings
+ echo swift-rings
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=swift_rings
+ date +%s
+ stamp=1556827946
+ date
+ date=Thu May  2 14:12:26 MDT 2019
+ eval LOGTIMESTART_swift_rings=1556827946
+ LOGTIMESTART_swift_rings=1556827946
+ echo START swift-rings 1556827946 Thu May  2 14:12:26 MDT 2019
+ pwd
+ cdir=/root/setup/neutron-lbaas-dashboard
+ cd /etc/swift
+ cat /root/setup/mgmt-hosts
+ grep ctl
+ cut -d   -f 1
+ objip=
+ swift-ring-builder account.builder create 10 2 1
+ swift-ring-builder account.builder add r1z1-:6002/swiftv1 100
Invalid ip 
The on-disk ring builder is unchanged.
+ swift-ring-builder account.builder add r1z1-:6002/swiftv1-2 100
Invalid ip 
The on-disk ring builder is unchanged.
+ swift-ring-builder account.builder rebalance
-------------------------------------------------------------------------------
An error has occurred during ring validation. Common
causes of failure are rings that are empty or do not
have enough devices to accommodate the replica count.
Original exception message:
 There are no devices in this ring, or all devices have been deleted
-------------------------------------------------------------------------------
+ swift-ring-builder container.builder create 10 2 1
+ swift-ring-builder container.builder add r1z1-:6001/swiftv1 100
Invalid ip 
The on-disk ring builder is unchanged.
+ swift-ring-builder container.builder add r1z1-:6001/swiftv1-2 100
Invalid ip 
The on-disk ring builder is unchanged.
+ swift-ring-builder container.builder rebalance
-------------------------------------------------------------------------------
An error has occurred during ring validation. Common
causes of failure are rings that are empty or do not
have enough devices to accommodate the replica count.
Original exception message:
 There are no devices in this ring, or all devices have been deleted
-------------------------------------------------------------------------------
+ swift-ring-builder object.builder create 10 2 1
+ swift-ring-builder object.builder add r1z1-:6000/swiftv1 100
Invalid ip 
The on-disk ring builder is unchanged.
+ swift-ring-builder object.builder add r1z1-:6000/swiftv1-2 100
Invalid ip 
The on-disk ring builder is unchanged.
+ swift-ring-builder object.builder rebalance
-------------------------------------------------------------------------------
An error has occurred during ring validation. Common
causes of failure are rings that are empty or do not
have enough devices to accommodate the replica count.
Original exception message:
 There are no devices in this ring, or all devices have been deleted
-------------------------------------------------------------------------------
+ chown -R swift:swift /etc/swift
+ [ ctl != controller ]
+ scp -o StrictHostKeyChecking=no account.ring.gz container.ring.gz object.ring.gz ctl:/etc/swift
ssh: Could not resolve hostname ctl: Name or service not known
lost connection
+ cd /root/setup/neutron-lbaas-dashboard
+ [ 1 -eq 0 ]
+ service_restart swift-proxy
+ service=swift-proxy
+ [ 1 -eq 0 ]
+ systemctl restart swift-proxy
+ echo OBJECT_RING_DONE="1"
+ logtend swift-rings
+ area=swift-rings
+ echo swift-rings
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=swift_rings
+ date +%s
+ stamp=1556827951
+ date
+ date=Thu May  2 14:12:31 MDT 2019
+ eval tss=$LOGTIMESTART_swift_rings
+ tss=1556827946
+ expr 1556827951 - 1556827946
+ tsres=5
+ perl -e print 5 / 60.0 . "\n"
+ resmin=0.0833333333333333
+ echo END swift-rings 1556827951 Thu May  2 14:12:31 MDT 2019
+ echo TOTAL swift-rings 5 0.0833333333333333
+ [ -z  ]
+ logtstart heat
+ area=heat
+ echo heat
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=heat
+ date +%s
+ stamp=1556827951
+ date
+ date=Thu May  2 14:12:31 MDT 2019
+ eval LOGTIMESTART_heat=1556827951
+ LOGTIMESTART_heat=1556827951
+ echo START heat 1556827951 Thu May  2 14:12:31 MDT 2019
+ openssl rand -hex 10
+ HEAT_DBPASS=be5944f95e4913f2eea7
+ openssl rand -hex 10
+ HEAT_PASS=5081e31b64a08677226f
+ openssl rand -hex 10
+ HEAT_DOMAIN_PASS=f20d96f0fd0c1fa51a4a
+ echo create database heat
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on heat.* to 'heat'@'localhost' identified by 'be5944f95e4913f2eea7'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on heat.* to 'heat'@'%' identified by 'be5944f95e4913f2eea7'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password 5081e31b64a08677226f heat
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 5081e31b64a08677226f heat
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | b48b4a0d4c61446db899048114ae458d |
| name                | heat                             |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user heat --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user heat --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role create heat_stack_owner
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role create heat_stack_owner
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | 4f7c96807fb940e0bf1620a5db46f384 |
| name      | heat_stack_owner                 |
+-----------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role create heat_stack_user
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role create heat_stack_user
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | a9400da17a7b41c6aad7fc21a3c17d26 |
| name      | heat_stack_user                  |
+-----------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name heat --description OpenStack Orchestration Service orchestration
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name heat --description OpenStack Orchestration Service orchestration
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Orchestration Service  |
| enabled     | True                             |
| id          | 60cd0ba15a094953b1c2b4a5ed0af117 |
| name        | heat                             |
| type        | orchestration                    |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name heat-cfn --description OpenStack Orchestration Service cloudformation
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name heat-cfn --description OpenStack Orchestration Service cloudformation
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Orchestration Service  |
| enabled     | True                             |
| id          | 6847cdcc41784139b5f91c29f8eaad71 |
| name        | heat-cfn                         |
| type        | cloudformation                   |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne orchestration public http://controller:8004/v1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne orchestration public http://controller:8004/v1/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 4834f3ba8d6a46679bd3be06773d4fc3        |
| interface    | public                                  |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | 60cd0ba15a094953b1c2b4a5ed0af117        |
| service_name | heat                                    |
| service_type | orchestration                           |
| url          | http://controller:8004/v1/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne orchestration internal http://controller:8004/v1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne orchestration internal http://controller:8004/v1/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 45724ac0c40a47a1ae78a5943ed86f98        |
| interface    | internal                                |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | 60cd0ba15a094953b1c2b4a5ed0af117        |
| service_name | heat                                    |
| service_type | orchestration                           |
| url          | http://controller:8004/v1/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne orchestration admin http://controller:8004/v1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne orchestration admin http://controller:8004/v1/%(tenant_id)s
+--------------+-----------------------------------------+
| Field        | Value                                   |
+--------------+-----------------------------------------+
| enabled      | True                                    |
| id           | 499b9bacc739412d8447fb7a5385b8b9        |
| interface    | admin                                   |
| region       | RegionOne                               |
| region_id    | RegionOne                               |
| service_id   | 60cd0ba15a094953b1c2b4a5ed0af117        |
| service_name | heat                                    |
| service_type | orchestration                           |
| url          | http://controller:8004/v1/%(tenant_id)s |
+--------------+-----------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne cloudformation public http://controller:8000/v1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne cloudformation public http://controller:8000/v1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 857edff4044141b8afe3213d9665dd85 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 6847cdcc41784139b5f91c29f8eaad71 |
| service_name | heat-cfn                         |
| service_type | cloudformation                   |
| url          | http://controller:8000/v1        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne cloudformation internal http://controller:8000/v1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne cloudformation internal http://controller:8000/v1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 24f27a1cb6104967b2e9c1d9c8e50fbe |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 6847cdcc41784139b5f91c29f8eaad71 |
| service_name | heat-cfn                         |
| service_type | cloudformation                   |
| url          | http://controller:8000/v1        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne cloudformation admin http://controller:8000/v1
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne cloudformation admin http://controller:8000/v1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | e85ea811da4842aca73f64d9ff8eb36e |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 6847cdcc41784139b5f91c29f8eaad71 |
| service_name | heat-cfn                         |
| service_type | cloudformation                   |
| url          | http://controller:8000/v1        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack domain create --description Stack projects and users heat
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack domain create --description Stack projects and users heat
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Stack projects and users         |
| enabled     | True                             |
| id          | 24337a12c2334b6ab18e06a3cf26b037 |
| name        | heat                             |
| tags        | []                               |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack user create --domain heat --password f20d96f0fd0c1fa51a4a heat_domain_admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain heat --password f20d96f0fd0c1fa51a4a heat_domain_admin
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 24337a12c2334b6ab18e06a3cf26b037 |
| enabled             | True                             |
| id                  | 4d4f6a0da95e416d88e38c01f41412e1 |
| name                | heat_domain_admin                |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --domain heat --user heat_domain_admin admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --domain heat --user heat_domain_admin admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project admin --user admin heat_stack_owner
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project admin --user admin heat_stack_owner
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project admin --user adminapi heat_stack_owner
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project admin --user adminapi heat_stack_owner
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages heat-api heat-api-cfn heat-engine python-heatclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed heat-api heat-api-cfn heat-engine python-heatclient
+ retval=1
+ [ ! -z heat-api ]
+ dpkg -s heat-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z heat-api-cfn ]
+ dpkg -s heat-api-cfn
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z heat-engine ]
+ dpkg -s heat-engine
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-heatclient ]
+ dpkg -s python-heatclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -ge 17 ]
+ maybe_install_packages python-heat-dashboard
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-heat-dashboard
+ retval=1
+ [ ! -z python-heat-dashboard ]
+ dpkg -s python-heat-dashboard
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ crudini --set /etc/heat/heat.conf database connection mysql+pymysql://heat:be5944f95e4913f2eea7@controller/heat
+ crudini --set /etc/heat/heat.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/heat/heat.conf DEFAULT my_ip 192.168.0.1
+ crudini --set /etc/heat/heat.conf glance host controller
+ crudini --set /etc/heat/heat.conf DEFAULT verbose False
+ crudini --set /etc/heat/heat.conf DEFAULT debug False
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/heat/heat.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 11 ]
+ crudini --set /etc/heat/heat.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/heat/heat.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/heat/heat.conf keystone_authtoken auth_type password
+ crudini --set /etc/heat/heat.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/heat/heat.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/heat/heat.conf keystone_authtoken project_name service
+ crudini --set /etc/heat/heat.conf keystone_authtoken username heat
+ crudini --set /etc/heat/heat.conf keystone_authtoken password 5081e31b64a08677226f
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/heat/heat.conf keystone_authtoken memcached_servers controller:11211
+ [ 17 -gt 13 ]
+ crudini --set /etc/heat/heat.conf trustee auth_type password
+ [ 17 -ge 12 ]
+ crudini --set /etc/heat/heat.conf trustee auth_url http://controller:5000
+ crudini --set /etc/heat/heat.conf trustee username heat
+ crudini --set /etc/heat/heat.conf trustee password 5081e31b64a08677226f
+ crudini --set /etc/heat/heat.conf trustee user_domain_name default
+ crudini --set /etc/heat/heat.conf clients_keystone auth_uri http://controller:5000
+ crudini --set /etc/heat/heat.conf DEFAULT heat_metadata_server_url http://controller:8000
+ crudini --set /etc/heat/heat.conf DEFAULT heat_waitcondition_server_url http://controller:8000/v1/waitcondition
+ [ x3 = x3 ]
+ crudini --set /etc/heat/heat.conf ec2authtoken auth_uri http://controller:5000
+ [ 17 -ge 11 ]
+ crudini --set /etc/heat/heat.conf DEFAULT stack_domain_admin heat_domain_admin
+ crudini --set /etc/heat/heat.conf DEFAULT stack_domain_admin_password f20d96f0fd0c1fa51a4a
+ crudini --set /etc/heat/heat.conf DEFAULT stack_user_domain_name heat
+ crudini --del /etc/heat/heat.conf DEFAULT auth_host
+ crudini --del /etc/heat/heat.conf DEFAULT auth_port
+ crudini --del /etc/heat/heat.conf DEFAULT auth_protocol
+ [ 17 -eq 11 ]
+ su -s /bin/sh -c /usr/bin/heat-manage db_sync heat
2019-05-02 14:13:01.787 17722 INFO migrate.versioning.api [-] 70 -> 71... 
2019-05-02 14:13:01.888 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.888 17722 INFO migrate.versioning.api [-] 71 -> 72... 
2019-05-02 14:13:01.909 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.909 17722 INFO migrate.versioning.api [-] 72 -> 73... 
2019-05-02 14:13:01.938 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.939 17722 INFO migrate.versioning.api [-] 73 -> 74... 
2019-05-02 14:13:01.942 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.942 17722 INFO migrate.versioning.api [-] 74 -> 75... 
2019-05-02 14:13:01.945 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.945 17722 INFO migrate.versioning.api [-] 75 -> 76... 
2019-05-02 14:13:01.949 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.950 17722 INFO migrate.versioning.api [-] 76 -> 77... 
2019-05-02 14:13:01.952 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.953 17722 INFO migrate.versioning.api [-] 77 -> 78... 
2019-05-02 14:13:01.956 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:01.956 17722 INFO migrate.versioning.api [-] 78 -> 79... 
2019-05-02 14:13:02.008 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:02.008 17722 INFO migrate.versioning.api [-] 79 -> 80... 
2019-05-02 14:13:02.044 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:02.044 17722 INFO migrate.versioning.api [-] 80 -> 81... 
2019-05-02 14:13:02.047 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:02.047 17722 INFO migrate.versioning.api [-] 81 -> 82... 
2019-05-02 14:13:02.051 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:02.051 17722 INFO migrate.versioning.api [-] 82 -> 83... 
2019-05-02 14:13:02.055 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:02.055 17722 INFO migrate.versioning.api [-] 83 -> 84... 
2019-05-02 14:13:02.058 17722 INFO migrate.versioning.api [-] done
2019-05-02 14:13:02.058 17722 INFO migrate.versioning.api [-] 84 -> 85... 
2019-05-02 14:13:02.062 17722 INFO migrate.versioning.api [-] done
+ service_restart heat-api
+ service=heat-api
+ [ 1 -eq 0 ]
+ systemctl restart heat-api
+ service_enable heat-api
+ service=heat-api
+ [ 1 -eq 0 ]
+ systemctl enable heat-api
Synchronizing state of heat-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable heat-api
+ service_restart heat-api-cfn
+ service=heat-api-cfn
+ [ 1 -eq 0 ]
+ systemctl restart heat-api-cfn
+ service_enable heat-api-cfn
+ service=heat-api-cfn
+ [ 1 -eq 0 ]
+ systemctl enable heat-api-cfn
Synchronizing state of heat-api-cfn.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable heat-api-cfn
+ service_restart heat-engine
+ service=heat-engine
+ [ 1 -eq 0 ]
+ systemctl restart heat-engine
+ service_enable heat-engine
+ service=heat-engine
+ [ 1 -eq 0 ]
+ systemctl enable heat-engine
Synchronizing state of heat-engine.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable heat-engine
+ rm -f /var/lib/heat/heat.sqlite
+ echo HEAT_DBPASS="be5944f95e4913f2eea7"
+ echo HEAT_PASS="5081e31b64a08677226f"
+ echo HEAT_DOMAIN_PASS="f20d96f0fd0c1fa51a4a"
+ logtend heat
+ area=heat
+ echo heat
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=heat
+ date +%s
+ stamp=1556827983
+ date
+ date=Thu May  2 14:13:03 MDT 2019
+ eval tss=$LOGTIMESTART_heat
+ tss=1556827951
+ expr 1556827983 - 1556827951
+ tsres=32
+ perl -e print 32 / 60.0 . "\n"
+ resmin=0.533333333333333
+ echo END heat 1556827983 Thu May  2 14:13:03 MDT 2019
+ echo TOTAL heat 32 0.533333333333333
+ [ -z  ]
+ logtstart ceilometer
+ area=ceilometer
+ echo ceilometer
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer
+ date +%s
+ stamp=1556827983
+ date
+ date=Thu May  2 14:13:03 MDT 2019
+ eval LOGTIMESTART_ceilometer=1556827983
+ LOGTIMESTART_ceilometer=1556827983
+ echo START ceilometer 1556827983 Thu May  2 14:13:03 MDT 2019
+ openssl rand -hex 10
+ CEILOMETER_DBPASS=77faf52d26578538035d
+ openssl rand -hex 10
+ CEILOMETER_PASS=35a506d19e014bd69529
+ openssl rand -hex 10
+ CEILOMETER_SECRET=f53edec5f10a98ae265b
+ USING_GNOCCHI=0
+ [ 17 -ge 15 ]
+ USING_GNOCCHI=1
+ [ 1 -eq 0 -a 0 = 1 ]
+ [ 1 -eq 0 ]
+ openssl rand -hex 10
+ GNOCCHI_DBPASS=c697aa9065aa23859ce5
+ openssl rand -hex 10
+ GNOCCHI_PASS=2f547f16ea9d1cfe53a3
+ maybe_install_packages mariadb-server python-mysqldb
+ [ ! 0 -eq 0 ]
+ are_packages_installed mariadb-server python-mysqldb
+ retval=1
+ [ ! -z mariadb-server ]
+ dpkg -s mariadb-server
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-mysqldb ]
+ dpkg -s python-mysqldb
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ echo create database gnocchi
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on gnocchi.* to 'gnocchi'@'localhost' identified by 'c697aa9065aa23859ce5'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on gnocchi.* to 'gnocchi'@'%' identified by 'c697aa9065aa23859ce5'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ [ 1 -eq 0 ]
+ __openstack user create --domain default --password 35a506d19e014bd69529 ceilometer
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 35a506d19e014bd69529 ceilometer
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | 1b6152762f064f528bec8d6245672ddc |
| name                | ceilometer                       |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user ceilometer --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user ceilometer --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name ceilometer --description OpenStack Telemetry Service metering
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name ceilometer --description OpenStack Telemetry Service metering
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Telemetry Service      |
| enabled     | True                             |
| id          | 66e57a59918f4b3eaa7b73ea1bb3e1df |
| name        | ceilometer                       |
| type        | metering                         |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ openssl rand -hex 10
+ GNOCCHI_PASS=8d14c20fa0dcc80d6a2e
+ __openstack user create --domain default --password 8d14c20fa0dcc80d6a2e gnocchi
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 8d14c20fa0dcc80d6a2e gnocchi
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | 5f534f2deb4e420a8e55dcb0255908b3 |
| name                | gnocchi                          |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user gnocchi --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user gnocchi --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name gnocchi --description OpenStack Metric Service metric
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name gnocchi --description OpenStack Metric Service metric
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Metric Service         |
| enabled     | True                             |
| id          | 46629326482940ec9d7eb5e44f7eee1a |
| name        | gnocchi                          |
| type        | metric                           |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne metric public http://controller:8041
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne metric public http://controller:8041
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | eecd5018f346411a8f35876889f3ea02 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 46629326482940ec9d7eb5e44f7eee1a |
| service_name | gnocchi                          |
| service_type | metric                           |
| url          | http://controller:8041           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne metric internal http://controller:8041
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne metric internal http://controller:8041
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | eedf5bebaa50478893a1f9a7b26c1a61 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 46629326482940ec9d7eb5e44f7eee1a |
| service_name | gnocchi                          |
| service_type | metric                           |
| url          | http://controller:8041           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne metric admin http://controller:8041
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne metric admin http://controller:8041
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 4868e0f4f0864c9fac151ef93fd1a86d |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 46629326482940ec9d7eb5e44f7eee1a |
| service_name | gnocchi                          |
| service_type | metric                           |
| url          | http://controller:8041           |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages ceilometer-agent-central ceilometer-agent-notification
+ [ ! 0 -eq 0 ]
+ are_packages_installed ceilometer-agent-central ceilometer-agent-notification
+ retval=1
+ [ ! -z ceilometer-agent-central ]
+ dpkg -s ceilometer-agent-central
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z ceilometer-agent-notification ]
+ dpkg -s ceilometer-agent-notification
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 1 -eq 0 ]
+ maybe_install_packages gnocchi-metricd python-gnocchiclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed gnocchi-metricd python-gnocchiclient
+ retval=1
+ [ ! -z gnocchi-metricd ]
+ dpkg -s gnocchi-metricd
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-gnocchiclient ]
+ dpkg -s python-gnocchiclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 = 17 ]
+ maybe_install_packages python-gnocchi
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-gnocchi
+ retval=1
+ [ ! -z python-gnocchi ]
+ dpkg -s python-gnocchi
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ maybe_install_packages python3-gnocchi python3-gnocchiclient
+ [ ! 0 -eq 0 ]
+ are_packages_installed python3-gnocchi python3-gnocchiclient
+ retval=1
+ [ ! -z python3-gnocchi ]
+ dpkg -s python3-gnocchi
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python3-gnocchiclient ]
+ dpkg -s python3-gnocchiclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ maybe_install_packages uwsgi-core uwsgi-plugin-python uwsgi-plugin-python3
+ [ ! 0 -eq 0 ]
+ are_packages_installed uwsgi-core uwsgi-plugin-python uwsgi-plugin-python3
+ retval=1
+ [ ! -z uwsgi-core ]
+ dpkg -s uwsgi-core
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z uwsgi-plugin-python ]
+ dpkg -s uwsgi-plugin-python
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z uwsgi-plugin-python3 ]
+ dpkg -s uwsgi-plugin-python3
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ cat
+ crudini --set /etc/gnocchi/gnocchi.conf api uwsgi_mode http-socket
+ systemctl daemon-reload
+ systemctl enable gnocchi-api
Created symlink /etc/systemd/system/multi-user.target.wants/gnocchi-api.service → /etc/systemd/system/gnocchi-api.service.
+ chown -R gnocchi:gnocchi /var/lib/gnocchi
+ systemctl restart gnocchi-api
+ chown -R ceilometer /etc/ceilometer
+ [ 1 -eq 0 -a 0 = 1 ]
+ [ 1 -eq 0 ]
+ crudini --set /etc/ceilometer/ceilometer.conf DEFAULT verbose False
+ crudini --set /etc/ceilometer/ceilometer.conf DEFAULT debug False
+ crudini --set /etc/ceilometer/ceilometer.conf DEFAULT log_dir /var/log/ceilometer
+ crudini --del /etc/ceilometer/ceilometer.conf DEFAULT auth_host
+ crudini --del /etc/ceilometer/ceilometer.conf DEFAULT auth_port
+ crudini --del /etc/ceilometer/ceilometer.conf DEFAULT auth_protocol
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/ceilometer/ceilometer.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 1 -eq 0 ]
+ [ 17 -lt 13 ]
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials auth_type password
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials auth_url http://controller:5000/v3
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials username ceilometer
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials project_domain_name default
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials user_domain_name default
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials project_name service
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials password 35a506d19e014bd69529
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials interface internalURL
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials region_name RegionOne
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/ceilometer/ceilometer.conf service_credentials memcached_servers controller:11211
+ [ 1 -eq 1 ]
+ crudini --del /etc/gnocchi/gnocchi.conf keystone_authtoken auth_host
+ crudini --del /etc/gnocchi/gnocchi.conf keystone_authtoken auth_port
+ crudini --del /etc/gnocchi/gnocchi.conf keystone_authtoken auth_protocol
+ crudini --del /etc/gnocchi/gnocchi.conf keystone_authtoken admin_user
+ crudini --del /etc/gnocchi/gnocchi.conf keystone_authtoken admin_password
+ crudini --del /etc/gnocchi/gnocchi.conf keystone_authtoken admin_tenant_name
+ crudini --set /etc/ceilometer/ceilometer.conf dispatcher_gnocchi filter_service_activity False
+ crudini --set /etc/ceilometer/ceilometer.conf dispatcher_gnocchi archive_policy low
+ crudini --set /etc/ceilometer/ceilometer.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ crudini --set /etc/gnocchi/gnocchi.conf api auth_mode keystone
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken auth_type password
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken project_name service
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken username gnocchi
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken password 8d14c20fa0dcc80d6a2e
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken interface internalURL
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken region_name RegionOne
+ crudini --set /etc/gnocchi/gnocchi.conf keystone_authtoken memcached_servers controller:11211
+ crudini --set /etc/gnocchi/gnocchi.conf indexer url mysql+pymysql://gnocchi:c697aa9065aa23859ce5@controller/gnocchi
+ crudini --set /etc/gnocchi/gnocchi.conf storage driver file
+ crudini --set /etc/gnocchi/gnocchi.conf storage file_basepath /var/lib/gnocchi
+ [ 1 -eq 0 ]
+ mkdir -p /var/lib/gnocchi/cache
+ mkdir -p /var/lib/gnocchi/measure
+ mkdir -p /var/lib/gnocchi/tmp
+ chown -R gnocchi:gnocchi /var/lib/gnocchi
+ usermod -a -G gnocchi ceilometer
+ chmod -R 770 /var/lib/gnocchi
+ [ 17 -lt 17 ]
+ service_restart gnocchi-api
+ service=gnocchi-api
+ [ 1 -eq 0 ]
+ systemctl restart gnocchi-api
+ sleep 4
+ gnocchi-upgrade --config-file=/etc/gnocchi/gnocchi.conf
2019-05-02 14:13:38,984 [18431] INFO     gnocchi.service: Gnocchi version 4.2.4
2019-05-02 14:13:39,425 [18431] INFO     gnocchi.cli.manage: Upgrading indexer SQLAlchemyIndexer: mysql+pymysql://gnocchi:c697aa9065aa23859ce5@controller/gnocchi
2019-05-02 14:13:39,516 [18431] INFO     gnocchi.cli.manage: Upgrading storage FileStorage: /var/lib/gnocchi
2019-05-02 14:13:39,517 [18431] INFO     gnocchi.cli.manage: Upgrading incoming storage FileStorage: /var/lib/gnocchi
+ chown -R gnocchi:gnocchi /var/lib/gnocchi
+ service_restart gnocchi-api
+ service=gnocchi-api
+ [ 1 -eq 0 ]
+ systemctl restart gnocchi-api
+ ceilometer-upgrade --debug
+ service_restart ceilometer-agent-central
+ service=ceilometer-agent-central
+ [ 1 -eq 0 ]
+ systemctl restart ceilometer-agent-central
+ service_enable ceilometer-agent-central
+ service=ceilometer-agent-central
+ [ 1 -eq 0 ]
+ systemctl enable ceilometer-agent-central
Synchronizing state of ceilometer-agent-central.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable ceilometer-agent-central
+ service_restart ceilometer-agent-notification
+ service=ceilometer-agent-notification
+ [ 1 -eq 0 ]
+ systemctl restart ceilometer-agent-notification
+ service_enable ceilometer-agent-notification
+ service=ceilometer-agent-notification
+ [ 1 -eq 0 ]
+ systemctl enable ceilometer-agent-notification
Synchronizing state of ceilometer-agent-notification.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable ceilometer-agent-notification
+ [ 0 -eq 1 -a 1 -eq 0 ]
+ [ 1 -eq 0 ]
+ [ 17 -lt 17 ]
+ [ 1 -eq 0 ]
+ service_restart gnocchi-metricd
+ service=gnocchi-metricd
+ [ 1 -eq 0 ]
+ systemctl restart gnocchi-metricd
+ service_enable gnocchi-metricd
+ service=gnocchi-metricd
+ [ 1 -eq 0 ]
+ systemctl enable gnocchi-metricd
Synchronizing state of gnocchi-metricd.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable gnocchi-metricd
+ unified
+ [ controller = controller ]
+ return 0
+ service neutron-metering-agent restart
+ echo CEILOMETER_DBPASS="77faf52d26578538035d"
+ echo CEILOMETER_PASS="35a506d19e014bd69529"
+ echo CEILOMETER_SECRET="f53edec5f10a98ae265b"
+ echo USING_GNOCCHI="1"
+ [ 1 -eq 1 ]
+ echo GNOCCHI_DBPASS="c697aa9065aa23859ce5"
+ echo GNOCCHI_PASS="8d14c20fa0dcc80d6a2e"
+ logtend ceilometer
+ area=ceilometer
+ echo ceilometer
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer
+ date +%s
+ stamp=1556828044
+ date
+ date=Thu May  2 14:14:04 MDT 2019
+ eval tss=$LOGTIMESTART_ceilometer
+ tss=1556827983
+ expr 1556828044 - 1556827983
+ tsres=61
+ perl -e print 61 / 60.0 . "\n"
+ resmin=1.01666666666667
+ echo END ceilometer 1556828044 Thu May  2 14:14:04 MDT 2019
+ echo TOTAL ceilometer 61 1.01666666666667
+ [ 17 -ge 16 -a -z  ]
+ logtstart grafana
+ area=grafana
+ echo grafana
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=grafana
+ date +%s
+ stamp=1556828044
+ date
+ date=Thu May  2 14:14:04 MDT 2019
+ eval LOGTIMESTART_grafana=1556828044
+ LOGTIMESTART_grafana=1556828044
+ echo START grafana 1556828044 Thu May  2 14:14:04 MDT 2019
+ echo deb https://packages.grafana.com/oss/deb stable main
+ curl https://packages.grafana.com/gpg.key
+ sudo apt-key add -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)
100  1694  100  1694    0     0   4867      0 --:--:-- --:--:-- --:--:--  4867
OK
+ apt-get update
Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease
Hit:2 http://us.archive.ubuntu.com/ubuntu bionic InRelease
Get:3 http://us.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Hit:4 https://packages.grafana.com/oss/deb stable InRelease
Get:5 http://us.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Fetched 163 kB in 1s (195 kB/s)
Reading package lists...
W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target CNF (main/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target CNF (main/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target CNF (main/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
W: Target CNF (main/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/grafana.list:1 and /etc/apt/sources.list.d/grafana.list:2
+ apt-get install -y grafana
Reading package lists...
Building dependency tree...
Reading state information...
The following packages will be upgraded:
  grafana
1 upgraded, 0 newly installed, 0 to remove and 227 not upgraded.
Need to get 56.7 MB of archives.
After this operation, 4,327 kB of additional disk space will be used.
Get:1 https://packages.grafana.com/oss/deb stable/main amd64 grafana amd64 6.1.6 [56.7 MB]
Fetched 56.7 MB in 1s (47.1 MB/s)
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 181951 files and directories currently installed.)
Preparing to unpack .../grafana_6.1.6_amd64.deb ...
Unpacking grafana (6.1.6) over (5.4.3) ...
Processing triggers for ureadahead (0.100.0-20) ...
Processing triggers for systemd (237-3ubuntu10.11) ...
Setting up grafana (6.1.6) ...
Restarting grafana-server service... OK
+ maybe_install_packages sqlite3
+ [ ! 0 -eq 0 ]
+ are_packages_installed sqlite3
+ retval=1
+ [ ! -z sqlite3 ]
+ dpkg -s sqlite3
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ systemctl daemon-reload
+ crudini --set /etc/grafana/grafana.ini paths data /var/lib/grafana
+ [ x7ae0982e4f73 = x ]
+ GPASSWD=7ae0982e4f73
+ crudini --set /etc/grafana/grafana.ini security admin_user admin
+ crudini --set /etc/grafana/grafana.ini security admin_password 7ae0982e4f73
+ chown -R grafana:grafana /var/lib/grafana/grafana.db
+ service_enable grafana-server
+ service=grafana-server
+ [ 1 -eq 0 ]
+ systemctl enable grafana-server
Synchronizing state of grafana-server.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable grafana-server
Created symlink /etc/systemd/system/multi-user.target.wants/grafana-server.service → /usr/lib/systemd/system/grafana-server.service.
+ service_restart grafana-server
+ service=grafana-server
+ [ 1 -eq 0 ]
+ systemctl restart grafana-server
+ grafana-cli admin reset-admin-password --config /etc/grafana/grafana.ini --homepath /usr/share/grafana 7ae0982e4f73
t=2019-05-02T14:14:13-0600 lvl=info msg="Connecting to DB" logger=sqlstore dbtype=sqlite3
t=2019-05-02T14:14:13-0600 lvl=info msg="Starting DB migration" logger=migrator

Admin password changed successfully ✔

+ service_restart grafana-server
+ service=grafana-server
+ [ 1 -eq 0 ]
+ systemctl restart grafana-server
+ echo select id from org where id=1
+ sqlite3 /var/lib/grafana/grafana.db
+ grep -q 1
+ [ ! 0 -eq 0 ]
+ echo select login from user where login='admin'
+ sqlite3 /var/lib/grafana/grafana.db
+ grep -q admin
+ [ ! 0 -eq 0 ]
+ grafana-cli admin reset-admin-password --config /etc/grafana/grafana.ini --homepath /usr/share/grafana 7ae0982e4f73
t=2019-05-02T14:14:13-0600 lvl=info msg="Connecting to DB" logger=sqlstore dbtype=sqlite3
t=2019-05-02T14:14:13-0600 lvl=info msg="Starting DB migration" logger=migrator

Admin password changed successfully ✔

+ grafana-cli plugins install gnocchixyz-gnocchi-datasource
installing gnocchixyz-gnocchi-datasource @ 1.7.0
from url: https://grafana.com/api/plugins/gnocchixyz-gnocchi-datasource/versions/1.7.0/download
into: /var/lib/grafana/plugins

✔ Installed gnocchixyz-gnocchi-datasource successfully 

Restart grafana after installing plugins . <service grafana-server restart>

+ chown -R grafana:grafana /var/lib/grafana/grafana.db
+ service_restart grafana-server
+ service=grafana-server
+ [ 1 -eq 0 ]
+ systemctl restart grafana-server
+ openstack token issue
+ awk / id / { print $4 }
+ TMPTOKEN=gAAAAABcy0-Yh-zvEwh0NehXt8ggVJ2TcYJDOjEY3-lmxg3h9661OhYmGhMQrCnMnmBsVas0guYSwAUdWWvuWDFmAyZiCAwRHvBQWihcoBrs4FEaNOF2utjXOBK2-7jtMVTQErljnbIxlVhnR1uM6Ep_6glyqMCO5UMQDgT_I3HkfReWyWTEqNQ
+ + echosqlite3 /var/lib/grafana/grafana.db
 INSERT INTO "data_source" (id,org_id,version,type,name,access,url,password,user,database,basic_auth,basic_auth_user,basic_auth_password,is_default,json_data,created,updated,with_credentials,secure_json_data) VALUES(1,1,1,'gnocchixyz-gnocchi-datasource','gnocchi','proxy','http://localhost:8041/','','','',0,'','',1,'{"mode":"token","token":"gAAAAABcy0-Yh-zvEwh0NehXt8ggVJ2TcYJDOjEY3-lmxg3h9661OhYmGhMQrCnMnmBsVas0guYSwAUdWWvuWDFmAyZiCAwRHvBQWihcoBrs4FEaNOF2utjXOBK2-7jtMVTQErljnbIxlVhnR1uM6Ep_6glyqMCO5UMQDgT_I3HkfReWyWTEqNQ"}',datetime('now'),datetime('now'),0,'{}');
+ cat
+ systemctl daemon-reload
+ service_enable grafana-gnocchi-openstack-token-renewer
+ service=grafana-gnocchi-openstack-token-renewer
+ [ 1 -eq 0 ]
+ systemctl enable grafana-gnocchi-openstack-token-renewer
Created symlink /etc/systemd/system/multi-user.target.wants/grafana-gnocchi-openstack-token-renewer.service → /etc/systemd/system/grafana-gnocchi-openstack-token-renewer.service.
+ service_restart grafana-gnocchi-openstack-token-renewer
+ service=grafana-gnocchi-openstack-token-renewer
+ [ 1 -eq 0 ]
+ systemctl restart grafana-gnocchi-openstack-token-renewer
+ [ 17 -lt 17 ]
+ ceilometer-upgrade --debug --skip-gnocchi-resource-types
+ echo import base64; import sys; sys.stdout.write(base64.b64encode('admin:7ae0982e4f73'));
+ python
+ AUTHSTR=YWRtaW46N2FlMDk4MmU0Zjcz
+ [ -f /local/repository/etc/grafana-default-dashboard-queens.json ]
+ curl -X POST -H Content-type: application/json -H Authorization: Basic YWRtaW46N2FlMDk4MmU0Zjcz -d @/local/repository/etc/grafana-default-dashboard-queens.json http://localhost:3000/api/dashboards/import
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 33398  100   295  100 33103   4916   538k --:--:-- --:--:-- --:--:--  543k
{"pluginId":"","title":"OpenStack Instance Statistics","imported":true,"importedUri":"db/openstack-instance-statistics","importedUrl":"/d/c3GVu6kZz/openstack-instance-statistics","slug":"","dashboardId":0,"folderId":0,"importedRevision":1,"revision":1,"description":"","path":"","removed":false}+ echo TELEMETRY_GRAFANA_DONE="1"
+ logtend grafana
+ area=grafana
+ echo grafana
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=grafana
+ date +%s
+ stamp=1556828057
+ date
+ date=Thu May  2 14:14:17 MDT 2019
+ eval tss=$LOGTIMESTART_grafana
+ tss=1556828044
+ expr 1556828057 - 1556828044
+ tsres=13
+ perl -e print 13 / 60.0 . "\n"
+ resmin=0.216666666666667
+ echo END grafana 1556828057 Thu May  2 14:14:17 MDT 2019
+ echo TOTAL grafana 13 0.216666666666667
+ [ -z  ]
+ logtstart ceilometer-nodes
+ area=ceilometer-nodes
+ echo ceilometer-nodes
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_nodes
+ date +%s
+ stamp=1556828057
+ date
+ date=Thu May  2 14:14:17 MDT 2019
+ eval LOGTIMESTART_ceilometer_nodes=1556828057
+ LOGTIMESTART_ceilometer_nodes=1556828057
+ echo START ceilometer-nodes 1556828057 Thu May  2 14:14:17 MDT 2019
+ TELEMETRY_COMPUTENODES_DONE=1
+ PHOSTS=
+ mkdir -p /root/setup/pssh.setup-compute-telemetry.stdout /root/setup/pssh.setup-compute-telemetry.stderr
+ getfqdn compute-1
+ n=compute-1
+ cat /root/setup/fqdn.map
+ grep -E compute-1\s
+ cut -f2
+ fqdn=compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ echo compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ fqdn=compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ scp -o StrictHostKeyChecking=no /root/setup/settings compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us:/root/setup/settings
+ PHOSTS= -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ echo *** Setting up Compute telemetry on nodes:  -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
*** Setting up Compute telemetry on nodes:  -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ /usr/bin/parallel-ssh -t 0 -O StrictHostKeyChecking=no -H compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us -o /root/setup/pssh.setup-compute-telemetry.stdout -e /root/setup/pssh.setup-compute-telemetry.stderr /local/repository/setup-compute-telemetry.sh
[1] 14:14:20 [SUCCESS] compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ touch /root/setup/compute-telemetry-done-compute-1
+ echo TELEMETRY_COMPUTENODES_DONE="1"
+ logtend ceilometer-nodes
+ area=ceilometer-nodes
+ echo ceilometer-nodes
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_nodes
+ date +%s
+ stamp=1556828060
+ date
+ date=Thu May  2 14:14:20 MDT 2019
+ eval tss=$LOGTIMESTART_ceilometer_nodes
+ tss=1556828057
+ expr 1556828060 - 1556828057
+ tsres=3
+ perl -e print 3 / 60.0 . "\n"
+ resmin=0.05
+ echo END ceilometer-nodes 1556828060 Thu May  2 14:14:20 MDT 2019
+ echo TOTAL ceilometer-nodes 3 0.05
+ [ -z  ]
+ logtstart ceilometer-glance
+ area=ceilometer-glance
+ echo ceilometer-glance
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_glance
+ date +%s
+ stamp=1556828060
+ date
+ date=Thu May  2 14:14:20 MDT 2019
+ eval LOGTIMESTART_ceilometer_glance=1556828060
+ LOGTIMESTART_ceilometer_glance=1556828060
+ echo START ceilometer-glance 1556828060 Thu May  2 14:14:20 MDT 2019
+ TELEMETRY_GLANCE_DONE=1
+ [ 17 -ge 12 ]
+ RIS=oslo_messaging_rabbit
+ [ 17 -lt 13 ]
+ crudini --set /etc/glance/glance-api.conf oslo_messaging_notifications driver messagingv2
+ [ 17 -lt 14 ]
+ crudini --set /etc/glance/glance-api.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 13 ]
+ crudini --set /etc/glance/glance-registry.conf oslo_messaging_notifications driver messagingv2
+ [ 17 -lt 14 ]
+ crudini --set /etc/glance/glance-registry.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ service_restart glance-registry
+ service=glance-registry
+ [ 1 -eq 0 ]
+ systemctl restart glance-registry
+ service_restart glance-api
+ service=glance-api
+ [ 1 -eq 0 ]
+ systemctl restart glance-api
+ echo TELEMETRY_GLANCE_DONE="1"
+ logtend ceilometer-glance
+ area=ceilometer-glance
+ echo ceilometer-glance
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_glance
+ date +%s
+ stamp=1556828061
+ date
+ date=Thu May  2 14:14:21 MDT 2019
+ eval tss=$LOGTIMESTART_ceilometer_glance
+ tss=1556828060
+ expr 1556828061 - 1556828060
+ tsres=1
+ perl -e print 1 / 60.0 . "\n"
+ resmin=0.0166666666666667
+ echo END ceilometer-glance 1556828061 Thu May  2 14:14:21 MDT 2019
+ echo TOTAL ceilometer-glance 1 0.0166666666666667
+ [ -z  ]
+ logtstart ceilometer-cinder
+ area=ceilometer-cinder
+ echo ceilometer-cinder
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_cinder
+ date +%s
+ stamp=1556828061
+ date
+ date=Thu May  2 14:14:21 MDT 2019
+ eval LOGTIMESTART_ceilometer_cinder=1556828061
+ LOGTIMESTART_ceilometer_cinder=1556828061
+ echo START ceilometer-cinder 1556828061 Thu May  2 14:14:21 MDT 2019
+ TELEMETRY_CINDER_DONE=1
+ crudini --set /etc/cinder/cinder.conf DEFAULT control_exchange cinder
+ crudini --set /etc/cinder/cinder.conf DEFAULT notification_driver messagingv2
+ service_restart cinder-api
+ service=cinder-api
+ [ 1 -eq 0 ]
+ systemctl restart cinder-api
Failed to restart cinder-api.service: Unit cinder-api.service not found.
+ service_restart cinder-scheduler
+ service=cinder-scheduler
+ [ 1 -eq 0 ]
+ systemctl restart cinder-scheduler
+ getfqdn ctl
+ n=ctl
+ cat /root/setup/fqdn.map
+ grep -E ctl\s
+ cut -f2
+ fqdn=
+ echo
+ fqdn=
+ [ ctl = controller ]
+ scp -o StrictHostKeyChecking=no /root/setup/settings :/root/setup/settings
cp: cannot create regular file ':/root/setup/settings': No such file or directory
+ ssh -o StrictHostKeyChecking=no /local/repository/setup-storage-telemetry.sh
Pseudo-terminal will not be allocated because stdin is not a terminal.
ssh: Could not resolve hostname /local/repository/setup-storage-telemetry.sh: Name or service not known
+ echo TELEMETRY_CINDER_DONE="1"
+ logtend ceilometer-cinder
+ area=ceilometer-cinder
+ echo ceilometer-cinder
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_cinder
+ date +%s
+ stamp=1556828076
+ date
+ date=Thu May  2 14:14:36 MDT 2019
+ eval tss=$LOGTIMESTART_ceilometer_cinder
+ tss=1556828061
+ expr 1556828076 - 1556828061
+ tsres=15
+ perl -e print 15 / 60.0 . "\n"
+ resmin=0.25
+ echo END ceilometer-cinder 1556828076 Thu May  2 14:14:36 MDT 2019
+ echo TOTAL ceilometer-cinder 15 0.25
+ [ -z  ]
+ logtstart ceilometer-swift
+ area=ceilometer-swift
+ echo ceilometer-swift
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_swift
+ date +%s
+ stamp=1556828076
+ date
+ date=Thu May  2 14:14:36 MDT 2019
+ eval LOGTIMESTART_ceilometer_swift=1556828076
+ LOGTIMESTART_ceilometer_swift=1556828076
+ echo START ceilometer-swift 1556828076 Thu May  2 14:14:36 MDT 2019
+ TELEMETRY_SWIFT_DONE=1
+ chmod g+w /var/log/ceilometer
+ [ ! 1 -eq 1 ]
+ maybe_install_packages python-ceilometermiddleware
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-ceilometermiddleware
+ retval=1
+ [ ! -z python-ceilometermiddleware ]
+ dpkg -s python-ceilometermiddleware
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -le 10 ]
+ __openstack role create ResellerAdmin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role create ResellerAdmin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | e9ce75acd5284b5492e18a43932800a3 |
| name      | ResellerAdmin                    |
+-----------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --project service --user ceilometer ResellerAdmin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --project service --user ceilometer ResellerAdmin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 17 -le 11 ]
+ [ 17 -ge 11 ]
+ crudini --set /etc/swift/proxy-server.conf filter:keystoneauth operator_roles admin, user, ResellerAdmin
+ crudini --set /etc/swift/proxy-server.conf filter:ceilometer paste.filter_factory ceilometermiddleware.swift:filter_factory
+ crudini --set /etc/swift/proxy-server.conf filter:ceilometer control_exchange swift
+ crudini --set /etc/swift/proxy-server.conf filter:ceilometer url rabbit://openstack:c00d09297284c93378d5@controller:5672/
+ crudini --set /etc/swift/proxy-server.conf filter:ceilometer driver messagingv2
+ crudini --set /etc/swift/proxy-server.conf filter:ceilometer topic notifications
+ crudini --set /etc/swift/proxy-server.conf filter:ceilometer log_level WARN
+ usermod -a -G ceilometer swift
+ sed -i -e s/^\(pipeline.*=\)\(.*\)$/\1 ceilometer \2/ /etc/swift/proxy-server.conf
+ sed -i -e s/^\(operator_roles.*=.*\)$/\1,ResellerAdmin/ /etc/swift/proxy-server.conf
+ service_restart swift-proxy
+ service=swift-proxy
+ [ 1 -eq 0 ]
+ systemctl restart swift-proxy
+ echo TELEMETRY_SWIFT_DONE="1"
+ logtend ceilometer-swift
+ area=ceilometer-swift
+ echo ceilometer-swift
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_swift
+ date +%s
+ stamp=1556828081
+ date
+ date=Thu May  2 14:14:41 MDT 2019
+ eval tss=$LOGTIMESTART_ceilometer_swift
+ tss=1556828076
+ expr 1556828081 - 1556828076
+ tsres=5
+ perl -e print 5 / 60.0 . "\n"
+ resmin=0.0833333333333333
+ echo END ceilometer-swift 1556828081 Thu May  2 14:14:41 MDT 2019
+ echo TOTAL ceilometer-swift 5 0.0833333333333333
+ [ -z  ]
+ logtstart ceilometer-heat
+ area=ceilometer-heat
+ echo ceilometer-heat
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_heat
+ date +%s
+ stamp=1556828081
+ date
+ date=Thu May  2 14:14:41 MDT 2019
+ eval LOGTIMESTART_ceilometer_heat=1556828081
+ LOGTIMESTART_ceilometer_heat=1556828081
+ echo START ceilometer-heat 1556828081 Thu May  2 14:14:41 MDT 2019
+ TELEMETRY_HEAT_DONE=1
+ [ 17 -lt 13 ]
+ crudini --set /etc/heat/heat.conf oslo_messaging_notifications driver messagingv2
+ service_restart heat-api
+ service=heat-api
+ [ 1 -eq 0 ]
+ systemctl restart heat-api
+ service_restart heat-api-cfn
+ service=heat-api-cfn
+ [ 1 -eq 0 ]
+ systemctl restart heat-api-cfn
+ service_restart heat-engine
+ service=heat-engine
+ [ 1 -eq 0 ]
+ systemctl restart heat-engine
+ echo TELEMETRY_HEAT_DONE="1"
+ logtend ceilometer-heat
+ area=ceilometer-heat
+ echo ceilometer-heat
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ceilometer_heat
+ date +%s
+ stamp=1556828081
+ date
+ date=Thu May  2 14:14:41 MDT 2019
+ eval tss=$LOGTIMESTART_ceilometer_heat
+ tss=1556828081
+ expr 1556828081 - 1556828081
+ tsres=0
+ perl -e print 0 / 60.0 . "\n"
+ resmin=0
+ echo END ceilometer-heat 1556828081 Thu May  2 14:14:41 MDT 2019
+ echo TOTAL ceilometer-heat 0 0
+ [ -z  ]
+ logtstart trove
+ area=trove
+ echo trove
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=trove
+ date +%s
+ stamp=1556828081
+ date
+ date=Thu May  2 14:14:41 MDT 2019
+ eval LOGTIMESTART_trove=1556828081
+ LOGTIMESTART_trove=1556828081
+ echo START trove 1556828081 Thu May  2 14:14:41 MDT 2019
+ openssl rand -hex 10
+ TROVE_DBPASS=0fdb6096bfc95bfbe6fa
+ openssl rand -hex 10
+ TROVE_PASS=281e3e9f3a568a57226e
+ maybe_install_packages trove-common
+ [ ! 0 -eq 0 ]
+ are_packages_installed trove-common
+ retval=1
+ [ ! -z trove-common ]
+ dpkg -s trove-common
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! 0 -eq 0 ]
+ maybe_install_packages python-trove python-troveclient python-glanceclient trove-api trove-taskmanager trove-conductor
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-trove python-troveclient python-glanceclient trove-api trove-taskmanager trove-conductor
+ retval=1
+ [ ! -z python-trove ]
+ dpkg -s python-trove
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-troveclient ]
+ dpkg -s python-troveclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z python-glanceclient ]
+ dpkg -s python-glanceclient
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z trove-api ]
+ dpkg -s trove-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z trove-taskmanager ]
+ dpkg -s trove-taskmanager
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z trove-conductor ]
+ dpkg -s trove-conductor
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -ge 13 ]
+ apt-cache search --names-only ^python-trove-dashboard$
+ wc -l
+ sepdashpkg=1
+ [ ! 1 = 0 ]
+ madedir=0
+ [ ! -f /var/lib/openstack-dashboard/secret-key/.secret_key_store ]
+ [ ! -d /var/lib/openstack-dashboard/secret-key ]
+ touch /var/lib/openstack-dashboard/secret-key/.secret_key_store
+ maybe_install_packages python-trove-dashboard
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-trove-dashboard
+ retval=1
+ [ ! -z python-trove-dashboard ]
+ dpkg -s python-trove-dashboard
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 0 -eq 1 ]
+ echo create database trove
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on trove.* to 'trove'@'localhost' identified by '0fdb6096bfc95bfbe6fa'
+ mysql -u root --password=26e250af84ca2965f0e6
+ + echo grant all privileges on trove.* to 'trove'@'%' identified by '0fdb6096bfc95bfbe6fa'
mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password 281e3e9f3a568a57226e trove
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 281e3e9f3a568a57226e trove
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | b91edc2fa71544a987b27b19f64ff33a |
| name                | trove                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user trove --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user trove --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name trove --description OpenStack Database Service database
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name trove --description OpenStack Database Service database
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Database Service       |
| enabled     | True                             |
| id          | 714da952347843848627ddf0907c730e |
| name        | trove                            |
| type        | database                         |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne database public http://controller:8779/v1.0/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne database public http://controller:8779/v1.0/%(tenant_id)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 4d6eec5865a4459db9a4e30cafeefe5c          |
| interface    | public                                    |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 714da952347843848627ddf0907c730e          |
| service_name | trove                                     |
| service_type | database                                  |
| url          | http://controller:8779/v1.0/%(tenant_id)s |
+--------------+-------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne database internal http://controller:8779/v1.0/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne database internal http://controller:8779/v1.0/%(tenant_id)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | ec6bc30c802f48158eeec48042f272ed          |
| interface    | internal                                  |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 714da952347843848627ddf0907c730e          |
| service_name | trove                                     |
| service_type | database                                  |
| url          | http://controller:8779/v1.0/%(tenant_id)s |
+--------------+-------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne database admin http://controller:8779/v1.0/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne database admin http://controller:8779/v1.0/%(tenant_id)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | acde9652d2f448b1a112e6850cb37d25          |
| interface    | admin                                     |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 714da952347843848627ddf0907c730e          |
| service_name | trove                                     |
| service_type | database                                  |
| url          | http://controller:8779/v1.0/%(tenant_id)s |
+--------------+-------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ crudini --set /etc/trove/trove.conf DEFAULT verbose False
+ crudini --set /etc/trove/trove.conf DEFAULT debug False
+ crudini --set /etc/trove/trove.conf DEFAULT log_dir /var/log/trove
+ crudini --set /etc/ceilometer/ceilometer.conf DEFAULT bind_host 192.168.0.1
+ [ 17 -lt 12 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/trove/trove.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ crudini --set /etc/trove/trove.conf DEFAULT trove_auth_url http://controller:5000/v3
+ crudini --set /etc/trove/trove.conf DEFAULT nova_compute_url http://controller:8774/v2.1
+ crudini --set /etc/trove/trove.conf DEFAULT cinder_url http://controller:8776/v1
+ crudini --set /etc/trove/trove.conf DEFAULT swift_url http://controller:8080/v1/AUTH_
+ crudini --set /etc/trove/trove.conf DEFAULT sql_connection mysql+pymysql://trove:0fdb6096bfc95bfbe6fa@controller/trove
+ crudini --set /etc/trove/trove.conf database connection mysql+pymysql://trove:0fdb6096bfc95bfbe6fa@controller/trove
+ crudini --set /etc/trove/trove.conf DEFAULT notifier_queue_hostname controller
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT verbose False
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT debug False
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT log_dir /var/log/trove
+ [ 17 -lt 12 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT trove_auth_url http://controller:5000/v3
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT nova_compute_url http://controller:8774/v2.1
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT cinder_url http://controller:8776/v1
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT swift_url http://controller:8080/v1/AUTH_
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT sql_connection mysql+pymysql://trove:0fdb6096bfc95bfbe6fa@controller/trove
+ crudini --set /etc/trove/trove-taskmanager.conf database connection mysql+pymysql://trove:0fdb6096bfc95bfbe6fa@controller/trove
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT notifier_queue_hostname controller
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT verbose False
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT debug False
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT log_dir /var/log/trove
+ [ 17 -lt 12 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT trove_auth_url http://controller:5000/v3
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT nova_compute_url http://controller:8774/v2.1
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT cinder_url http://controller:8776/v1
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT swift_url http://controller:8080/v1/AUTH_
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT sql_connection mysql+pymysql://trove:0fdb6096bfc95bfbe6fa@controller/trove
+ crudini --set /etc/trove/trove-conductor.conf database connection mysql+pymysql://trove:0fdb6096bfc95bfbe6fa@controller/trove
+ crudini --set /etc/trove/trove-conductor.conf DEFAULT notifier_queue_hostname controller
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT nova_proxy_admin_user adminapi
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT nova_proxy_admin_pass a9f3d26fda5b4ee3d9a8
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT nova_proxy_admin_tenant_name service
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT taskmanager_manager trove.taskmanager.manager.Manager
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT
+ crudini --set /etc/trove/trove-taskmanager.conf DEFAULT
+ crudini --set /etc/trove/trove.conf DEFAULT default_datastore mysql
+ crudini --set /etc/trove/trove.conf DEFAULT add_addresses True
+ crudini --set /etc/trove/trove.conf DEFAULT network_label_regex ^NETWORK_LABEL$
+ crudini --set /etc/trove/trove.conf DEFAULT api_paste_config /etc/trove/api-paste.ini
+ cat
+ [ 17 -lt 11 ]
+ crudini --set /etc/trove/trove.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/trove/trove.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/trove/trove.conf keystone_authtoken auth_type password
+ crudini --set /etc/trove/trove.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/trove/trove.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/trove/trove.conf keystone_authtoken project_name service
+ crudini --set /etc/trove/trove.conf keystone_authtoken username trove
+ crudini --set /etc/trove/trove.conf keystone_authtoken password 281e3e9f3a568a57226e
+ crudini --set /etc/trove/trove.conf keystone_authtoken region_name RegionOne
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/trove/trove.conf keystone_authtoken memcached_servers controller:11211
+ sed -i -e s/^\(.*auth_host.*=.*\)$/#\1/ /etc/trove/api-paste.ini
+ sed -i -e s/^\(.*auth_port.*=.*\)$/#\1/ /etc/trove/api-paste.ini
+ sed -i -e s/^\(.*auth_protocol.*=.*\)$/#\1/ /etc/trove/api-paste.ini
+ mkdir -p /var/cache/trove
+ chown -R trove:trove /var/cache/trove
+ sed -i.orig -E -e s|(CONFIG_FILE=.*)$|CONFIG_FILE="/etc/trove/trove-taskmanager.conf"| /etc/init/trove-taskmanager.conf
+ sed -i.orig -E -e s|(CONFIG_FILE=.*)$|CONFIG_FILE="/etc/trove/trove-conductor.conf"| /etc/init/trove-conductor.conf
+ sed -i.orig -E -e s|(CONFIG_FILE=.*)$|CONFIG_FILE="/etc/trove/trove-taskmanager.conf"| /etc/init.d/trove-taskmanager
+ sed -i.orig -E -e s|(CONFIG_FILE=.*)$|CONFIG_FILE="/etc/trove/trove-conductor.conf"| /etc/init.d/trove-conductor
+ [ 1 -eq 1 ]
+ systemctl daemon-reload
+ su -s /bin/sh -c /usr/bin/trove-manage db_sync trove
/usr/lib/python2.7/dist-packages/sqlalchemy/sql/elements.py:4323: SAWarning: Textual column expression 'id' should be explicitly declared with text('id'), or use column('id') for more specificity (this warning may be suppressed after 10 occurrences)
  if guess_is_literal else "column"
+ [ ! 0 -eq 0 ]
+ su -s /bin/sh -c trove-manage datastore_update mysql '' trove
Datastore 'mysql' updated.
+ service_restart trove-api
+ service=trove-api
+ [ 1 -eq 0 ]
+ systemctl restart trove-api
+ service_enable trove-api
+ service=trove-api
+ [ 1 -eq 0 ]
+ systemctl enable trove-api
Synchronizing state of trove-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable trove-api
+ service_restart trove-taskmanager
+ service=trove-taskmanager
+ [ 1 -eq 0 ]
+ systemctl restart trove-taskmanager
+ service_enable trove-taskmanager
+ service=trove-taskmanager
+ [ 1 -eq 0 ]
+ systemctl enable trove-taskmanager
Synchronizing state of trove-taskmanager.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable trove-taskmanager
+ service_restart trove-conductor
+ service=trove-conductor
+ [ 1 -eq 0 ]
+ systemctl restart trove-conductor
+ service_enable trove-conductor
+ service=trove-conductor
+ [ 1 -eq 0 ]
+ systemctl enable trove-conductor
Synchronizing state of trove-conductor.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable trove-conductor
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ echo TROVE_DBPASS="0fdb6096bfc95bfbe6fa"
+ echo TROVE_PASS="281e3e9f3a568a57226e"
+ logtend trove
+ area=trove
+ echo trove
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=trove
+ date +%s
+ stamp=1556828096
+ date
+ date=Thu May  2 14:14:56 MDT 2019
+ eval tss=$LOGTIMESTART_trove
+ tss=1556828081
+ expr 1556828096 - 1556828081
+ tsres=15
+ perl -e print 15 / 60.0 . "\n"
+ resmin=0.25
+ echo END trove 1556828096 Thu May  2 14:14:56 MDT 2019
+ echo TOTAL trove 15 0.25
+ [ -z  ]
+ logtstart sahara
+ area=sahara
+ echo sahara
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=sahara
+ date +%s
+ stamp=1556828096
+ date
+ date=Thu May  2 14:14:56 MDT 2019
+ eval LOGTIMESTART_sahara=1556828096
+ LOGTIMESTART_sahara=1556828096
+ echo START sahara 1556828096 Thu May  2 14:14:56 MDT 2019
+ openssl rand -hex 10
+ SAHARA_DBPASS=9b8ad8f0992bacc1ad64
+ openssl rand -hex 10
+ SAHARA_PASS=734a389dbca1ab92a14e
+ echo create database sahara
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on sahara.* to 'sahara'@'localhost' identified by '9b8ad8f0992bacc1ad64'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on sahara.* to 'sahara'@'%' identified by '9b8ad8f0992bacc1ad64'
+ mysql -u root --password=26e250af84ca2965f0e6
+ [ 17 -eq 10 ]
+ __openstack user create --domain default --password 734a389dbca1ab92a14e sahara
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 734a389dbca1ab92a14e sahara
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | dccc953befdb43b4b4c3f04912416ea0 |
| name                | sahara                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user sahara --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user sahara --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name sahara --description OpenStack Data Processing Service data_processing
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name sahara --description OpenStack Data Processing Service data_processing
+-------------+-----------------------------------+
| Field       | Value                             |
+-------------+-----------------------------------+
| description | OpenStack Data Processing Service |
| enabled     | True                              |
| id          | 3a215d7f7b75405881e59bce1344e381  |
| name        | sahara                            |
| type        | data_processing                   |
+-------------+-----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne data_processing public http://controller:8386/v1.1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne data_processing public http://controller:8386/v1.1/%(tenant_id)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 2ac8744583be489880b0b3d538fcceab          |
| interface    | public                                    |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 3a215d7f7b75405881e59bce1344e381          |
| service_name | sahara                                    |
| service_type | data_processing                           |
| url          | http://controller:8386/v1.1/%(tenant_id)s |
+--------------+-------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne data_processing internal http://controller:8386/v1.1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne data_processing internal http://controller:8386/v1.1/%(tenant_id)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 7e0b8e43c60f41f8af551ade1d84f209          |
| interface    | internal                                  |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 3a215d7f7b75405881e59bce1344e381          |
| service_name | sahara                                    |
| service_type | data_processing                           |
| url          | http://controller:8386/v1.1/%(tenant_id)s |
+--------------+-------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne data_processing admin http://controller:8386/v1.1/%(tenant_id)s
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne data_processing admin http://controller:8386/v1.1/%(tenant_id)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 5d32fafc9fba43ceb9270b8b4f08dc9d          |
| interface    | admin                                     |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 3a215d7f7b75405881e59bce1344e381          |
| service_name | sahara                                    |
| service_type | data_processing                           |
| url          | http://controller:8386/v1.1/%(tenant_id)s |
+--------------+-------------------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ aserr=0
+ apt-cache search ^sahara$
+ grep -q sahara
+ [ 0 -eq 0 ]
+ APT_HAS_SAHARA=1
+ [ 1 -eq 0 ]
+ maybe_install_packages sahara-common
+ [ ! 0 -eq 0 ]
+ are_packages_installed sahara-common
+ retval=1
+ [ ! -z sahara-common ]
+ dpkg -s sahara-common
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ aserr=0
+ maybe_install_packages sahara-api sahara-engine
+ [ ! 0 -eq 0 ]
+ are_packages_installed sahara-api sahara-engine
+ retval=1
+ [ ! -z sahara-api ]
+ dpkg -s sahara-api
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z sahara-engine ]
+ dpkg -s sahara-engine
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 17 -ge 13 ]
+ apt-cache search --names-only ^python-sahara-dashboard$
+ wc -l
+ sepdashpkg=1
+ [ ! 1 = 0 ]
+ madedir=0
+ [ ! -f /var/lib/openstack-dashboard/secret-key/.secret_key_store ]
+ maybe_install_packages python-sahara-dashboard
+ [ ! 0 -eq 0 ]
+ are_packages_installed python-sahara-dashboard
+ retval=1
+ [ ! -z python-sahara-dashboard ]
+ dpkg -s python-sahara-dashboard
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ 0 -eq 1 ]
+ mkdir -p /etc/sahara
+ touch /etc/sahara/sahara.conf
+ chown -R sahara /etc/sahara
+ crudini --set /etc/sahara/sahara.conf database connection mysql+pymysql://sahara:9b8ad8f0992bacc1ad64@controller/sahara
+ crudini --set /etc/sahara/sahara.conf DEFAULT verbose False
+ crudini --set /etc/sahara/sahara.conf DEFAULT debug False
+ crudini --set /etc/sahara/sahara.conf DEFAULT auth_strategy keystone
+ crudini --set /etc/sahara/sahara.conf DEFAULT use_neutron true
+ [ 17 -lt 11 ]
+ [ 17 -lt 14 ]
+ crudini --set /etc/sahara/sahara.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ [ 17 -lt 11 ]
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken auth_type password
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken project_name service
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken username sahara
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken password 734a389dbca1ab92a14e
+ [ 17 -ge 13 -o 1 -eq 1 ]
+ crudini --set /etc/sahara/sahara.conf keystone_authtoken memcached_servers controller:11211
+ crudini --set /etc/sahara/sahara.conf ec2authtoken auth_uri http://controller:5000/v3
+ sed -i -e s/^\(.*auth_host.*=.*\)$/#\1/ /etc/sahara/sahara.conf
+ sed -i -e s/^\(.*auth_port.*=.*\)$/#\1/ /etc/sahara/sahara.conf
+ sed -i -e s/^\(.*auth_protocol.*=.*\)$/#\1/ /etc/sahara/sahara.conf
+ [ ! 0 -eq 0 ]
+ sahara-db-manage --config-file /etc/sahara/sahara.conf upgrade head
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> 001, Icehouse release
INFO  [alembic.runtime.migration] Running upgrade 001 -> 002, placeholder
INFO  [alembic.runtime.migration] Running upgrade 002 -> 003, placeholder
INFO  [alembic.runtime.migration] Running upgrade 003 -> 004, placeholder
INFO  [alembic.runtime.migration] Running upgrade 004 -> 005, placeholder
INFO  [alembic.runtime.migration] Running upgrade 005 -> 006, placeholder
INFO  [alembic.runtime.migration] Running upgrade 006 -> 007, convert clusters.status_description to LongText
INFO  [alembic.runtime.migration] Running upgrade 007 -> 008, add security_groups field to node groups
INFO  [alembic.runtime.migration] Running upgrade 008 -> 009, add rollback info to cluster
INFO  [alembic.runtime.migration] Running upgrade 009 -> 010, add auto_security_groups flag to node group
INFO  [alembic.runtime.migration] Running upgrade 010 -> 011, add Sahara settings info to cluster
INFO  [alembic.runtime.migration] Running upgrade 011 -> 012, add availability_zone field to node groups
INFO  [alembic.runtime.migration] Running upgrade 012 -> 013, add volumes_availability_zone field to node groups
INFO  [alembic.runtime.migration] Running upgrade 013 -> 014, add_volume_type
INFO  [alembic.runtime.migration] Running upgrade 014 -> 015, add_events_objects
INFO  [alembic.runtime.migration] Running upgrade 015 -> 016, Add is_proxy_gateway
INFO  [alembic.runtime.migration] Running upgrade 016 -> 017, drop progress in JobExecution
INFO  [alembic.runtime.migration] Running upgrade 017 -> 018, add volume_local_to_instance flag
INFO  [alembic.runtime.migration] Running upgrade 018 -> 019, Add is_default field for cluster and node_group templates
INFO  [alembic.runtime.migration] Running upgrade 019 -> 020, remove redandunt progress ops
INFO  [alembic.runtime.migration] Running upgrade 020 -> 021, Add data_source_urls to job_executions to support placeholders
INFO  [alembic.runtime.migration] Running upgrade 021 -> 022, add_job_interface
INFO  [alembic.runtime.migration] Running upgrade 022 -> 023, add_use_autoconfig
INFO  [alembic.runtime.migration] Running upgrade 023 -> 024, manila_shares
INFO  [alembic.runtime.migration] Running upgrade 024 -> 025, Increase internal_ip and management_ip column size to work with IPv6
INFO  [alembic.runtime.migration] Running upgrade 025 -> 026, add is_public and is_protected flags
INFO  [alembic.runtime.migration] Running upgrade 026 -> 027, Rename oozie_job_id
INFO  [alembic.runtime.migration] Running upgrade 027 -> 028, add_storage_devices_number
INFO  [alembic.runtime.migration] Running upgrade 028 -> 029, set is_protected on is_default
INFO  [alembic.runtime.migration] Running upgrade 029 -> 030, health-check
INFO  [alembic.runtime.migration] Running upgrade 030 -> 031, added_plugins_table
INFO  [alembic.runtime.migration] Running upgrade 031 -> 032, 032_add_domain_name
INFO  [alembic.runtime.migration] Running upgrade 032 -> 033, 033_add anti_affinity_ratio field to cluster
+ mkdir -p /var/log/sahara
+ [ 1 -eq 0 ]
+ service_restart sahara-api
+ service=sahara-api
+ [ 1 -eq 0 ]
+ systemctl restart sahara-api
+ service_enable sahara-api
+ service=sahara-api
+ [ 1 -eq 0 ]
+ systemctl enable sahara-api
Synchronizing state of sahara-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable sahara-api
+ service_restart sahara-engine
+ service=sahara-engine
+ [ 1 -eq 0 ]
+ systemctl restart sahara-engine
+ service_enable sahara-engine
+ service=sahara-engine
+ [ 1 -eq 0 ]
+ systemctl enable sahara-engine
Synchronizing state of sahara-engine.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable sahara-engine
+ service_restart apache2
+ service=apache2
+ [ 1 -eq 0 ]
+ systemctl restart apache2
+ service_restart memcached
+ service=memcached
+ [ 1 -eq 0 ]
+ systemctl restart memcached
+ echo SAHARA_DBPASS="9b8ad8f0992bacc1ad64"
+ echo SAHARA_PASS="734a389dbca1ab92a14e"
+ logtend sahara
+ area=sahara
+ echo sahara
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=sahara
+ date +%s
+ stamp=1556828117
+ date
+ date=Thu May  2 14:15:17 MDT 2019
+ eval tss=$LOGTIMESTART_sahara
+ tss=1556828096
+ expr 1556828117 - 1556828096
+ tsres=21
+ perl -e print 21 / 60.0 . "\n"
+ resmin=0.35
+ echo END sahara 1556828117 Thu May  2 14:15:17 MDT 2019
+ echo TOTAL sahara 21 0.35
+ [ 0 = 1 -a queens = kilo -a -n  -a -z  ]
+ [ 17 -ge 14 -a -z  ]
+ logtstart designate
+ area=designate
+ echo designate
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=designate
+ date +%s
+ stamp=1556828117
+ date
+ date=Thu May  2 14:15:17 MDT 2019
+ eval LOGTIMESTART_designate=1556828117
+ LOGTIMESTART_designate=1556828117
+ echo START designate 1556828117 Thu May  2 14:15:17 MDT 2019
+ openssl rand -hex 10
+ DESIGNATE_DBPASS=36e7c7d33514571f4521
+ openssl rand -hex 10
+ DESIGNATE_PASS=6ed5bbda06adabaf6f64
+ echo create database designate
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on designate.* to 'designate'@'localhost' identified by '36e7c7d33514571f4521'
+ mysql -u root --password=26e250af84ca2965f0e6
+ echo grant all privileges on designate.* to 'designate'@'%' identified by '36e7c7d33514571f4521'
+ mysql -u root --password=26e250af84ca2965f0e6
+ __openstack user create --domain default --password 6ed5bbda06adabaf6f64 designate
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack user create --domain default --password 6ed5bbda06adabaf6f64 designate
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | 1876f1092a7c41c38cd70d7d116a9abc |
| enabled             | True                             |
| id                  | 7d8da1b1645e4618ad2df76e130e7056 |
| name                | designate                        |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack role add --user designate --project service admin
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack role add --user designate --project service admin
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack service create --name designate --description OpenStack Domain Name Service dns
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack service create --name designate --description OpenStack Domain Name Service dns
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Domain Name Service    |
| enabled     | True                             |
| id          | 4dd6f961ff724e3481ce99115a5248bf |
| name        | designate                        |
| type        | dns                              |
+-------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ [ 3 -lt 3 ]
+ __openstack endpoint create --region RegionOne dns public http://controller:9001/v2
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne dns public http://controller:9001/v2
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | cfe916dbbfaa4c22baaa95aafdc2c2b5 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 4dd6f961ff724e3481ce99115a5248bf |
| service_name | designate                        |
| service_type | dns                              |
| url          | http://controller:9001/v2        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne dns internal http://controller:9001/v2
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne dns internal http://controller:9001/v2
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | be9ae8ab91294d12830ebd38d6f0a0a3 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 4dd6f961ff724e3481ce99115a5248bf |
| service_name | designate                        |
| service_type | dns                              |
| url          | http://controller:9001/v2        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ __openstack endpoint create --region RegionOne dns admin http://controller:9001/v2
+ __err=1
+ __debug=
+ __times=0
+ [ 0 -lt 16 -a ! 1 -eq 0 ]
+ openstack endpoint create --region RegionOne dns admin http://controller:9001/v2
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | e255500ab14144978ae0fe87e5bc3934 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 4dd6f961ff724e3481ce99115a5248bf |
| service_name | designate                        |
| service_type | dns                              |
| url          | http://controller:9001/v2        |
+--------------+----------------------------------+
+ __err=0
+ [ 0 -eq 0 ]
+ break
+ maybe_install_packages designate bind9 bind9utils bind9-doc
+ [ ! 0 -eq 0 ]
+ are_packages_installed designate bind9 bind9utils bind9-doc
+ retval=1
+ [ ! -z designate ]
+ dpkg -s designate
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z bind9 ]
+ dpkg -s bind9
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z bind9utils ]
+ dpkg -s bind9utils
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z bind9-doc ]
+ dpkg -s bind9-doc
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ rndc-confgen -a -k designate -c /etc/designate/rndc.key
wrote key file "/etc/designate/rndc.key"
+ chgrp bind /etc/designate/rndc.key
+ chmod g+r /etc/designate/rndc.key
+ hostname
+ sed -n -e s/[^\.]*\.\(.*\)$/\1/p
+ mydomain=tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us
+ sed -n -e s/^nameserver \([0-9]*\.[0-9]*\.[0-9]*\.[0-9]*\).*$/\1/p
+ head -1
+ mynameserver=128.110.156.4
+ [ -z 128.110.156.4 ]
+ [ -z 128.110.156.4 ]
+ fstr=forwarders { 128.110.156.4; };
+ cat
+ service_enable bind9
+ service=bind9
+ [ 1 -eq 0 ]
+ systemctl enable bind9
Synchronizing state of bind9.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable bind9
+ service_restart bind9
+ service=bind9
+ [ 1 -eq 0 ]
+ systemctl restart bind9
+ crudini --set /etc/designate/designate.conf storage:sqlalchemy connection mysql+pymysql://designate:36e7c7d33514571f4521@controller/designate
+ crudini --del /etc/designate/designate.conf keystone_authtoken auth_host
+ crudini --del /etc/designate/designate.conf keystone_authtoken auth_port
+ crudini --del /etc/designate/designate.conf keystone_authtoken auth_protocol
+ crudini --set /etc/designate/designate.conf service:api auth_strategy keystone
+ crudini --set /etc/designate/designate.conf service:api api_host 0.0.0.0
+ crudini --set /etc/designate/designate.conf service:api api_port 9001
+ crudini --set /etc/designate/designate.conf service:api listen 0.0.0.0:9001
+ crudini --set /etc/designate/designate.conf service:api enable_api_v1 True
+ crudini --set /etc/designate/designate.conf service:api api_base_url http://controller:9001/
+ crudini --set /etc/designate/designate.conf service:api enabled_extensions_v1 quotas,reports
+ crudini --set /etc/designate/designate.conf service:api enable_api_v2 True
+ crudini --set /etc/designate/designate.conf service:api enabled_extensions_v2 quotas,reports
+ crudini --set /etc/designate/designate.conf service:worker enabled True
+ crudini --set /etc/designate/designate.conf service:worker notify True
+ crudini --set /etc/designate/designate.conf DEFAULT verbose False
+ crudini --set /etc/designate/designate.conf DEFAULT debug False
+ [ 17 -lt 14 ]
+ crudini --set /etc/designate/designate.conf DEFAULT transport_url rabbit://openstack:c00d09297284c93378d5@controller
+ crudini --del /etc/designate/designate.conf keystone_authtoken project_domain_id
+ crudini --del /etc/designate/designate.conf keystone_authtoken user_domain_id
+ crudini --set /etc/designate/designate.conf keystone_authtoken auth_uri http://controller:5000
+ crudini --set /etc/designate/designate.conf keystone_authtoken auth_url http://controller:5000
+ crudini --set /etc/designate/designate.conf keystone_authtoken auth_type password
+ crudini --set /etc/designate/designate.conf keystone_authtoken project_domain_name default
+ crudini --set /etc/designate/designate.conf keystone_authtoken user_domain_name default
+ crudini --set /etc/designate/designate.conf keystone_authtoken project_name service
+ crudini --set /etc/designate/designate.conf keystone_authtoken username designate
+ crudini --set /etc/designate/designate.conf keystone_authtoken password 6ed5bbda06adabaf6f64
+ crudini --set /etc/designate/designate.conf keystone_authtoken region_name RegionOne
+ crudini --set /etc/designate/designate.conf keystone_authtoken memcached_servers controller:11211
+ su -s /bin/sh -c designate-manage database sync designate
2019-05-02 14:15:36.981 21773 INFO migrate.versioning.api [designate-manage - - - - -] 69 -> 70... 
2019-05-02 14:15:37.150 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.150 21773 INFO migrate.versioning.api [designate-manage - - - - -] 70 -> 71... 
2019-05-02 14:15:37.157 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.158 21773 INFO migrate.versioning.api [designate-manage - - - - -] 71 -> 72... 
2019-05-02 14:15:37.165 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.165 21773 INFO migrate.versioning.api [designate-manage - - - - -] 72 -> 73... 
2019-05-02 14:15:37.172 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.173 21773 INFO migrate.versioning.api [designate-manage - - - - -] 73 -> 74... 
2019-05-02 14:15:37.180 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.180 21773 INFO migrate.versioning.api [designate-manage - - - - -] 74 -> 75... 
2019-05-02 14:15:37.188 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.188 21773 INFO migrate.versioning.api [designate-manage - - - - -] 75 -> 76... 
2019-05-02 14:15:37.195 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.196 21773 INFO migrate.versioning.api [designate-manage - - - - -] 76 -> 77... 
2019-05-02 14:15:37.203 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.203 21773 INFO migrate.versioning.api [designate-manage - - - - -] 77 -> 78... 
2019-05-02 14:15:37.211 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.212 21773 INFO migrate.versioning.api [designate-manage - - - - -] 78 -> 79... 
2019-05-02 14:15:37.219 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.219 21773 INFO migrate.versioning.api [designate-manage - - - - -] 79 -> 80... 
2019-05-02 14:15:37.332 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.332 21773 INFO migrate.versioning.api [designate-manage - - - - -] 80 -> 81... 
2019-05-02 14:15:37.472 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.473 21773 INFO migrate.versioning.api [designate-manage - - - - -] 81 -> 82... 
2019-05-02 14:15:37.496 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.496 21773 INFO migrate.versioning.api [designate-manage - - - - -] 82 -> 83... 
2019-05-02 14:15:37.534 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.534 21773 INFO migrate.versioning.api [designate-manage - - - - -] 83 -> 84... 
2019-05-02 14:15:37.536 21773 INFO 084_add_delayed_notify_column [designate-manage - - - - -] Adding boolean column delayed_notify to table 'zones'
2019-05-02 14:15:37.572 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.572 21773 INFO migrate.versioning.api [designate-manage - - - - -] 84 -> 85... 
2019-05-02 14:15:37.615 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.615 21773 INFO migrate.versioning.api [designate-manage - - - - -] 85 -> 86... 
2019-05-02 14:15:37.669 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.670 21773 INFO migrate.versioning.api [designate-manage - - - - -] 86 -> 87... 
2019-05-02 14:15:37.676 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.676 21773 INFO migrate.versioning.api [designate-manage - - - - -] 87 -> 88... 
2019-05-02 14:15:37.684 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.685 21773 INFO migrate.versioning.api [designate-manage - - - - -] 88 -> 89... 
2019-05-02 14:15:37.692 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.693 21773 INFO migrate.versioning.api [designate-manage - - - - -] 89 -> 90... 
2019-05-02 14:15:37.699 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.700 21773 INFO migrate.versioning.api [designate-manage - - - - -] 90 -> 91... 
2019-05-02 14:15:37.708 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.708 21773 INFO migrate.versioning.api [designate-manage - - - - -] 91 -> 92... 
2019-05-02 14:15:37.715 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.715 21773 INFO migrate.versioning.api [designate-manage - - - - -] 92 -> 93... 
2019-05-02 14:15:37.722 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.723 21773 INFO migrate.versioning.api [designate-manage - - - - -] 93 -> 94... 
2019-05-02 14:15:37.729 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.730 21773 INFO migrate.versioning.api [designate-manage - - - - -] 94 -> 95... 
2019-05-02 14:15:37.737 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.737 21773 INFO migrate.versioning.api [designate-manage - - - - -] 95 -> 96... 
2019-05-02 14:15:37.745 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.745 21773 INFO migrate.versioning.api [designate-manage - - - - -] 96 -> 97... 
2019-05-02 14:15:37.761 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.761 21773 INFO migrate.versioning.api [designate-manage - - - - -] 97 -> 98... 
2019-05-02 14:15:37.778 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
2019-05-02 14:15:37.780 21773 INFO migrate.versioning.api [designate-manage - - - - -] 98 -> 99... 
2019-05-02 14:15:37.823 21773 INFO migrate.versioning.api [designate-manage - - - - -] done
+ service_restart designate-central
+ service=designate-central
+ [ 1 -eq 0 ]
+ systemctl restart designate-central
+ service_enable designate-central
+ service=designate-central
+ [ 1 -eq 0 ]
+ systemctl enable designate-central
Synchronizing state of designate-central.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable designate-central
+ service_restart designate-api
+ service=designate-api
+ [ 1 -eq 0 ]
+ systemctl restart designate-api
+ service_enable designate-api
+ service=designate-api
+ [ 1 -eq 0 ]
+ systemctl enable designate-api
Synchronizing state of designate-api.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable designate-api
+ cat
+ chown designate /etc/designate/pools.yaml
+ su -s /bin/sh -c designate-manage pool update designate
Updating Pools Configuration
****************************
2019-05-02 14:15:40.187 21970 INFO designate.manage.pool [designate-manage - - - - -] Updating existing pool: <Pool id:'794ccc2c-d751-44fe-b57f-8894c9f5c842' name:'default'>

+ maybe_install_packages designate-worker designate-producer designate-mdns
+ [ ! 0 -eq 0 ]
+ are_packages_installed designate-worker designate-producer designate-mdns
+ retval=1
+ [ ! -z designate-worker ]
+ dpkg -s designate-worker
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z designate-producer ]
+ dpkg -s designate-producer
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z designate-mdns ]
+ dpkg -s designate-mdns
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ service_restart designate-worker
+ service=designate-worker
+ [ 1 -eq 0 ]
+ systemctl restart designate-worker
+ service_enable designate-worker
+ service=designate-worker
+ [ 1 -eq 0 ]
+ systemctl enable designate-worker
Synchronizing state of designate-worker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable designate-worker
+ service_restart designate-producer
+ service=designate-producer
+ [ 1 -eq 0 ]
+ systemctl restart designate-producer
+ service_enable designate-producer
+ service=designate-producer
+ [ 1 -eq 0 ]
+ systemctl enable designate-producer
Synchronizing state of designate-producer.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable designate-producer
+ service_restart designate-mdns
+ service=designate-mdns
+ [ 1 -eq 0 ]
+ systemctl restart designate-mdns
+ service_enable designate-mdns
+ service=designate-mdns
+ [ 1 -eq 0 ]
+ systemctl enable designate-mdns
Synchronizing state of designate-mdns.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable designate-mdns
+ rm -f /var/lib/designate/designate.sqlite
+ crudini --set /etc/neutron/neutron.conf DEFAULT dns_domain tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us.
+ crudini --set /etc/neutron/neutron.conf DEFAULT external_dns_driver designate
+ crudini --set /etc/neutron/neutron.conf designate url http://controller:9001/v2
+ crudini --set /etc/neutron/neutron.conf designate auth_url http://controller:5000
+ crudini --set /etc/neutron/neutron.conf designate allow_reverse_dns_lookup True
+ crudini --set /etc/neutron/neutron.conf designate ipv4_ptr_zone_prefix_size 24
+ crudini --set /etc/neutron/neutron.conf designate ipv6_ptr_zone_prefix_size 116
+ crudini --set /etc/neutron/neutron.conf designate project_domain_name default
+ crudini --set /etc/neutron/neutron.conf designate user_domain_name default
+ crudini --set /etc/neutron/neutron.conf designate auth_type password
+ crudini --set /etc/neutron/neutron.conf designate project_name service
+ crudini --set /etc/neutron/neutron.conf designate username designate
+ crudini --set /etc/neutron/neutron.conf designate password 6ed5bbda06adabaf6f64
+ crudini --set /etc/neutron/neutron.conf designate region_name RegionOne
+ crudini --set /etc/neutron/neutron.conf designate memcached_servers controller:11211
+ crudini --set /etc/neutron/neutron.conf designate insecure True
+ service_restart neutron-server
+ service=neutron-server
+ [ 1 -eq 0 ]
+ systemctl restart neutron-server
+ echo DESIGNATE_DBPASS="36e7c7d33514571f4521"
+ echo DESIGNATE_PASS="6ed5bbda06adabaf6f64"
+ logtend designate
+ area=designate
+ echo designate
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=designate
+ date +%s
+ stamp=1556828143
+ date
+ date=Thu May  2 14:15:43 MDT 2019
+ eval tss=$LOGTIMESTART_designate
+ tss=1556828117
+ expr 1556828143 - 1556828117
+ tsres=26
+ perl -e print 26 / 60.0 . "\n"
+ resmin=0.433333333333333
+ echo END designate 1556828143 Thu May  2 14:15:43 MDT 2019
+ echo TOTAL designate 26 0.433333333333333
+ [ -z  ]
+ /local/repository/setup-basic.sh
+ dirname /local/repository/setup-basic.sh
+ DIRNAME=/local/repository
+ [ -ne 0 ]
/local/repository/setup-basic.sh: 12: [: -ne: unexpected operator
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-basic.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=a8b1960cfdc6927218b1
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x = x ]
+ UPDATING=0
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ + perl -ecat /root/setup/manifests.0.xml
 $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } }
+ xargs
DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ [ controller != controller ]
+ logtstart basic
+ area=basic
+ echo basic
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=basic
+ date +%s
+ stamp=1556828143
+ date
+ date=Thu May  2 14:15:43 MDT 2019
+ eval LOGTIMESTART_basic=1556828143
+ LOGTIMESTART_basic=1556828143
+ echo START basic 1556828143 Thu May  2 14:15:43 MDT 2019
+ [ -f /root/setup/settings ]
+ . /root/setup/settings
+ GENIUSER=1
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ STORAGEHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=compute-1 
+ MIRRORSETUP=1
+ DB_ROOT_PASS=26e250af84ca2965f0e6
+ RABBIT_USER=openstack
+ RABBIT_PASS=c00d09297284c93378d5
+ RABBIT_URL=rabbit://openstack:c00d09297284c93378d5@controller
+ MEMCACHE_DONE=1
+ ADMIN_API=adminapi
+ ADMIN_API_PASS=a9f3d26fda5b4ee3d9a8
+ KEYSTONE_DBPASS=0a4f4c06d3fefbade0ee
+ GLANCE_DBPASS=a6614c0139dcbb0d7b6c
+ GLANCE_PASS=a3e74e251a8755f42955
+ NOVA_DBPASS=c959955373a27ee1342a
+ NOVA_PASS=1da42256777cafc9360e
+ PLACEMENT_PASS=4eb4cb430c0078c3cc6b
+ NOVA_COMPUTENODES_DONE=1
+ NEUTRON_DBPASS=8271678362114f5f9c59
+ NEUTRON_PASS=60bd8ad0f9e9306d6413
+ NEUTRON_METADATA_SECRET=1cb9ce7786a31ef78c37
+ NEUTRON_NETWORKMANAGER_DONE=1
+ NEUTRON_COMPUTENODES_DONE=1
+ NEUTRON_NETWORKS_DONE=1
+ DASHBOARD_DONE=1
+ CINDER_DBPASS=4b6df6d901da295f578c
+ CINDER_PASS=bcabcfa033c81284dec0
+ STORAGE_HOST_DONE=1
+ MANILA_DBPASS=f505ef603e7c86506660
+ MANILA_PASS=4ca14b3298a2cff8544c
+ SHARE_HOST_DONE=1
+ SWIFT_PASS=c7c1df7fd68c420c3e31
+ SWIFT_HASH_PATH_PREFIX=e659c49e9cdde2be26f9
+ SWIFT_HASH_PATH_SUFFIX=e7a14a31a9cd466e9ffc
+ OBJECT_HOST_DONE=1
+ OBJECT_RING_DONE=1
+ HEAT_DBPASS=be5944f95e4913f2eea7
+ HEAT_PASS=5081e31b64a08677226f
+ HEAT_DOMAIN_PASS=f20d96f0fd0c1fa51a4a
+ CEILOMETER_DBPASS=77faf52d26578538035d
+ CEILOMETER_PASS=35a506d19e014bd69529
+ CEILOMETER_SECRET=f53edec5f10a98ae265b
+ USING_GNOCCHI=1
+ GNOCCHI_DBPASS=c697aa9065aa23859ce5
+ GNOCCHI_PASS=8d14c20fa0dcc80d6a2e
+ TELEMETRY_GRAFANA_DONE=1
+ TELEMETRY_COMPUTENODES_DONE=1
+ TELEMETRY_GLANCE_DONE=1
+ TELEMETRY_CINDER_DONE=1
+ TELEMETRY_SWIFT_DONE=1
+ TELEMETRY_HEAT_DONE=1
+ TROVE_DBPASS=0fdb6096bfc95bfbe6fa
+ TROVE_PASS=281e3e9f3a568a57226e
+ SAHARA_DBPASS=9b8ad8f0992bacc1ad64
+ SAHARA_PASS=734a389dbca1ab92a14e
+ DESIGNATE_DBPASS=36e7c7d33514571f4521
+ DESIGNATE_PASS=6ed5bbda06adabaf6f64
+ . /root/setup/admin-openrc.sh
+ export OS_PROJECT_DOMAIN_NAME=default
+ export OS_USER_DOMAIN_NAME=default
+ export OS_PROJECT_NAME=admin
+ export OS_TENANT_NAME=admin
+ export OS_USERNAME=adminapi
+ export OS_PASSWORD=a9f3d26fda5b4ee3d9a8
+ export OS_AUTH_URL=http://controller:5000/v3
+ export OS_IDENTITY_API_VERSION=3
+ export OS_IMAGE_API_VERSION=2
+ export OS_AUTH_TYPE=password
+ echo *** Backgrounding quota setup...
*** Backgrounding quota setup...
+ quotaspid=22349
+ echo *** Backgrounding network setup...
*** Backgrounding network setup...
+ + networkspid=22350
/local/repository/setup-basic-quotas.sh+ 
echo *** Backgrounding user setup...
*** Backgrounding user setup...
+ + userspid=22351/local/repository/setup-basic-networks.sh

+ . /local/repository/setup-images-lib.sh
+ /local/repository/setup-basic-users.sh
+ dirname /local/repository/setup-basic.sh
+ DIRNAME=/local/repository
+ id -u
+ [ 0 -ne 0 ]
+ . /local/repository/setup-lib.sh
+ dirname /local/repository/setup-basic.sh
+ DIRNAME=/local/repository
+ OURDIR=/root/setup
+ SETTINGS=/root/setup/settings
+ LOCALSETTINGS=/root/setup/settings.local
+ TOPOMAP=/root/setup/topomap
+ BOOTDIR=/var/emulab/boot
+ TMCC=/usr/local/etc/emulab/tmcc
+ TIMELOGFILE=/root/setup/setup-time.log
+ FIRSTTIME=0
+ [ ! -f /root/setup/setup-lib-first ]
+ [ 0 -ne 0 ]
+ mkdir -p /root/setup
+ touch /root/setup/settings
+ touch /root/setup/settings.local
+ cd /root/setup
+ LOCKFILE=lockfile-create --retry 65535 
+ RMLOCKFILE=lockfile-remove 
+ PSWDGEN=openssl rand -hex 10
+ SSH=ssh -o StrictHostKeyChecking=no
+ SCP=scp -p -o StrictHostKeyChecking=no
+ CONTROLLER=ctl
+ NETWORKMANAGER=nm
+ STORAGEHOST=ctl
+ SHAREHOST=ctl
+ OBJECTHOST=ctl
+ COMPUTENODES=
+ BAREMETALNODES=
+ BLOCKNODES=
+ OBJECTNODES=
+ DATALAN=lan-1
+ MGMTLAN=lan-2
+ BLOCKLAN=
+ OBJECTLAN=
+ DATATUNNELS=1
+ DATAFLATLANS=lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATAOTHERLANS=
+ USE_EXISTING_IPS=1
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_APT_UPDATE=1
+ UBUNTUMIRRORHOST=
+ UBUNTUMIRRORPATH=
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DO_UBUNTU_CLOUDARCHIVE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ BUILD_AARCH64_FROM_CORE=0
+ DISABLE_SECURITY_GROUPS=0
+ ENABLE_HOST_PASSTHROUGH=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ SUPPORT_DYNAMIC_NODES=0
+ KEYSTONEAPIVERSION=
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ GLANCE_LV_SIZE=32
+ SWIFT_LV_SIZE=4
+ CEILOMETER_USE_WSGI=0
+ QUOTASOFF=1
+ KEYSTONEUSEMEMCACHE=0
+ KEYSTONEUSEWSGI=
+ COMPUTE_EXTRA_NOVA_DISK_SPACE=1
+ ML2PLUGIN=openvswitch
+ MANILADRIVER=generic
+ EXTRAIMAGEURLS=
+ LINUXBRIDGE_STATIC=0
+ USE_DESIGNATE_AS_RESOLVER=1
+ USE_NEUTRON_LBAAS=1
+ ENABLE_OPENSTACK_SLOTHD=0
+ OSRELEASE=
+ ADMIN_API=adminapi
+ openssl rand -hex 10
+ ADMIN_API_PASS=be358c800b4c7e3f0e24
+ ADMIN=admin
+ ADMIN_PASS=
+ ADMIN_PASS_HASH=
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ [ x0 = x ]
+ [ ! 0 -eq 0 ]
+ NEWNODELIST=
+ OLDNODELIST=
+ [ ! -e /root/setup/apt-configured ]
+ export DEBIAN_FRONTEND=noninteractive
+ DPKGOPTS=
+ APTGETINSTALLOPTS=-y
+ APTGETINSTALL=apt-get  install -y
+ [ 1 -eq 0 ]
+ grep GENIUSER /root/setup/settings
GENIUSER=1
+ [ ! 0 -eq 0 ]
+ grep GENIUSER=1 /root/setup/settings
GENIUSER=1
+ [ 0 -eq 0 ]
+ GENIUSER=1
+ [ 1 -eq 1 ]
+ dpkg -s python-m2crypto
+ [ ! 0 -eq 0 ]
+ [ ! -e /root/setup/geni.key ]
+ HAS_GENI_KEY=1
+ [ ! -e /root/setup/geni.certificate ]
+ HAS_GENI_CERT=1
+ [ ! -e /root/.ssl/encrypted.pem ]
+ [ ! -e /root/setup/manifests.xml -o 0 -ne 0 ]
+ [ ! -e /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/decrypted_admin_pass -a -s /root/setup/encrypted_admin_pass ]
+ [ ! -e /root/setup/parameters ]
+ . /root/setup/parameters
+ CONTROLLER=controller
+ NETWORKMANAGER=controller
+ COMPUTENODES=compute-1 
+ DATALANS=flat-lan-1
+ DATAFLATLANS=flat-lan-1
+ DATAVLANS=
+ DATAVXLANS=0
+ DATATUNNELS=1
+ MGMTLAN=
+ DO_APT_INSTALL=1
+ DO_APT_UPGRADE=0
+ DO_APT_DIST_UPGRADE=0
+ DO_UBUNTU_CLOUDARCHIVE_STAGING=0
+ DO_APT_UPDATE=1
+ ADMIN_PASS_HASH=
+ ENABLE_HOST_PASSTHROUGH=1
+ ENABLE_NEW_SERIAL_SUPPORT=0
+ DISABLE_SECURITY_GROUPS=0
+ DEFAULT_SECGROUP_ENABLE_SSH_ICMP=1
+ USE_NEUTRON_LBAAS=1
+ CEILOMETER_USE_MONGODB=0
+ VERBOSE_LOGGING=False
+ DEBUG_LOGGING=False
+ TOKENTIMEOUT=14400
+ SESSIONTIMEOUT=14400
+ KEYSTONEUSEMEMCACHE=0
+ QUOTASOFF=1
+ ML2PLUGIN=openvswitch
+ USE_DESIGNATE_AS_RESOLVER=1
+ EXTRAIMAGEURLS=
+ OSRELEASE=queens
+ SWIFT_LV_SIZE=4
+ GLANCE_LV_SIZE=32
+ ADMIN_PASS=7ae0982e4f73
+ ADMIN_PASS_HASH=$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1
+ [ x$1$JGzdJiHD$WN/XNXmqUjdSOwVAyxJPU1 = x ]
+ cat /var/emulab/boot/creator
+ CREATOR=tboudw0
+ cat /var/emulab/boot/swapper
+ SWAPPER=geniuser
+ cat /var/emulab/boot/nickname
+ cut -d . -f 1
+ NODEID=controller
+ cat /var/emulab/boot/nodeid
+ PNODEID=hp104
+ cat /var/emulab/boot/nickname
+ cut -d . -f 2
+ EEID=tboudwin-QV51740
+ cat /var/emulab/boot/nickname
+ cut -d . -f 3
+ EPID=pdc-edu-lab-PG0
+ cat /var/emulab/boot/mydomain
+ OURDOMAIN=utah.cloudlab.us
+ cat /var/emulab/boot/nickname
+ NFQDN=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ cat /var/emulab/boot/nodeid
+ PFQDN=hp104.utah.cloudlab.us
+ cat /var/emulab/boot/myip
+ MYIP=128.110.154.185
+ cat /var/emulab/boot/controlif
+ EXTERNAL_NETWORK_INTERFACE=br-ex
+ cat /var/emulab/boot/nickname
+ cut -f1 -d.
+ HOSTNAME=controller
+ uname -m
+ ARCH=x86_64
+ dpkg-query -S /sbin/init
+ grep -q systemd
+ expr 0 = 0
+ HAVE_SYSTEMD=1
+ OSJUNO=10
+ OSKILO=11
+ OSLIBERTY=12
+ OSMITAKA=13
+ OSNEWTON=14
+ OSOCATA=15
+ OSPIKE=16
+ OSQUEENS=17
+ . /etc/lsb-release
+ DISTRIB_ID=Ubuntu
+ DISTRIB_RELEASE=18.04
+ DISTRIB_CODENAME=bionic
+ DISTRIB_DESCRIPTION=Ubuntu 18.04.1 LTS
+ [ ! xqueens = x ]
+ OSCODENAME=queens
+ [ queens = juno ]
+ [ queens = kilo ]
+ [ queens = liberty ]
+ [ queens = mitaka ]
+ [ queens = newton ]
+ [ queens = ocata ]
+ [ queens = pike ]
+ [ queens = queens ]
+ OSVERSION=17
+ echo Ubuntu 18.04.1 LTS
+ grep -qi LTS
+ [ 0 -eq 0 ]
+ DO_UBUNTU_CLOUDARCHIVE=1
+ echo 18.04
+ cut -d. -f1
+ DISTRIB_MAJOR=18
+ [ 17 -ge 13 ]
+ KEYSTONEUSEMEMCACHE=1
+ [ 17 -eq 10 ]
+ REGION=RegionOne
+ [ 17 -ge 17 ]
+ KADMINPORT=5000
+ [ x = x3 ]
+ [  != 2 -a 17 -ge 12 ]
+ KAPISTR=v3
+ KEYSTONEAPIVERSION=3
+ NAPISTR=v2
+ [ 17 -ge 13 ]
+ NAPISTR=v2.1
+ [ x = x -a 17 -ge 11 ]
+ KEYSTONEUSEWSGI=1
+ [ 17 -ge 13 ]
+ PROJECT_DOMAIN_PARAM=project_domain_name
+ USER_DOMAIN_PARAM=user_domain_name
+ AUTH_TYPE_PARAM=auth_type
+ [ 17 -ge 14 ]
+ DBDPACKAGE=python-pymysql
+ DBDSTRING=mysql+pymysql
+ [ 1 -eq 1 ]
+ geni-get slice_email
+ SWAPPER_EMAIL=tboudwin@wcupa.edu
+ [ 1 -eq 1 ]
+ perl+  -ecat $found = 0; while (<STDIN>) { if ($_ =~ /\<[\d\w:]*routable_pool [^\>\<]*\/>/) { print STDERR "DEBUG: found empty pool: $_\n"; next; } if ($_ =~ /\<[\d\w:]*routable_pool [^\>]*client_id=['"]controller['"]/) { $found = 1; print STDERR "DEBUG: found: $_\n" } if ($found) { while ($_ =~ m/\<emulab:ipv4 address="([\d.]+)\" netmask=\"([\d\.]+)\"/g) { print "$1\n"; } } if ($found && $_ =~ /routable_pool\>/) { print STDERR "DEBUG: end found: $_\n"; $found = 0; } } /root/setup/manifests.0.xml

+ xargs
DEBUG: found:   <emulab:routable_pool client_id="controller" count="4" type="any" component_manager_id="urn:publicid:IDN+utah.cloudlab.us+authority+cm">

DEBUG: end found:   <emulab:ipv4 address="128.110.155.158" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.159" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.160" netmask="255.255.252.0"/><emulab:ipv4 address="128.110.155.161" netmask="255.255.252.0"/></emulab:routable_pool>

+ PUBLICADDRS=128.110.155.158 128.110.155.159 128.110.155.160 128.110.155.161
+ PUBLICCOUNT=0
+ expr 0 + 1
+ PUBLICCOUNT=1
+ expr 1 + 1
+ PUBLICCOUNT=2
+ expr 2 + 1
+ PUBLICCOUNT=3
+ expr 3 + 1
+ PUBLICCOUNT=4
+ [ ! -f /root/setup/topomap -o 0 -ne 0 ]
+ [ ! -z  ]
+ [ ( -s /root/setup/manifests.xml ) -a ( ! ( -s /root/setup/fqdn.map ) ) ]
+ [ ! -s /root/setup/fqdn.map ]
+ cat /root/setup/fqdn.map
+ cut -f1
+ xargs
+ NODES=controller compute-1
+ cat /root/setup/fqdn.map
+ cut -f2
+ xargs
+ FQDNS=controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us compute-1.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us
+ [ -z compute-1  ]
+ OTHERNODES=
+ [ controller = controller ]
+ continue
+ [ compute-1 = controller ]
+ OTHERNODES= compute-1
+ grep CONTROLLER /root/setup/settings
CONTROLLER="controller"
+ [ ! 0 = 0 ]
+ [ 0 -ne 0 ]
+ grep MIRRORSETUP /root/setup/settings
MIRRORSETUP=1
+ [ ! 0 -eq 0 ]
+ [ ! -f /root/setup/apt-updated -a 1 = 1 ]
+ [ ! -f /root/setup/cloudarchive-added -a 1 = 1 ]
+ [ ! -f /root/setup/apt-dist-upgraded -a 0 = 1 ]
+ maybe_install_packages crudini
+ [ ! 0 -eq 0 ]
+ are_packages_installed crudini
+ retval=1
+ [ ! -z crudini ]
+ dpkg -s crudini
+ [ ! 0 -eq 0 ]
+ shift
+ [ ! -z  ]
+ return 1
+ [ 1 -eq 1 ]
+ return 0
+ [ ! -f /root/setup/nextsparesubnet ]
+ cat /root/setup/nextsparesubnet
+ NEXTSPARESUBNET=253
+ [ ! -f /root/setup/mgmt-hosts -o 0 -ne 0 ]
+ [ 1 -gt 0 ]
+ i=0
+ [ 0 -lt 1 ]
+ [ -f /root/setup/ipinfo.tun0 ]
+ expr 0 + 1
+ i=1
+ continue
+ [ 1 -lt 1 ]
+ echo 253
+ echo 253
+ [ 0 -gt 0 ]
+ [ ! -e /root/setup/info.mgmt ]
+ . /root/setup/info.mgmt
+ MGMTIP=192.168.0.1
+ MGMTNETMASK=255.255.0.0
+ MGMTPREFIX=16
+ MGMTVLAN=0
+ MGMTMAC=
+ MGMT_NETWORK_INTERFACE=tun0
+ MGMTVLANDEV=
+ MGMTVLANTAG=
+ [ -e /root/setup/info.flat-lan-1 ]
+ continue
+ [ ! -f /root/setup/neutron.vars ]
+ [ ! -f /etc/emulab/bossnode -a 17 -ge 14 -a 1 = 1 ]
+ [ 0 -ne 0 ]
+ which wget
+ GETTER=/usr/bin/wget
+ [ -n /usr/bin/wget ]
+ GETTEROUT=/usr/bin/wget --remote-encoding=unix -c -O
+ GETTER=/usr/bin/wget --remote-encoding=unix -c -N
+ GETTERLOGARG=-o
+ [ 0 -ne 0 ]
+ IMAGEDIR=/root/setup/images
+ [ ! -d /root/setup/images ]
+ IMAGESETUPLOCKFILE=/root/setup/images/image-setup-lockfile
+ IMAGEUPLOADCMDFILE=/root/setup/images/image-upload-commands.sh
+ uname -m
+ ARCH=x86_64
+ [ x86_64 = aarch64 ]
+ [ -f /local/repository/setup-images-lib-x86_64.sh ]
+ lockfile-create --retry 65535 /root/setup/images/image-setup-lockfile
+ [ -f /root/setup/images/image-upload-commands.sh ]
+ echo *** Adding Images ...
*** Adding Images ...
+ . /root/setup/admin-openrc.sh
+ export OS_PROJECT_DOMAIN_NAME=default
+ export OS_USER_DOMAIN_NAME=default
+ export OS_PROJECT_NAME=admin
+ export OS_TENANT_NAME=admin
+ export OS_USERNAME=adminapi
+ export OS_PASSWORD=a9f3d26fda5b4ee3d9a8
+ export OS_AUTH_URL=http://controller:5000/v3
+ export OS_IDENTITY_API_VERSION=3
+ export OS_IMAGE_API_VERSION=2
+ export OS_AUTH_TYPE=password
+ . /root/setup/images/image-upload-commands.sh
+ glance image-create --name trusty-server --disk-format qcow2 --container-format bare --progress --file /root/setup/images/trusty-server-cloudimg-amd64-disk1.img
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 672b717df4baaf78a6ac714605b92f99     |
| container_format | bare                                 |
| created_at       | 2019-05-02T20:15:45Z                 |
| disk_format      | qcow2                                |
| id               | 3f806e61-9bd9-4e35-ba51-2c57aa11cf78 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | trusty-server                        |
| owner            | a3b1d2ed83f74877a4ef333e937e3474     |
| protected        | False                                |
| size             | 265617408                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2019-05-02T20:15:46Z                 |
| virtual_size     | None                                 |
| visibility       | shared                               |
+------------------+--------------------------------------+
+ glance image-create --name manila-service-image --disk-format qcow2 --container-format bare --progress --file /root/setup/images/manila-service-image-master.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | ef407c537caa8ae31c64a2662ec5da7e     |
| container_format | bare                                 |
| created_at       | 2019-05-02T20:15:48Z                 |
| disk_format      | qcow2                                |
| id               | 0b7ed317-753f-4204-90ed-cef4c21eb8dc |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | manila-service-image                 |
| owner            | a3b1d2ed83f74877a4ef333e937e3474     |
| protected        | False                                |
| size             | 390791168                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2019-05-02T20:15:50Z                 |
| virtual_size     | None                                 |
| visibility       | shared                               |
+------------------+--------------------------------------+
+ lockfile-remove /root/setup/images/image-setup-lockfile
+ uname -m
+ ARCH=x86_64
+ [ x86_64 = aarch64 ]
+ echo *** Doing x86_64-specific setup...
*** Doing x86_64-specific setup...
+ /local/repository/setup-basic-x86_64.sh
+ wait 22349 22350 22351
+ logtend basic
+ area=basic
+ echo basic
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=basic
+ date +%s
+ stamp=1556828228
+ date
+ date=Thu May  2 14:17:08 MDT 2019
+ eval tss=$LOGTIMESTART_basic
+ tss=1556828143
+ expr 1556828228 - 1556828143
+ tsres=85
+ perl -e print 85 / 60.0 . "\n"
+ resmin=1.41666666666667
+ echo END basic 1556828228 Thu May  2 14:17:08 MDT 2019
+ echo TOTAL basic 85 1.41666666666667
+ exit 0
+ echo SETUP_BASIC_DONE="1"
+ cp -p /local/repository/openstack-slothd.py /root/setup/
+ [ 17 -ge 11 ]
+ cat
+ [ 0 -eq 1 ]
+ RANDPASSSTRING=
+ [ -e /root/setup/random_admin_pass ]
+ logtstart ext
+ area=ext
+ echo ext
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ext
+ date +%s
+ stamp=1556828228
+ date
+ date=Thu May  2 14:17:08 MDT 2019
+ eval LOGTIMESTART_ext=1556828228
+ LOGTIMESTART_ext=1556828228
+ echo START ext 1556828228 Thu May  2 14:17:08 MDT 2019
+ find /local/repository/ext -maxdepth 1 -type d
+ grep -v ^.$
+ grep -v /local/repository/ext$
+ xargs
find: ‘/local/repository/ext’: No such file or directory
+ EXTDIRS=
+ [ ! -z  ]
+ logtend ext
+ area=ext
+ echo ext
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=ext
+ date +%s
+ stamp=1556828228
+ date
+ date=Thu May  2 14:17:08 MDT 2019
+ eval tss=$LOGTIMESTART_ext
+ tss=1556828228
+ expr 1556828228 - 1556828228
+ tsres=0
+ perl -e print 0 / 60.0 . "\n"
+ resmin=0
+ echo END ext 1556828228 Thu May  2 14:17:08 MDT 2019
+ echo TOTAL ext 0 0
+ [ -n 6ed5bbda06adabaf6f64 -a 1 = 1 ]
+ hostname
+ sed -n -e s/[^\.]*\.\(.*\)$/\1/p
+ mydomain=tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us
+ sed -n -e s/^nameserver \([0-9]*\.[0-9]*\.[0-9]*\.[0-9]*\).*$/\1/p
+ head -1
+ mynameserver=128.110.156.4
+ [ -z 128.110.156.4 ]
+ cp -p /etc/resolv.conf /etc/resolv.conf.orig
+ grep -q forwarders /etc/bind/named.conf.options
+ [ 0 -eq 0 -a -n 128.110.156.4 ]
+ cat /var/emulab/boot/mydomain
+ outerdomain=utah.cloudlab.us
+ dig @127.0.0.1 tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us

; <<>> DiG 9.11.3-1ubuntu1.3-Ubuntu <<>> @127.0.0.1 tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 38712
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5b8039345c95314e55567db05ccb504430d8305d3c0b6f23 (good)
;; QUESTION SECTION:
;tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us. IN A

;; AUTHORITY SECTION:
tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us. 3600	IN SOA ns1-1.example.org. root.localhost. 1556828150 3532 600 86400 3600

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Thu May 02 14:17:08 MDT 2019
;; MSG SIZE  rcvd: 173

+ [ 0 -eq 0 ]
+ echo nameserver 192.168.0.1
+ echo nameserver 128.110.156.4
+ echo search tboudwin-qv51740.pdc-edu-lab-pg0.utah.cloudlab.us utah.cloudlab.us
+ dig @192.168.0.1 boss.utah.cloudlab.us

; <<>> DiG 9.11.3-1ubuntu1.3-Ubuntu <<>> @192.168.0.1 boss.utah.cloudlab.us
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 46493
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 13, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: b0acd6a809e021a17b7190e45ccb5044e99ebd95c7fe4ed1 (good)
;; QUESTION SECTION:
;boss.utah.cloudlab.us.		IN	A

;; ANSWER SECTION:
boss.utah.cloudlab.us.	30	IN	A	128.110.156.4

;; AUTHORITY SECTION:
.			86216	IN	NS	h.root-servers.net.
.			86216	IN	NS	c.root-servers.net.
.			86216	IN	NS	d.root-servers.net.
.			86216	IN	NS	f.root-servers.net.
.			86216	IN	NS	g.root-servers.net.
.			86216	IN	NS	e.root-servers.net.
.			86216	IN	NS	j.root-servers.net.
.			86216	IN	NS	l.root-servers.net.
.			86216	IN	NS	b.root-servers.net.
.			86216	IN	NS	i.root-servers.net.
.			86216	IN	NS	k.root-servers.net.
.			86216	IN	NS	m.root-servers.net.
.			86216	IN	NS	a.root-servers.net.

;; Query time: 4 msec
;; SERVER: 192.168.0.1#53(192.168.0.1)
;; WHEN: Thu May 02 14:17:08 MDT 2019
;; MSG SIZE  rcvd: 305

+ [ ! 0 -eq 0 ]
+ mkdir -p /root/setup/pssh.setup-designate.stdout
+ mkdir -p /root/setup/pssh.setup-designate.stderr
+ cat /root/setup/fqdn.map
+ cut -f1
+ grep -v ^controller$
+ echo *** Saving original /etc/resolv.conf on all hosts...
*** Saving original /etc/resolv.conf on all hosts...
+ /usr/bin/parallel-ssh -t 0 -O StrictHostKeyChecking=no -h /tmp/pssh.hosts -o /root/setup/pssh.setup-designate.stdout -e /root/setup/pssh.setup-designate.stderr /bin/cp -p /etc/resolv.conf /etc/resolv.conf.pre-designate
[1] 14:17:08 [SUCCESS] compute-1
+ echo *** Copying Designate /etc/resolv.conf on all hosts...
*** Copying Designate /etc/resolv.conf on all hosts...
+ /usr/bin/parallel-scp -t 0 -O StrictHostKeyChecking=no -h /tmp/pssh.hosts -o /root/setup/pssh.setup-designate.stdout -e /root/setup/pssh.setup-designate.stderr /etc/resolv.conf /etc/resolv.conf
[1] 14:17:08 [SUCCESS] compute-1
+ echo ***
***
+ echo *** Done with OpenStack Setup!
*** Done with OpenStack Setup!
+ echo ***
***
+ echo *** Login to your shiny new cloud at 
*** Login to your shiny new cloud at 
+ echo   http://controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us/horizon/auth/login/?next=/horizon/project/instances/ !  
  http://controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us/horizon/auth/login/?next=/horizon/project/instances/ !  
+ echo ***
***
+ echo Your OpenStack instance has completed setup!  Browse to http://controller.tboudwin-QV51740.pdc-edu-lab-PG0.utah.cloudlab.us/horizon/auth/login/?next=/horizon/project/instances/ .  
+ mail -s OpenStack Instance Finished Setting Up tboudwin@wcupa.edu
+ touch /root/setup/controller-done
+ logtend controller
+ area=controller
+ echo controller
+ sed -e s/[^a-zA-Z_0-9]/_/g
+ varea=controller
+ date +%s
+ stamp=1556828228
+ date
+ date=Thu May  2 14:17:08 MDT 2019
+ eval tss=$LOGTIMESTART_controller
+ tss=1556827517
+ expr 1556828228 - 1556827517
+ tsres=711
+ perl -e print 711 / 60.0 . "\n"
+ resmin=11.85
+ echo END controller 1556828228 Thu May  2 14:17:08 MDT 2019
+ echo TOTAL controller 711 11.85
+ exit 0
